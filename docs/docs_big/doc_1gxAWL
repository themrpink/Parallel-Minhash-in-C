== Notes ==
&lt;references /&gt;

==References==
* Allen K. Kneese and Clifford S. Russell (1987). &quot;environmental economics,&quot; ''[[The New Palgrave: A Dictionary of Economics]]'', v. 2, pp.&amp;nbsp;159–64.
* Robert N. Stavins (2008). &quot;environmental economics,&quot; ''[[The New Palgrave Dictionary of Economics]]'', 2nd Edition. [http://www.dictionaryofeconomics.com/sample_article Abstract &amp; article.]
* Maureen L. Cropper and [[Wallace E. Oates]] (1992). &quot;Environmental Economics: A Survey,&quot; ''Journal of Economic Literature'', 30(2), pp. [https://web.archive.org/web/20100923035535/http://econ-server.umd.edu/~cropper/publications/jc22.pdf 675-740](press '''+''').
* David Pearce (2002). &quot;An Intellectual History Of Environmental Economics&quot;, [http://arjournals.annualreviews.org/loi/energy.2 Annual Review of Energy and the Environment] 2002, 27:57–81.
* Tausch, Arno, ‘Smart Development’. An Essay on a New Political Economy of the Environment (March 22, 2016). Available at SSRN: https://ssrn.com/abstract=2752988 or https://dx.doi.org/10.2139/ssrn.2752988
* UNEP (2007). Guidelines for Conducting Economic Valuation of Coastal Ecosystem Goods and Services, [http://www.unepscs.org/SCS_Documents/Download/19_-_Technical_Publications_and_Guidelines/Technical_Publication_08_-_Guidelines_for_Conducting_Economic_Valuation_of_Coastal_Ecosystem_Goods_and_Services.html UNEP/GEF/SCS Technical Publication No. 8.]
* UNEP (2007). Procedure for Determination of National and Regional Economic Values for Ecotone Goods and Services, and Total Economic Values of Coastal Habitats in the context of the UNEP/GEF Project Entitled: “Reversing Environmental Degradation Trends in the South China Sea and Gulf of Thailand”, [http://www.unepscs.org/SCS_Documents/Download/14_-_South_China_Sea_Project_Knowledge_Documents/South_China_Sea_Knowledge_Document_on_the_Economic_Valuation_of_the_Goods_and_Services_of_Coastal_Habitats.html South China Sea Knowledge Document No. 3. UNEP/GEF/SCS/Inf.3]
*{{cite book|url=https://eclass.unipi.gr/modules/document/file.php/NAS247/tselepidis/ATT00106.pdf|edition=3|author=Perman, Roger|display-authors=etal|isbn=978-0273655596|year=2003|title=Natural Resource and Environment Economics|publisher=Pearson}}
* {{cite book | last=Field | first=Barry | title=Environmental economics : an introduction | publisher=McGraw-Hill | location=New York, NY | year=2017 | isbn=978-0-07-802189-3 | oclc=931860817 }}

==Further reading==
* David A. Anderson (2019). Environmental Economics and Natural Resource Management 5e, [https://www.routledge.com/Environmental-Economics-and-Natural-Resource-Management-5th-Edition/Anderson/p/book/9780815359036] New York: Routledge.
* John Asafu-Adjaye (2005). [https://archive.org/details/environmentaleco0000asaf &lt;!-- quote=&quot;Environmental Economics&quot;. --&gt; Environmental Economics for Non-Economists] 2e, Singapore: World Scientific.
* Gregory C. Chow (2014). Economics Analysis of Environmental Problems, Singapore: World Scientific.

{{Navboxes
|list=
{{Environmental science}}
{{Environmental social science}}
{{Industrial ecology}}
{{Sustainability}}
}}
{{Authority control}}

{{DEFAULTSORT:Environmental Economics}}
[[Category:Environmental economics| ]]
[[Category:Environmental social science]]
[[Category:Industrial ecology]]
[[Category:Market failure]]</text>
      <sha1>a56psyg4zxjby2qb7ub5g794lreg38y</sha1>
    </revision>
  </page>
  <page>
    <title>Resource economics</title>
    <ns>0</ns>
    <id>47764</id>
    <redirect title="Natural resource economics" />
    <revision>
      <id>235936501</id>
      <parentid>235929789</parentid>
      <timestamp>2008-09-03T01:11:11Z</timestamp>
      <contributor>
        <username>ImperfectlyInformed</username>
        <id>5106682</id>
      </contributor>
      <comment>redirecting to [[natural resource economics]], as discussed on the talkpage here and [[Talk:Environmental economics]]. Feel free to revert if you disagree.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="40" xml:space="preserve">#REDIRECT [[Natural resource economics]]</text>
      <sha1>jdytkgfkvlh4wt49vpgwrsdvqxd1vp4</sha1>
    </revision>
  </page>
  <page>
    <title>Bit-mapped graphics</title>
    <ns>0</ns>
    <id>47765</id>
    <redirect title="Raster graphics" />
    <revision>
      <id>15942837</id>
      <parentid>45925</parentid>
      <timestamp>2002-04-05T19:36:19Z</timestamp>
      <contributor>
        <username>Damian Yerrick</username>
        <id>1</id>
      </contributor>
      <minor />
      <comment>keep on consolidating =&amp;gt; Raster graphics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="29" xml:space="preserve">#REDIRECT [[Raster graphics]]</text>
      <sha1>tp9pj7jf2c0vxnf62l443slhnbbjvkp</sha1>
    </revision>
  </page>
  <page>
    <title>CDR coding</title>
    <ns>0</ns>
    <id>47766</id>
    <revision>
      <id>812389903</id>
      <parentid>726150580</parentid>
      <timestamp>2017-11-27T16:20:35Z</timestamp>
      <contributor>
        <username>Schol-R-LEA</username>
        <id>280594</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2747" xml:space="preserve">{{No footnotes|date=October 2011}}
In [[computer science]] '''CDR coding''' is a [[data compression|compressed]] [[data (computing)|data representation]] for [[Lisp programming language|Lisp]] [[linked list]]s. It was developed and patented by the [[MIT Artificial Intelligence Laboratory]], and implemented in [[computer]] hardware in a number of [[Lisp machine]]s derived from the MIT [[CADR (computing system)|CADR]].

CDR coding is in fact a fairly general idea; whenever a data object ''A'' ends in a [[reference]] to another data structure ''B'', we can instead place the structure ''B'' itself there, overlapping and running off the end of ''A''. By doing this we free the space required by the reference, which can add up if done many times, and also improve [[locality of reference]], enhancing performance on modern machines. The transformation is especially effective for the [[cons]]-based lists it was created for; we free about half of the space for each node we perform this transformation on.

It is not always possible to perform this substitution, because there might not be a large enough chunk of free space beyond the end of A. Thus, some objects will end in a real reference, and some with the referenced object, and the machine must be able to tell by reading the final cell which one it is. This can be accomplished with some inefficiency in software by the use of [[tagged pointer]]s, which allow a pointer in a final position to be specifically tagged as such, but is best done in hardware.

In the presence of [[mutable object]]s, CDR coding becomes more complex. If a reference is updated to point to another object, but currently has an object stored in that field, the object must be relocated, along with any other pointers to it. Not only are such moves typically expensive or impossible, but over time they cause [[fragmentation (computer)|fragmentation]] of the store. This problem is typically avoided by using CDR coding only on [[immutable object|immutable]] data structures.

[[Unrolled linked list]]s are simpler and often higher-performance than CDR coding (no &quot;tagged pointers&quot;; typically less fragmentation).{{Citation needed|date=December 2011}} For short lists, CDR coding uses the least amount of space.

==External links==
* {{cite web | editor1 = Mark Kantrowitz | editor2 = Barry Margolin | url = http://www.faqs.org/faqs/lisp-faq/part2/section-9.html | work = FAQ: Lisp Frequently Asked Questions | title = (2-9) What is CDR-coding? | publisher = Advameg, Inc. | accessdate = 2011-10-09 }}
* {{cite book | first = John | last = Allen | title = The Anatomy of Lisp | publisher = McGraw-Hill | date = 1978 }}
[[Category:Lisp (programming language)]]
[[Category:Data compression]]

{{compu-prog-stub}}</text>
      <sha1>sttxio7qyt9smvrpp62q01naouyzyc8</sha1>
    </revision>
  </page>
  <page>
    <title>Threaded code</title>
    <ns>0</ns>
    <id>47767</id>
    <revision>
      <id>971857784</id>
      <parentid>970752945</parentid>
      <timestamp>2020-08-08T17:40:33Z</timestamp>
      <contributor>
        <username>Qwfp</username>
        <id>6032993</id>
      </contributor>
      <comment>/* History */ [[MOS:CAPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28007" xml:space="preserve">{{distinguish|text=[[thread (computing)|multi-threaded programming]]}}

In [[computer science]], '''threaded code''' is a programming technique where the code has a form that essentially consists entirely of calls to [[subroutine]]s. It is often used in [[compiler]]s, which may generate code in that form or be implemented in that form themselves. The code may be processed by an [[interpreter (computing)|interpreter]] or it may simply be a sequence of [[machine code]] [[subroutine call|call]] instructions.

Threaded code has better [[code density|density]] than code generated by alternative generation techniques and by alternative [[calling convention]]s.  In cached architectures, it may execute slightly slower.{{citation needed|date=February 2012}} However, a program that is small enough to fit in a [[computer processor]]'s [[CPU cache|cache]] may run faster than a larger program that suffers many [[cache miss]]es.&lt;ref name=&quot;tuwien1&quot;&gt;{{cite web|url=http://www.complang.tuwien.ac.at/forth/threading/ |title=Speed of various interpreter dispatch techniques V2}}&lt;/ref&gt; Small programs may also be faster at thread switching, when other programs have filled the cache.

Threaded code is best known for its use in many compilers of [[programming language]]s, such as [[Forth (programming language)|Forth]], many implementations of [[BASIC]], some implementations of [[COBOL]], early versions of [[B (programming language)|B]],&lt;ref&gt;Dennis M. Ritchie, [https://www.bell-labs.com/usr/dmr/www/chist.html &quot;The Development of the C Language&quot;], 1993. Quote: &quot;The B compiler on the PDP-7 did not generate machine instructions, but instead 'threaded code' ...&quot;&lt;/ref&gt; and other languages for small [[minicomputer]]s and for [[amateur radio satellite]]s.{{citation needed|date=February 2016}}

==History==
{{Original research section|date=February 2020}}
The common way to make computer programs is to use a [[compiler]] to translate [[source code]] (written in some [[Symbolic language (programming)|symbolic language]]) to [[machine code]]. The resulting [[executable]] is typically fast but, because it is specific to a [[computer hardware|hardware]] platform, it isn't portable. A different approach is to generate [[instruction set|instructions]] for a [[virtual machine]] and to use an [[interpreter (computing)|interpreter]] on each hardware platform. The interpreter instantiates the virtual machine environment and executes the instructions.  Thus, only the interpreter must be compiled.

Early computers had relatively little memory. For example, most [[Data General Nova]], [[IBM 1130]], and many of the first [[microcomputer]]s had only 4&amp;nbsp;kB of RAM installed. Consequently, a lot of time was spent trying to find ways to reduce a program's size, to fit in the available memory.

One solution is to use an interpreter which reads the symbolic language a bit at a time, and calls functions to perform the actions. As the source code is typically much [[code density|denser]] than the resulting machine code, this can reduce overall memory use. This was the reason [[Microsoft BASIC]] is an interpreter:{{efn|[[Dartmouth BASIC]], upon which MS is ultimately based, was a compiler that ran on mainframe machines.}} its own code had to share the 4&amp;nbsp;kB memory of machines like the [[Altair 8800]] with the user's source code. A compiler translates from a source language to machine code, so the compiler, source, and output must all be in memory at the same time. In an interpreter, there is no output.  Code is created a line at a time, executed, and then discarded.

Threaded code is a formatting style for compiled code that minimizes memory use. Instead of writing out every step of an operation at its every occurrence in the program, as was common in [[macro assembler]]s for instance, the compiler writes each common bit of code into a subroutine.  Thus, each bit exists in only one place in memory (see &quot;[[Don't repeat yourself]]&quot;). The top-level application in these programs may consist of nothing but subroutine calls. Many of these subroutines, in turn, also consist of nothing but lower-level subroutine calls. This technique {{mdashb}}[[code refactoring]]{{mdashb}} remains widely used today, although for different reasons.

Mainframes and some early microprocessors such as the [[RCA 1802]] required several instructions to call a subroutine. In the top-level application and in many subroutines, that sequence is constantly repeated, with only the subroutine address changing from one call to the next. This means that a program consisting of many function calls may have considerable amounts of repeated code as well.

To address this, threaded code systems used pseudo-code to represent function calls in a single operator. At run time, a tiny &quot;interpreter&quot; would scan over the top-level code, extract the subroutine's address in memory, and call it. In other systems, this same basic concept is implemented as a [[branch table]], [[dispatch table]], or [[virtual method table]], all of which consist of a table of subroutine addresses.

During the 1970s, hardware designers spent considerable effort to make subroutine calls faster and simpler. On the improved designs, only a single instruction is expended to call a subroutine, so the use of a pseudo-instruction saves no room.{{Citation needed|date=March 2020|reason=storing only the address of the subroutine actually does save room compared to storing a call instruction, which necessarily must contain something in addition to the address of the subroutine.}} Additionally, the performance of these calls is almost free of additional overhead.  Today, though almost all programming languages focus on isolating code into subroutines, they do so for code clarity and maintainability, not to save space.

Threaded code systems save room by replacing that list of function calls, where only the subroutine address changes from one call to the next, with a list of execution tokens, which are essentially function calls with the call opcode(s) stripped off, leaving behind only a list of addresses.&lt;ref&gt;
David Frech.
[https://muforth.nimblemachines.com/readme/ &quot;muforth readme&quot;].
section &quot;Simple and tail-recursive native compiler&quot;.
&lt;/ref&gt;&lt;ref name=&quot;heller&quot;&gt;
Steve Heller.
[https://books.google.com/books?id=gaajBQAAQBAJ &quot;Efficient C/C++ Programming: Smaller, Faster, Better&quot;].
2014.
Chapter 5: &quot;Do you need an interpreter?&quot;
p. 195.
&lt;/ref&gt;&lt;ref&gt;
Jean-Paul Tremblay; P. G. Sorenson.
[https://books.google.com/books?id=MacmAAAAMAAJ &quot;The Theory and Practice of Compiler Writing&quot;].
1985.
p. 527
&lt;/ref&gt;&lt;ref&gt;
[https://books.google.com/books?id=4WJVAAAAYAAJ &quot;Wireless World: Electronics, Radio, Television, Volume 89&quot;].
p. 73.
&lt;/ref&gt;&lt;ref&gt;
[https://books.google.com/books?id=ZWIfAAAAMAAJ &quot;Byte, Volume 5&quot;].
1980.
p. 212
&lt;/ref&gt;

Over the years, programmers have created many variations on that &quot;interpreter&quot; or &quot;small selector&quot;. The particular address in the list of addresses may be extracted using an index, [[general purpose register]] or [[pointer (computer programming)|pointer]]. The addresses may be direct or indirect, contiguous or non-contiguous (linked by pointers), relative or absolute, resolved at compile time or dynamically built. No single variation is &quot;best&quot; for all situations.

==Development==
To save space, programmers squeezed the lists of subroutine calls into simple lists of subroutine addresses, and used a small loop to call each subroutine in turn. For example, the following pseudocode uses this technique to add two numbers A and B. In the example, the list is labeled '''thread''' and a variable '''ip''' (Instruction Pointer) tracks our place within the list. Another variable '''sp''' (Stack Pointer) contains an address elsewhere in memory that is available to hold a value temporarily.

&lt;syntaxhighlight lang=&quot;c&quot;&gt;
start:
  ip = &amp;thread  // points to the address '&amp;pushA', not the textual label 'thread'
top:
  jump *ip++  // follow ip to address in thread, follow that address to subroutine, advance ip
thread:
  &amp;pushA
  &amp;pushB
  &amp;add
  ...
pushA:
  *sp++ = A  // follow sp to available memory, store A there, advance sp to next 
  jump top
pushB:
  *sp++ = B
  jump top
add:
  addend = *--sp  // point sp to last value saved on stack, follow it to copy that value out
  *sp++ = *--sp + addend  // copy another value out of stack, add, copy sum into stack
  jump top
&lt;/syntaxhighlight&gt;

&lt;!--In this case, decoding the [[bytecode]]s is performed once, during program compilation or program load, so it is not repeated each time an instruction is executed. This can save much time and space when decode and dispatch overhead is large compared to the execution cost.

Note, however, addresses in &lt;code&gt;thread&lt;/code&gt; for &lt;code&gt;&amp;pushA&lt;/code&gt;, &lt;code&gt;&amp;pushB&lt;/code&gt;, etc., are two or more bytes, compared to (typically) one byte, for the decode and dispatch interpreter described above. In general, instructions for a decode and dispatch interpreter may be any size. For example, a decode and dispatch interpreter to simulate an Intel Pentium decodes instructions that range from 1 to 16 bytes. However, bytecoded systems typically choose 1-byte codes for the most-common operations. Thus, the thread often has a higher space cost than bytecodes. In most uses, the reduction in decode cost outweighs the increase in space cost.

Note also that while bytecodes are nominally machine-independent, the format and value of the pointers in threads generally depend on the target machine which is executing the interpreter. Thus, an interpreter might load a portable bytecode program, decode the bytecodes to generate platform-dependent threaded code, then execute threaded code without further reference to the bytecodes.--&gt;

The calling loop at &lt;code&gt;top&lt;/code&gt; is so simple that it can be repeated inline at the end of each subroutine. Control now jumps once, from the end of a subroutine to the start of another, instead of jumping twice via &lt;code&gt;top&lt;/code&gt;. For example:

&lt;syntaxhighlight lang=&quot;c&quot;&gt;
start:
  ip = &amp;thread  // ip points to &amp;pushA (which points to the first instruction of pushA)
  jump *ip++  // send control to first instruction of pushA and advance ip to &amp;pushB
thread:
  &amp;pushA
  &amp;pushB
  &amp;add
  ...
pushA:
  *sp++ = A  // follow sp to available memory, store A there, advance sp to next 
  jump *ip++  // send control where ip says to (i.e. to pushB) and advance ip
pushB:
  *sp++ = B
  jump *ip++
add:
  addend = *--sp  // point sp to last value saved on stack, follow it to copy that value out
  *sp++ = *--sp + addend  // copy another value out of stack, add, copy sum into stack
  jump *ip++
&lt;/syntaxhighlight&gt;

This is called '''direct threaded code''' (DTC). Although the technique is older, the first widely circulated use of the term &quot;threaded code&quot; is probably James R. Bell's 1973 article &quot;Threaded Code&quot;.&lt;ref&gt;{{cite journal|last=Bell|first=James R.|title=Threaded code|journal=Communications of the ACM|year=1973|volume=16|issue=6|pages=370–372|doi=10.1145/362248.362270}}&lt;/ref&gt;

In 1970, [[Charles H. Moore]] invented a more compact arrangement, '''indirect threaded code''' (ITC), for his Forth virtual machine.  Moore arrived at this arrangement because [[Data General Nova|Nova]] minicomputers had an [[indirection bit]] in every address, which made ITC easy and fast. Later, he said that he found it so convenient that he propagated it into all later Forth designs.&lt;ref&gt;Moore, Charles H., published remarks in Byte Magazine's Forth Issue&lt;/ref&gt;

Today, some Forth compilers generate direct-threaded code while others generate indirect-threaded code. The executables act the same either way.

==Threading models==
Practically all executable threaded code uses one or another of these methods for invoking subroutines (each method is called a &quot;threading model&quot;).

===Direct threading===
Addresses in the thread are the addresses of machine language. This form is simple, but may have overheads because the thread consists only of machine addresses, so all further parameters must be loaded indirectly from memory. Some Forth systems produce direct-threaded code. On many machines direct-threading is faster than subroutine threading (see reference below).

An example of a stack machine might execute the sequence &quot;push A, push B, add&quot;. That might be translated to the following thread and routines, where &lt;code&gt;ip&lt;/code&gt; is initialized to the address labeled &lt;code&gt;thread&lt;/code&gt; (i.e., the address where &lt;code&gt;&amp;pushA&lt;/code&gt; is stored).

&lt;syntaxhighlight lang=&quot;c&quot;&gt;
start:
  ip = &amp;thread  // ip points to &amp;pushA (which points to the first instruction of pushA)
  jump *ip++  // send control to first instruction of pushA and advance ip to &amp;pushB
thread:
  &amp;pushA
  &amp;pushB
  &amp;add
  ...
pushA:
  *sp++ = A
  jump *ip++ // send control where ip says to (i.e. to pushB) and advance ip
pushB:
  *sp++ = B
  jump *ip++
add:
  addend = *--sp
  *sp++ = *--sp + addend
  jump *ip++
&lt;/syntaxhighlight&gt;

Alternatively, operands may be included in the thread. This can remove some indirection needed above, but makes the thread larger:

&lt;syntaxhighlight lang=&quot;c&quot;&gt;
start:
  ip = &amp;thread
  jump *ip++
thread:
  &amp;push
  &amp;A  // address where A is stored, not literal A
  &amp;push
  &amp;B
  &amp;add
  ...
push:
  *sp++ = *ip++  // must move ip past operand address, since it is not a subroutine address
  jump *ip++
add:
  addend = *--sp
  *sp++ = *--sp + addend
  jump *ip++
&lt;/syntaxhighlight&gt;

===Indirect threading===
Indirect threading uses pointers to locations that in turn point to machine code. The indirect pointer may be followed by operands which are stored in the indirect &quot;block&quot; rather than storing them repeatedly in the thread. Thus, indirect code is often more compact than direct-threaded code.  The indirection typically makes it slower, though usually still faster than bytecode interpreters. Where the handler operands include both values and types, the space savings over direct-threaded code may be significant. Older FORTH systems typically produce indirect-threaded code.

For example, if the goal is to execute &quot;push A, push B, add&quot;, the following might be used. Here, &lt;code&gt;ip&lt;/code&gt; is initialized to address &lt;code&gt;&amp;thread&lt;/code&gt;, each code fragment (&lt;code&gt;push&lt;/code&gt;, &lt;code&gt;add&lt;/code&gt;) is found by double-indirecting through &lt;code&gt;ip&lt;/code&gt; and an indirect block; and any operands to the fragment are found in the indirect block following the fragment's address. This requires keeping the ''current'' subroutine in &lt;code&gt;ip&lt;/code&gt;, unlike all previous examples where it contained the ''next'' subroutine to be called.

&lt;syntaxhighlight lang=&quot;c&quot;&gt;
start:
  ip = &amp;thread  // points to '&amp;i_pushA'
  jump *(*ip)  // follow pointers to 1st instruction of 'push', DO NOT advance ip yet
thread:
  &amp;i_pushA
  &amp;i_pushB
  &amp;i_add
  ...
i_pushA:
  &amp;push
  &amp;A
i_pushB:
  &amp;push
  &amp;B
i_add:
  &amp;add
push:
  *sp++ = *(*ip + 1)  // look 1 past start of indirect block for operand address
  jump *(*++ip)  // advance ip in thread, jump through next indirect block to next subroutine
add:
  addend = *--sp
  *sp++ = *--sp + addend
  jump *(*++ip)
&lt;/syntaxhighlight&gt;

===Subroutine threading===
So-called &quot;subroutine-threaded code&quot; (also &quot;call-threaded code&quot;) consists of a series of machine-language &quot;call&quot; instructions (or addresses of functions to &quot;call&quot;, as opposed to direct threading's use of &quot;jump&quot;). Early compilers for [[ALGOL]], Fortran, Cobol and some Forth systems often produced subroutine-threaded code. The code in many of these systems operated on a last-in-first-out (LIFO) stack of operands, for which compiler theory was well-developed. Most modern processors have special hardware support for subroutine &quot;call&quot; and &quot;return&quot; instructions, so the overhead of one extra machine instruction per dispatch is somewhat diminished.

Anton Ertl, the [[Gforth]] compiler's co-creator, stated that &quot;in contrast to popular myths, subroutine threading is usually slower than direct threading&quot;.&lt;ref&gt;{{cite web| url=http://www.complang.tuwien.ac.at/forth/threaded-code.html#what| title=What is Threaded Code?| first=Anton| last=Ertl}}&lt;/ref&gt; However, Ertl's most recent tests&lt;ref name=&quot;tuwien1&quot;/&gt; show that subroutine threading is faster than direct threading in 15 out of 25 test cases. More specifically, he found that direct threading is the fastest threading model on Xeon, Opteron, and Athlon processors, indirect threading is fastest on Pentium M processors, and subroutine threading is fastest on Pentium 4, Pentium III, and PPC processors.

As an example of call threading for &quot;push A, push B, add&quot;:

&lt;syntaxhighlight lang=&quot;c&quot;&gt;
thread:
  call pushA
  call pushB
  call add
  ret
pushA:
  *sp++ = A
  ret
pushB:
  *sp++ = B
  ret
add:
  addend = *--sp
  *sp++ = *--sp + addend
  ret
&lt;/syntaxhighlight&gt;

===Token threading===
Token-threaded code uses lists of 8 or 12-bit {{cn|date=July 2020}} indexes to a table of pointers. It is notably compact, without much special effort by a programmer. It is usually half to three-fourths the size of other threadings, which are themselves a quarter to an eighth the size of non-threaded code. The table's pointers can either be indirect or direct. Some Forth compilers produce token-threaded code. Some programmers consider the &quot;[[p-code machine|p-code]]&quot; generated by some [[Pascal (programming language)|Pascal]] compilers, as well as the [[bytecode]]s used by [[.NET Framework|.NET]], [[Java (programming language)|Java]], BASIC and some [[C (programming language)|C]] compilers, to be token-threading.

A common approach, historically, is bytecode, which uses 8-bit opcodes and, often, a stack-based virtual machine. A typical interpreter is known as a &quot;[[decode and dispatch interpreter]]&quot;, and follows the form:

&lt;syntaxhighlight lang=&quot;c&quot;&gt;
start:
  vpc = &amp;thread
top:
  i = decode(vpc++)  /* may be implemented simply as:  return *vpc */
  addr = table[i]
  jump *addr
thread:  /* Contains bytecode, not machine addresses.  Hence it is more compact. */
  1 /*pushA*/
  2 /*pushB*/
  0 /*add*/
table:
  &amp;add    /* table[0] = address of machine code that implements bytecode 0 */
  &amp;pushA  /* table[1] ... */
  &amp;pushB  /* table[2] ... */
pushA:
  *sp++ = A
  jump top
pushB:
  *sp++ = B
  jump top
add:
  addend = *--sp
  *sp++ = *--sp + addend
  jump top
&lt;/syntaxhighlight&gt;

If the virtual machine uses only byte-size instructions, &lt;code&gt;decode()&lt;/code&gt; is simply a fetch from &lt;code&gt;thread&lt;/code&gt;, but often there are commonly used 1-byte instructions plus some less-common multibyte instructions (see [[complex instruction set computer]]), in which case &lt;code&gt;decode()&lt;/code&gt; is more complex. The decoding of single byte opcodes can be very simply and efficiently handled by a branch table using the opcode directly as an index.
