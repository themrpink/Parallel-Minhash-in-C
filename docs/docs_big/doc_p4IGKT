== History ==

Bayes' theorem is named after Reverend [[Thomas Bayes]] ({{IPAc-en|b|eɪ|z}}; 1701?–1761), who first used conditional probability to provide an algorithm (his Proposition 9) that uses evidence to calculate limits on an unknown parameter, published as ''[[An Essay towards solving a Problem in the Doctrine of Chances]]'' (1763). He studied how to compute a distribution for the probability parameter of a [[binomial distribution]] (in modern terminology). Bayes's unpublished manuscript was significantly edited by [[Richard Price]] before it was posthumously read at the [[Royal Society]]. Price edited&lt;ref&gt;{{cite book |first = Richard |last = Allen |title=David Hartley on Human Nature |url = https://books.google.com/books?id=NCu6HhGlAB8C&amp;pg=PA243 |access-date=16 June 2013 |year=1999 |publisher=SUNY Press |isbn=978-0-7914-9451-6 |pages=243–4 }}&lt;/ref&gt; Bayes's major work &quot;[[An Essay towards solving a Problem in the Doctrine of Chances]]&quot; (1763), which appeared in ''[[Philosophical Transactions]]'',&lt;ref name=&quot;Price1763&quot;&gt;{{cite journal |doi=10.1098/rstl.1763.0053 |journal=Philosophical Transactions of the Royal Society of London |volume=53 |year=1763 |pages=370–418 |url=http://www.stat.ucla.edu/history/essay.pdf |title=An Essay towards solving a Problem in the Doctrine of Chance. By the late Rev. Mr. Bayes, communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S. |author1=Bayes, Thomas |author2=Price, Richard |name-list-style=amp |access-date=2003-12-27 |archive-url=https://web.archive.org/web/20110410085940/http://www.stat.ucla.edu/history/essay.pdf |archive-date=2011-04-10 |url-status=dead }}&lt;/ref&gt; and contains Bayes' theorem. Price wrote an introduction to the paper which provides some of the philosophical basis of [[Bayesian statistics]]. In 1765, he was elected a Fellow of the Royal Society in recognition of his work on the legacy of Bayes.&lt;ref name=&quot;Holland46&quot;&gt;Holland, pp.&amp;nbsp;46–7.&lt;/ref&gt;&lt;ref&gt;{{cite book |first = Richard |last = Price |title=Price: Political Writings |url = https://books.google.com/books?id=xdH-gjy2vzUC&amp;pg=PR23 |access-date=16 June 2013 |year=1991 |publisher = Cambridge University Press |isbn = 978-0-521-40969-8 |page = xxiii }}&lt;/ref&gt; In what he called a scholium, Bayes extended his algorithm to any unknown prior cause.

Independently of Bayes, [[Pierre-Simon Laplace]] in 1774, and later in his 1812 ''[[Théorie analytique des probabilités]]'', used conditional probability to formulate the relation of an updated [[posterior probability]] from a prior probability, given evidence. He reproduced and extended Bayes's results in 1774, apparently unaware of Bayes's work.{{NoteTag |1 = Laplace refined Bayes's theorem over a period of decades:
* Laplace announced his independent discovery of Bayes' theorem in: Laplace (1774) &quot;Mémoire sur la probabilité des causes par les événements,&quot; &quot;Mémoires de l'Académie royale des Sciences de MI (Savants étrangers),&quot; '''4''': 621–656. Reprinted in: Laplace, &quot;Oeuvres complètes&quot; (Paris, France: Gauthier-Villars et fils, 1841), vol. 8, pp.&amp;nbsp;27–65. Available on-line at: [http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32.image Gallica]. Bayes' theorem appears on p.&amp;nbsp;29.
* Laplace presented a refinement of Bayes' theorem in: Laplace (read: 1783 / published: 1785) &quot;Mémoire sur les approximations des formules qui sont fonctions de très grands nombres,&quot; &quot;Mémoires de l'Académie royale des Sciences de Paris,&quot; 423–467. Reprinted in: Laplace, &quot;Oeuvres complètes&quot; (Paris, France: Gauthier-Villars et fils, 1844), vol. 10, pp.&amp;nbsp;295–338. Available on-line at: [http://gallica.bnf.fr/ark:/12148/bpt6k775981/f218.image.langEN Gallica]. Bayes' theorem is stated on page 301.
* See also: Laplace, &quot;Essai philosophique sur les probabilités&quot; (Paris, France: Mme. Ve. Courcier [Madame veuve (i.e., widow) Courcier], 1814), [https://books.google.com/books?id=rDUJAAAAIAAJ&amp;pg=PA10#v=onepage page 10]. English translation: Pierre Simon, Marquis de Laplace with F. W. Truscott and F. L. Emory, trans., &quot;A Philosophical Essay on Probabilities&quot; (New York, New York: John Wiley &amp; Sons, 1902), [https://google.com/books?id=WxoPAAAAIAAJ&amp;pg=PA15#v=onepage page 15].}}&lt;ref&gt;{{cite book |title = Classical Probability in the Enlightenment |first=Lorraine |last=Daston |publisher=Princeton Univ Press |year=1988 |page=268 |isbn=0-691-08497-1 |url = https://books.google.com/books?id=oq8XNbKyUewC&amp;pg=PA268 }}&lt;/ref&gt; The [[Bayesian probability|Bayesian interpretation]] of probability was developed mainly by Laplace.&lt;ref&gt;{{cite book |last1=Stigler |first1=Stephen M. |chapter=Inverse Probability |pages=99–138 |chapter-url={{Google books|M7yvkERHIIMC|page=99|plainurl=yes}} |title=The History of Statistics: The Measurement of Uncertainty Before 1900 |date=1986 |publisher=Harvard University Press |isbn=978-0-674-40341-3 }}&lt;/ref&gt;

[[Harold Jeffreys|Sir Harold Jeffreys]] put Bayes's algorithm and Laplace’s formulation on an [[axiomatic system|axiomatic]] basis, writing that Bayes' theorem &quot;is to the theory of probability what the [[Pythagorean theorem]] is to geometry&quot;.&lt;ref&gt;{{cite book |last=Jeffreys |first=Harold |author-link=Harold Jeffreys |year=1973 |title=Scientific Inference |url=https://archive.org/details/scientificinfere0000jeff |url-access=registration |publisher=[[Cambridge University Press]] |edition=3rd |isbn=978-0-521-18078-8 |page=[https://archive.org/details/scientificinfere0000jeff/page/31 31]}}&lt;/ref&gt;

[[Stephen Stigler]] used a Bayesian argument to conclude that Bayes' theorem was discovered by [[Nicholas Saunderson]], a blind English mathematician, some time before Bayes;&lt;ref&gt;{{cite journal |last = Stigler |first = Stephen M. |year = 1983 |title = Who Discovered Bayes' Theorem? |journal = The American Statistician |volume = 37 |issue = 4 |pages = 290–296 |doi = 10.1080/00031305.1983.10483122 }}&lt;/ref&gt;&lt;ref name=&quot;Stats, Data and Models&quot;&gt;{{cite book |title = Stats, Data and Models |last1 = de Vaux |first1=Richard |last2=Velleman |first2=Paul |last3=Bock |first3=David |year=2016 |publisher=Pearson |isbn = 978-0-321-98649-8 |edition=4th |pages=380–381 }}&lt;/ref&gt; that interpretation, however, has been disputed.&lt;ref&gt;{{cite journal |last = Edwards |first = A. W. F. | year = 1986 | title = Is the Reference in Hartley (1749) to Bayesian Inference? |journal = The American Statistician |volume = 40 |issue = 2 |pages = 109–110 |doi = 10.1080/00031305.1986.10475370 }}&lt;/ref&gt;
Martyn Hooper&lt;ref&gt;{{cite journal |last = Hooper |first = Martyn |s2cid = 153704746 | year = 2013 |title = Richard Price, Bayes' theorem, and God |journal = Significance |volume = 10 |issue = 1 |pages = 36–39 |doi = 10.1111/j.1740-9713.2013.00638.x }}&lt;/ref&gt; and Sharon McGrayne&lt;ref name=&quot;mcgrayne2011theory&quot;&gt;{{cite book |last = McGrayne |first = S. B. |title = The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines &amp; Emerged Triumphant from Two Centuries of Controversy |url = https://archive.org/details/theorythatwouldn0000mcgr |url-access = registration |publisher=[[Yale University Press]] |year=2011 |isbn=978-0-300-18822-6 }}&lt;/ref&gt; have argued that [[Richard Price]]'s contribution was substantial:
{{block quote|By modern standards, we should refer to the Bayes–Price rule. Price discovered Bayes's work, recognized its importance, corrected it, contributed to the article, and found a use for it. The modern convention of employing Bayes's name alone is unfair but so entrenched that anything else makes little sense.&lt;ref name=&quot;mcgrayne2011theory&quot; /&gt;|sign=|source=}}

== Use in genetics ==
In genetics, Bayes' theorem can be used to calculate the probability of an individual having a specific genotype. Many people seek to approximate their chances of being affected by a genetic disease or their likelihood of being a carrier for a recessive gene of interest. A Bayesian analysis can be done based on family history or genetic testing, in order to predict whether an individual will develop a disease or pass one on to their children. Genetic testing and prediction is a common practice among couples who plan to have children but are concerned that they may both be carriers for a disease, especially within communities with low genetic variance.{{fact|date=May 2020}}

The first step in Bayesian analysis for genetics is to propose mutually exclusive hypotheses: for a specific allele, an individual either is or is not a carrier. Next, four probabilities are calculated: Prior Probability (the likelihood of each hypothesis considering information such as family history or predictions based on Mendelian Inheritance), Conditional Probability (of a certain outcome), Joint Probability (product of the first two), and Posterior Probability (a weighted product calculated by dividing the Joint Probability for each hypothesis by the sum of both joint probabilities). This type of analysis can be done based purely on family history of a condition or in concert with genetic testing.{{fact|date=May 2020}}

=== Using pedigree to calculate probabilities ===
{| class=&quot;wikitable&quot;
!Hypothesis
!Hypothesis 1: Patient is a carrier
!Hypothesis 2: Patient is not a carrier
|-
!Prior Probability
|1/2
|1/2
|-
!Conditional Probability that all four offspring will be unaffected
|(1/2) &amp;middot; (1/2) &amp;middot; (1/2) &amp;middot; (1/2) = 1/16 
|About 1
|-
!Joint Probability
|(1/2) &amp;middot; (1/16) = 1/32 
|(1/2) &amp;middot; 1 = 1/2
|-
!Posterior Probability
|(1/32) / (1/32 + 1/2) = 1/17
|(1/2) / (1/32 + 1/2) = 16/17
|}
Example of a Bayesian analysis table for a female individual's risk for a disease based on the knowledge that the disease is present in her siblings but not in her parents or any of her four children. Based solely on the status of the subject’s siblings and parents, she is equally likely to be a carrier as to be a non-carrier (this likelihood is denoted by the Prior Hypothesis). However, the probability that the subject’s four sons would all be unaffected is 1/16 (½&amp;middot;½&amp;middot;½&amp;middot;½) if she is a carrier, about 1 if she is a non-carrier (this is the Conditional Probability). The Joint Probability reconciles these two predictions by multiplying them together. The last line (the Posterior Probability) is calculated by dividing the Joint Probability for each hypothesis by the sum of both joint probabilities.&lt;ref name=&quot;Ogino et al 2004&quot;&gt;{{cite journal |last1=Ogino |first1=Shuji |last2=Wilson |first2=Robert B |last3=Gold |first3=Bert |last4=Hawley |first4=Pamela |last5=Grody |first5=Wayne W |title=Bayesian analysis for cystic fibrosis risks in prenatal and carrier screening |journal=Genetics in Medicine |date=October 2004 |volume=6 |issue=5 |pages=439–449 |doi=10.1097/01.GIM.0000139511.83336.8F |pmid=15371910 |doi-access=free }}&lt;/ref&gt;

=== Using genetic test results ===
Parental genetic testing, while still a controversial practice, can detect around 90% of known disease alleles in parents that can lead to carrier or affected status in their child. Cystic fibrosis is a heritable disease caused by an autosomal recessive mutation on the CFTR gene,&lt;ref&gt;&quot;Types of CFTR Mutations&quot;. Cystic Fibrosis Foundation, www.cff.org/What-is-CF/Genetics/Types-of-CFTR-Mutations/.&lt;/ref&gt; located on the q arm of chromosome 7.&lt;ref&gt;&quot;CFTR Gene – Genetics Home Reference&quot;. U.S. National Library of Medicine, National Institutes of Health, ghr.nlm.nih.gov/gene/CFTR#location.&lt;/ref&gt; 

Bayesian analysis of a female patient with a family history of cystic fibrosis (CF), who has tested negative for CF, demonstrating how this method was used to determine her risk of having a child born with CF: 

Because the patient is unaffected, she is either homozygous for the wild-type allele, or heterozygous. To establish prior probabilities, a Punnett square is used, based on the knowledge that neither parent was affected by the disease but both could have been carriers:
{| class=&quot;wikitable&quot; style=&quot;text-align:center;&quot;
! {{diagonal split header|&lt;br /&gt;&lt;br /&gt;Father|Mother}}
!W
Homozygous for the wild-&lt;br /&gt;type allele (a non-carrier)
!M
Heterozygous (a CF carrier)
|-
!W
Homozygous for the wild-&lt;br /&gt;type allele (a non-carrier)
|WW
|MW
|-
!M
Heterozygous (a CF carrier)
|MW
|MM

(affected by cystic fibrosis)
|}
Given that the patient is unaffected, there are only three possibilities. Within these three, there are two scenarios in which the patient carries the mutant allele. Thus the prior probabilities are ⅔ and ⅓. 

Next, the patient undergoes genetic testing and tests negative for cystic fibrosis. This test has a 90% detection rate, so the conditional probabilities of a negative test are 1/10 and 1.&amp;nbsp; Finally, the joint and posterior probabilities are calculated as before.
{| class=&quot;wikitable&quot; style=&quot;text-align:center;&quot;
!Hypothesis
!Hypothesis 1: Patient is a carrier
!Hypothesis 2: Patient is not a carrier
|-
!Prior Probability
|2/3
|1/3
|-
!Conditional Probability of a negative test
|1/10
|1
|-
!Joint Probability
|1/15
|1/3
|-
!Posterior Probability
|1/6
|5/6
|}
After carrying out the same analysis on the patient’s male partner (with a negative test result), the chances of their child being affected is equal to the product of the parents' respective posterior probabilities for being carriers times the chances that two carriers will produce an affected offspring (¼).

=== Genetic testing done in parallel with other risk factor identification. ===
Bayesian analysis can be done using phenotypic information associated with a genetic condition, and when combined with genetic testing this analysis becomes much more complicated. Cystic Fibrosis, for example, can be identified in a fetus through an ultrasound looking for an echogenic bowel, meaning one that appears brighter than normal on a scan2. This is not a foolproof test, as an echogenic bowel can be present in a&amp;nbsp; perfectly healthy fetus. Parental genetic testing is very influential in this case, where a phenotypic facet can be overly influential in probability calculation. In the case of a fetus with an echogenic bowel, with a mother who has been tested and is known to be a CF carrier, the posterior probability that the fetus actually has the disease is very high (0.64). However, once the father has tested negative for CF, the posterior probability drops significantly (to 0.16).&lt;ref name=&quot;Ogino et al 2004&quot;/&gt;

Risk factor calculation is a powerful tool in genetic counseling and reproductive planning, but it cannot be treated as the only important factor to consider. As above, incomplete testing can yield falsely high probability of carrier status, and testing can be financially inaccessible or unfeasible when a parent is not present.

==See also==
{{Portal|Mathematics}}
*[[Inductive probability]]
* [[Quantum Bayesianism]]
* [[Why Most Published Research Findings Are False]]

== Notes ==
{{NoteFoot}}

== References ==
{{Reflist}}

==Further reading==
* {{cite journal |last1=Grunau |first1=Hans-Christoph |title=Preface Issue 3/4-2013 |journal=Jahresbericht der Deutschen Mathematiker-Vereinigung |date=24 January 2014 |volume=115 |issue=3–4 |pages=127–128 |doi=10.1365/s13291-013-0077-z |doi-access=free }}
* Gelman, A, Carlin, JB, Stern, HS, and Rubin, DB (2003), &quot;Bayesian Data Analysis,&quot; Second Edition, CRC Press.
* Grinstead, CM and Snell, JL (1997), &quot;Introduction to Probability (2nd edition),&quot; American Mathematical Society (free pdf available) [http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html].
* {{springer|title=Bayes formula|id=p/b015380}}
* {{cite book |last=McGrayne |first=SB |title=The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines &amp; Emerged Triumphant from Two Centuries of Controversy |url=https://archive.org/details/theorythatwouldn0000mcgr |url-access=registration |publisher=[[Yale University Press]] |year=2011 |isbn=978-0-300-18822-6}}
* {{cite journal |last1=Laplace |first1=Pierre Simon |title=Memoir on the Probability of the Causes of Events |journal=Statistical Science |date=1986 |volume=1 |issue=3 |pages=364–378 |doi=10.1214/ss/1177013621 |jstor=2245476 |doi-access=free }}
* Lee, Peter M (2012), &quot;Bayesian Statistics: An Introduction,&quot; 4th edition. Wiley. {{ISBN|978-1-118-33257-3}}.
* {{Cite journal|date=31 March 2015|title=Bayes' theorem|journal=[[Nat. Methods|Nature Methods]]|volume=12|issue=4|pages=277–278|doi=10.1038/nmeth.3335|pmid=26005726|vauthors=Puga JL, Krzywinski M, Altman N}}
* Rosenthal, Jeffrey S (2005), &quot;Struck by Lightning: The Curious World of Probabilities&quot;. HarperCollins. (Granta, 2008. {{ISBN|9781862079960}}).
* {{cite journal |last1=Stigler |first1=Stephen M. |title=Laplace's 1774 Memoir on Inverse Probability |journal=Statistical Science |date=August 1986 |volume=1 |issue=3 |pages=359–363 |doi=10.1214/ss/1177013620 |doi-access=free }}
* Stone, JV (2013), download chapter 1 of [http://jim-stone.staff.shef.ac.uk/BookBayes2012/HTML_BayesRulev5EbookHTMLFiles/ops/xhtml/ch01BayesJVSone.html &quot;Bayes' Rule: A Tutorial Introduction to Bayesian Analysis&quot;], Sebtel Press, England.
* ''[http://santafe.edu/~simon/br.pdf Bayesian Reasoning for Intelligent People]'', An introduction and tutorial to the use of Bayes' theorem in statistics and cognitive science.
* Morris, Dan (2016), Read first 6 chapters for free of &quot;[https://web.archive.org/web/20190213034233/https://www.bayestheorem.net/ Bayes' Theorem Examples: A Visual Introduction For Beginners]&quot; Blue Windmill {{ISBN|978-1549761744}}. A short tutorial on how to understand problem scenarios and find P(B), P(A), and P(B|A).

==External links==
* {{Britannica|56808|Bayes' theorem}}
* [https://www.nytimes.com/2011/08/07/books/review/the-theory-that-would-not-die-by-sharon-bertsch-mcgrayne-book-review.html The Theory That Would Not Die by Sharon Bertsch McGrayne] New York Times Book Review by [[John Allen Paulos]] on 5 August 2011
* [https://www.youtube.com/watch?v=Zxm4Xxvzohk Visual explanation of Bayes using trees] (video)
* [https://www.youtube.com/watch?v=D8VZqxcu0I0 Bayes' frequentist interpretation explained visually] (video)
* [http://jeff560.tripod.com/b.html Earliest Known Uses of Some of the Words of Mathematics (B)]. Contains origins of &quot;Bayesian&quot;, &quot;Bayes' Theorem&quot;, &quot;Bayes Estimate/Risk/Solution&quot;, &quot;Empirical Bayes&quot;, and &quot;Bayes Factor&quot;.
* {{MathWorld | urlname=BayesTheorem | title=Bayes' Theorem}}
* {{PlanetMath | urlname=BayesTheorem | title=Bayes' theorem | id=1051}}
* [https://web.archive.org/web/20140202131520/http://rldinvestments.com/Articles/BayesTheorem.html Bayes Theorem and the Folly of Prediction]
* [http://www.celiagreen.com/charlesmccreery/statistics/bayestutorial.pdf A tutorial on probability and Bayes' theorem devised for Oxford University psychology students]
* [http://yudkowsky.net/rational/bayes An Intuitive Explanation of Bayes' Theorem by Eliezer S. Yudkowsky]
* [https://folk.universitetetioslo.no/josang/sl/Op.html Online demonstrator of the subjective Bayes' theorem]
{{DEFAULTSORT:Bayes' Theorem}}
[[Category:Bayesian statistics|Theorem]]
[[Category:Probability theorems]]
[[Category:Theorems in statistics]]</text>
      <sha1>rgmzlc6quzo0ms0kabspzybhdb4vmjv</sha1>
    </revision>
  </page>
  <page>
    <title>Bayesian inference</title>
    <ns>0</ns>
    <id>49571</id>
    <revision>
      <id>990966046</id>
      <parentid>983696320</parentid>
      <timestamp>2020-11-27T15:09:33Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor />
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 57 templates: del empty params (7×); hyphenate params (10×); del |ref=harv (3×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="61788" xml:space="preserve">{{short description|Method of statistical inference}}
{{Bayesian statistics}}
'''Bayesian inference''' is a method of [[statistical inference]] in which [[Bayes' theorem]] is used to update the probability for a hypothesis as more [[evidence]] or [[information]] becomes available. Bayesian inference is an important technique in [[statistics]], and especially in [[mathematical statistics]]. Bayesian updating is particularly important in the [[Sequential analysis|dynamic analysis of a sequence of data]]. Bayesian inference has found application in a wide range of activities, including [[science]], [[engineering]], [[philosophy]], [[medicine]], [[sport]], and [[law]]. In the philosophy of [[decision theory]], Bayesian inference is closely related to subjective probability, often called &quot;[[Bayesian probability]]&quot;.
&lt;!--
; however, non-Bayesian updating rules are compatible with rationality, according to philosophers [[Ian Hacking]] and [[Bas van&amp;nbsp;Fraassen]].&lt;ref&gt;Stanford encyclopedia of philosophy; Bayesian Epistemology; http://plato.stanford.edu/entries/epistemology-bayesian&lt;/ref&gt;&lt;ref&gt;Gillies, Donald (2000); &quot;Philosophical Theories of Probability&quot;; Routledge; Chapter 4 &quot;The subjective theory&quot;&lt;/ref&gt;
--&gt;

==Introduction to Bayes' rule==
[[File:Bayes_theorem_visualisation.svg|thumb|300px|A geometric visualisation of Bayes' theorem. In the table, the values 2, 3, 6 and 9 give the relative weights of each corresponding condition and case. The figures denote the cells of the table involved in each metric, the probability being the fraction of each figure that is shaded. This shows that P(A&lt;nowiki&gt;|&lt;/nowiki&gt;B) P(B) = P(B&lt;nowiki&gt;|&lt;/nowiki&gt;A) P(A) i.e. P(A&lt;nowiki&gt;|&lt;/nowiki&gt;B) = {{sfrac|P(B&lt;nowiki&gt;|&lt;/nowiki&gt;A) P(A)|P(B)}} . Similar reasoning can be used to show that P(&amp;not;A&lt;nowiki&gt;|&lt;/nowiki&gt;B) = {{sfrac|P(B&lt;nowiki&gt;|&lt;/nowiki&gt;&amp;not;A) P(&amp;not;A)|P(B)}} etc.]]
{{Main|Bayes' theorem}}
{{See also|Bayesian probability}}

===Formal explanation===
{| class=&quot;infobox wikitable&quot; style=&quot;font-size:100%;&quot;
|+ [[Contingency table]]
! {{diagonal split header|&lt;br /&gt;Evidence|&amp;nbsp; Hypothesis}} !! Satisfy hypothesis&lt;br /&gt;H !! Violate hypothesis&lt;br /&gt;&amp;not;H !! rowspan=&quot;5&quot; style=&quot;padding:0;&quot;| !! Total
|-
! Has evidence&lt;br /&gt;E
| |'''P(H|E)&amp;middot;P(E)'''&lt;br /&gt;= P(E|H)&amp;middot;P(H) || |'''P(&amp;not;H|E)&amp;middot;P(E)'''&lt;br /&gt;= P(E|&amp;not;H)&amp;middot;P(&amp;not;H) || '''P(E)'''
|-
! No evidence&lt;br /&gt;&amp;not;E
| |'''P(H|&amp;not;E)&amp;middot;P(&amp;not;E)'''&lt;br /&gt;= P(&amp;not;E|H)&amp;middot;P(H) || |'''P(&amp;not;H|&amp;not;E)&amp;middot;P(&amp;not;E)'''&lt;br /&gt;= P(&amp;not;E|&amp;not;H)&amp;middot;P(&amp;not;H) || '''P(&amp;not;E) =&lt;br /&gt;1&amp;minus;P(E)'''
|-
| colspan=&quot;5&quot; style=&quot;padding:0;&quot;|
|-
! Total
| &amp;nbsp;&amp;nbsp; P(H) || &amp;nbsp;&amp;nbsp; P(&amp;not;H) = 1&amp;minus;P(H) || style=&quot;text-align:center;&quot;|1
|}
Bayesian inference derives the [[posterior probability]] as a [[consequence relation|consequence]] of two [[Antecedent (logic)|antecedent]]s: a [[prior probability]] and a &quot;[[likelihood function]]&quot; derived from a [[statistical model]] for the observed data. Bayesian inference computes the posterior probability according to [[Bayes' theorem]]:
:&lt;math&gt;P(H\mid E) = \frac{P(E\mid H) \cdot P(H)}{P(E)}&lt;/math&gt;
where
* &lt;math&gt;\textstyle H&lt;/math&gt; stands for any ''hypothesis'' whose probability may be affected by [[Experimental data|data]] (called ''evidence'' below). Often there are competing hypotheses, and the task is to determine which is the most probable.
* &lt;math&gt;\textstyle P(H)&lt;/math&gt;, the ''[[prior probability]]'', is the estimate of the probability of the hypothesis &lt;math&gt;\textstyle H&lt;/math&gt; ''before'' the data &lt;math&gt;\textstyle E&lt;/math&gt;, the current evidence, is observed.
*&lt;math&gt;\textstyle E&lt;/math&gt;, the ''evidence'', corresponds to new data that were not used in computing the prior probability.
* &lt;math&gt;\textstyle P(H\mid E)&lt;/math&gt;, the ''[[posterior probability]]'', is the probability of &lt;math&gt;\textstyle H&lt;/math&gt; ''given'' &lt;math&gt;\textstyle E&lt;/math&gt;, i.e., ''after'' &lt;math&gt;\textstyle E&lt;/math&gt; is observed.  This is what we want to know: the probability of a hypothesis ''given'' the observed evidence.
* &lt;math&gt;\textstyle P(E\mid H)&lt;/math&gt; is the probability of observing &lt;math&gt;\textstyle E&lt;/math&gt; ''given'' &lt;math&gt;\textstyle H&lt;/math&gt;, and is called the ''[[Likelihood function|likelihood]]''.  As a function of &lt;math&gt;\textstyle E&lt;/math&gt; with &lt;math&gt;\textstyle H&lt;/math&gt; fixed, it indicates the compatibility of the evidence with the given hypothesis.  The likelihood function is a function of the evidence, &lt;math&gt;\textstyle E&lt;/math&gt;, while the posterior probability is a function of the hypothesis, &lt;math&gt;\textstyle H&lt;/math&gt;.
* &lt;math&gt;\textstyle P(E)&lt;/math&gt; is sometimes termed the [[marginal likelihood]] or &quot;model evidence&quot;.  This factor is the same for all possible hypotheses being considered (as is evident from the fact that the hypothesis &lt;math&gt;\textstyle H&lt;/math&gt; does not appear anywhere in the symbol, unlike for all the other factors), so this factor does not enter into determining the relative probabilities of different hypotheses.

For different values of &lt;math&gt;\textstyle H&lt;/math&gt;, only the factors &lt;math&gt;\textstyle P(H)&lt;/math&gt; and &lt;math&gt;\textstyle P(E\mid H)&lt;/math&gt;, both in the numerator, affect the value of &lt;math&gt;\textstyle P(H\mid E)&lt;/math&gt; – the posterior probability of a hypothesis is proportional to its prior probability (its inherent likeliness) and the newly acquired likelihood (its compatibility with the new observed evidence).

Bayes' rule can also be written as follows:
:&lt;math&gt;\begin{align}P(H\mid E) &amp;= \frac{P(E\mid H) P(H)}{P(E)} \\
\\
&amp;= \frac{P(E\mid H) P(H)}{P(E\mid H) P(H) + P(E\mid \neg H) P(\neg H)} \\
\\
 &amp;= \frac{1}{1 + \left(\frac{1}{P(H)}-1 \right) \frac{P(E\mid \neg H)}{P(E\mid H)} } \\
 \end{align}&lt;/math&gt;
because
:&lt;math&gt; P(E) = P(E\mid H) P(H) + P(E\mid \neg H) P(\neg H) &lt;/math&gt;
and
:&lt;math&gt; P(H) + P(\neg H) = 1 &lt;/math&gt;
where &lt;math&gt;\neg H&lt;/math&gt; is &quot;not &lt;math&gt;\textstyle H&lt;/math&gt;&quot;, the [[logical negation]] of &lt;math&gt;\textstyle H&lt;/math&gt;.

One quick and easy way to remember the equation would be to use Rule of Multiplication:

&lt;math&gt;P(E\cap H) = P(E\mid H) P(H) = P(H\mid E) P(E)&lt;/math&gt;

===Alternatives to Bayesian updating===
Bayesian updating is widely used and computationally convenient. However, it is not the only updating rule that might be considered rational.

[[Ian Hacking]] noted that traditional &quot;[[Dutch book]]&quot; arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. [[Ian Hacking|Hacking]] wrote&lt;ref&gt;{{cite journal|last =Hacking|first = Ian |date = December 1967|page = 316|title = Slightly More Realistic Personal Probability|journal = Philosophy of Science|doi = 10.1086/288169|volume =34|number = 4}}&lt;/ref&gt;&lt;ref&gt;Hacking (1988, p. 124){{full citation needed|date=April 2019}}&lt;/ref&gt; &quot;And neither the Dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour.&quot;

Indeed, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on &quot;[[probability kinematics]]&quot;) following the publication of [[Richard C. Jeffrey]]'s rule, which applies Bayes' rule to the case where the evidence itself is assigned a probability.&lt;ref&gt;{{cite web|url=http://plato.stanford.edu/entries/bayes-theorem/ |title=Bayes' Theorem (Stanford Encyclopedia of Philosophy) |publisher=Plato.stanford.edu |access-date=2014-01-05}}&lt;/ref&gt; The additional hypotheses needed to uniquely require Bayesian updating have been deemed to be substantial, complicated, and unsatisfactory.&lt;ref&gt;[[Bas van Fraassen|van Fraassen, B.]] (1989) ''Laws and Symmetry'', Oxford University Press. {{ISBN|0-19-824860-1}}&lt;/ref&gt;

==Formal description of Bayesian inference==

===Definitions===
*&lt;math&gt;x&lt;/math&gt;, a data point in general.  This may in fact be a [[random vector|vector]] of values.
*&lt;math&gt;\theta&lt;/math&gt;, the [[parameter]] of the data point's distribution, i.e., &lt;math&gt;x \sim p(x \mid \theta)&lt;/math&gt; .  This may in fact be a [[random vector|vector]] of parameters.
*&lt;math&gt;\alpha&lt;/math&gt;, the [[hyperparameter]] of the parameter distribution, i.e., &lt;math&gt;\theta \sim p(\theta \mid \alpha)&lt;/math&gt; .  This may in fact be a [[random vector|vector]] of hyperparameters.
*&lt;math&gt;\mathbf{X}&lt;/math&gt; is the sample, a set of &lt;math&gt;n&lt;/math&gt; observed data points, i.e., &lt;math&gt;x_1,\ldots,x_n&lt;/math&gt;.
*&lt;math&gt;\tilde{x}&lt;/math&gt;, a new data point whose distribution is to be predicted.

===Bayesian inference===

*The [[prior distribution]] is the distribution of the parameter(s) before any data is observed, i.e. &lt;math&gt;p(\theta \mid \alpha)&lt;/math&gt; . The prior distribution might not be easily determined; in such a case, one possibility may be to use the [[Jeffreys prior]] to obtain a prior distribution before updating it with newer observations.
*The [[sampling distribution]] is the distribution of the observed data conditional on its parameters, i.e. &lt;math&gt;p(\mathbf{X} \mid \theta)&lt;/math&gt; .  This is also termed the [[likelihood function|likelihood]], especially when viewed as a function of the parameter(s), sometimes written &lt;math&gt;\operatorname{L}(\theta  \mid \mathbf{X}) = p(\mathbf{X} \mid \theta)&lt;/math&gt; .
*The [[marginal likelihood]] (sometimes also termed the ''evidence'') is the distribution of the observed data [[marginal distribution|marginalized]] over the parameter(s), i.e. &lt;math&gt;p(\mathbf{X} \mid \alpha) = \int p(\mathbf{X} \mid \theta) p(\theta \mid \alpha) \operatorname{d}\!\theta&lt;/math&gt; .
*The [[posterior distribution]] is the distribution of the parameter(s) after taking into account the observed data.  This is determined by [[Bayes' rule]], which forms the heart of Bayesian inference:
:&lt;math&gt;p(\theta \mid \mathbf{X},\alpha) = \frac{p(\theta,\mathbf{X},\alpha)}{p(\mathbf{X},\alpha)} = \frac{p(\mathbf{X}\mid\theta,\alpha)p(\theta,\alpha)}{p(\mathbf{X}\mid\alpha)p(\alpha)}
= \frac{p(\mathbf{X} \mid \theta,\alpha) p(\theta \mid \alpha)}{p(\mathbf{X} \mid \alpha)} \propto p(\mathbf{X} \mid \theta,\alpha) p(\theta \mid \alpha)&lt;/math&gt;

This is expressed in words as &quot;posterior is proportional to likelihood times prior&quot;, or sometimes as &quot;posterior = likelihood times prior, over evidence&quot;.

===Bayesian prediction===

*The [[posterior predictive distribution]] is the distribution of a new data point, marginalized over the posterior:
:&lt;math&gt;p(\tilde{x} \mid \mathbf{X},\alpha) = \int p(\tilde{x} \mid \theta) p(\theta \mid \mathbf{X},\alpha) \operatorname{d}\!\theta&lt;/math&gt;
*The [[prior predictive distribution]] is the distribution of a new data point, marginalized over the prior:
:&lt;math&gt;p(\tilde{x} \mid \alpha) = \int p(\tilde{x} \mid \theta) p(\theta \mid \alpha) \operatorname{d}\!\theta&lt;/math&gt;

Bayesian theory calls for the use of the posterior predictive distribution to do [[predictive inference]], i.e., to [[prediction|predict]] the distribution of a new, unobserved data point. That is, instead of a fixed point as a prediction, a distribution over possible points is returned.  Only this way is the entire posterior distribution of the parameter(s) used.  By comparison, prediction in [[frequentist statistics]] often involves finding an optimum point estimate of the parameter(s)—e.g., by [[maximum likelihood]] or [[maximum a posteriori estimation]] (MAP)—and then plugging this estimate into the formula for the distribution of a data point. This has the disadvantage that it does not account for any uncertainty in the value of the parameter, and hence will underestimate the [[variance]] of the predictive distribution.

(In some instances, frequentist statistics can work around this problem. For example, [[confidence interval]]s and [[prediction interval]]s in frequentist statistics when constructed from a [[normal distribution]] with unknown [[mean]] and [[variance]] are constructed using a [[Student's t-distribution]].  This correctly estimates the variance, due to the fact that (1)&amp;nbsp;the average of normally distributed random variables is also normally distributed; (2)&amp;nbsp;the predictive distribution of a normally distributed data point with unknown mean and variance, using conjugate or uninformative priors, has a student's t-distribution. In Bayesian statistics, however, the posterior predictive distribution can always be determined exactly—or at least, to an arbitrary level of precision, when numerical methods are used.)

Both types of predictive distributions have the form of a [[compound probability distribution]] (as does the [[marginal likelihood]]). In fact, if the prior distribution is a [[conjugate prior]], and hence the prior and posterior distributions come from the same family, it can easily be seen that both prior and posterior predictive distributions also come from the same family of compound distributions. The only difference is that the posterior predictive distribution uses the updated values of the hyperparameters (applying the Bayesian update rules given in the [[conjugate prior]] article), while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution.

==Inference over exclusive and exhaustive possibilities==
If evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions, Bayesian inference may be thought of as acting on this belief distribution as a whole.

===General formulation===
[[File:Bayesian inference event space.svg|thumb|Diagram illustrating event space &lt;math&gt;\Omega&lt;/math&gt; in general formulation of Bayesian inference. Although this diagram shows discrete models and events, the continuous case may be visualized similarly using probability densities.]]

&lt;!-- This section is not clear as it now stands. --&gt;
Suppose a process is generating independent and identically distributed events &lt;math&gt;E_n, \,\, n=1,2,3,\ldots&lt;/math&gt;, but the probability distribution is unknown. Let the event space &lt;math&gt;\Omega&lt;/math&gt; represent the current state of belief for this process. Each model is represented by event &lt;math&gt;M_m&lt;/math&gt;. The conditional probabilities &lt;math&gt;P(E_n \mid M_m)&lt;/math&gt; are specified to define the models. &lt;math&gt;P(M_m)&lt;/math&gt; is the degree of belief in &lt;math&gt;M_m&lt;/math&gt;. Before the first inference step, &lt;math&gt;\{P(M_m)\}&lt;/math&gt; is a set of ''initial prior probabilities''. These must sum to 1, but are otherwise arbitrary.

Suppose that the process is observed to generate &lt;math&gt;\textstyle E \in \{E_n\}&lt;/math&gt;. For each &lt;math&gt;M \in \{M_m\}&lt;/math&gt;, the prior &lt;math&gt;P(M)&lt;/math&gt; is updated to the posterior &lt;math&gt;P(M \mid E)&lt;/math&gt;. From [[Bayes' theorem]]:&lt;ref&gt;Gelman, Andrew; Carlin, John B.; Stern, Hal S.; Dunson, David B.;Vehtari, Aki; Rubin, Donald B. (2013). ''Bayesian Data Analysis'', Third Edition. Chapman and Hall/CRC. {{ISBN|978-1-4398-4095-5}}.&lt;/ref&gt;

:&lt;math&gt;P(M \mid E) = \frac{P(E \mid M)}{\sum_m {P(E \mid M_m) P(M_m)}} \cdot P(M)&lt;/math&gt;

Upon observation of further evidence, this procedure may be repeated.

===Multiple observations===

For a sequence of [[independent and identically distributed]] observations &lt;math&gt;\mathbf{E} = (e_1, \dots, e_n)&lt;/math&gt;, it can be shown by induction that repeated application of the above is equivalent to

:&lt;math&gt;P(M \mid \mathbf{E}) = \frac{P(\mathbf{E} \mid M)}{\sum_m {P(\mathbf{E} \mid M_m) P(M_m)}} \cdot P(M)&lt;/math&gt;

Where

:&lt;math&gt;P(\mathbf{E} \mid M) = \prod_k{P(e_k \mid M)}.&lt;/math&gt;
