Let &lt;math&gt;f: X \times Y \rightarrow Z&lt;/math&gt; where we assume in the typical case that &lt;math&gt; X=Y=\{0,1\}^n &lt;/math&gt; and &lt;math&gt; Z=\{0,1\}&lt;/math&gt;. Alice holds an &lt;math&gt;n&lt;/math&gt;-bit string  &lt;math&gt;x \in X&lt;/math&gt; while Bob holds an &lt;math&gt;n&lt;/math&gt;-bit string  &lt;math&gt;y \in Y&lt;/math&gt;. By communicating to each other one [[bit]] at a time (adopting some [[Protocol (computing)|''communication protocol'']] which is agreed upon in advance), Alice and Bob wish to compute the value of &lt;math&gt;f(x,y)&lt;/math&gt; such that at least one party knows the value at the end of the communication. At this point the answer can be communicated back so that at the cost of one extra bit, both parties will know the answer. The worst case communication complexity of this communication problem of computing &lt;math&gt;f&lt;/math&gt;, denoted as &lt;math&gt; D(f) &lt;/math&gt;, is then defined to be

:&lt;math&gt; D(f) = &lt;/math&gt; minimum number of bits exchanged between Alice and Bob in the worst case.

As observed above, for any function &lt;math&gt;f: \{0, 1\}^n \times \{0, 1\}^n \rightarrow \{0, 1\}&lt;/math&gt;, we have &lt;math&gt;D(f) \leq n&lt;/math&gt;.
Using the above definition, it is useful to think of the function &lt;math&gt;f&lt;/math&gt; as a [[Matrix (mathematics)|matrix]] &lt;math&gt;A&lt;/math&gt; (called the ''input matrix'' or ''communication matrix'') where the rows are indexed by &lt;math&gt;x \in X&lt;/math&gt; and columns by &lt;math&gt;y \in Y&lt;/math&gt;. The entries of the matrix are &lt;math&gt;A_{x,y}=f(x,y)&lt;/math&gt;. Initially both Alice and Bob have a copy of the entire matrix &lt;math&gt;A&lt;/math&gt; (assuming the function &lt;math&gt;f&lt;/math&gt; is known to both parties). Then, the problem of computing the function value can be rephrased as &quot;zeroing-in&quot; on the corresponding matrix entry. This problem can be solved if either Alice or Bob knows both &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;. At the start of communication, the number of choices for the value of the function on the inputs is the size of matrix, i.e. &lt;math&gt;2^{2n}&lt;/math&gt;. Then, as and when each party communicates a bit to the other, the number of choices for the answer reduces as this eliminates a set of rows/columns resulting in a [[submatrix]] of &lt;math&gt;A&lt;/math&gt;.

More formally, a set &lt;math&gt;R \subseteq X \times Y&lt;/math&gt; is called a ''(combinatorial) rectangle'' if whenever &lt;math&gt;(x_1,y_1) \in R&lt;/math&gt; and &lt;math&gt;(x_2,y_2) \in R&lt;/math&gt; then &lt;math&gt;(x_1,y_2) \in R&lt;/math&gt;. Equivalently, &lt;math&gt;R&lt;/math&gt; is a combinatorial rectangle if it can be expressed as &lt;math&gt;R = M \times N&lt;/math&gt; for some &lt;math&gt;M \subseteq X&lt;/math&gt; and &lt;math&gt;N \subseteq Y&lt;/math&gt;. Consider the case when &lt;math&gt;k&lt;/math&gt; bits are already exchanged between the parties. Now, for a particular &lt;math&gt;h \in \{0,1\}^k&lt;/math&gt;, let us define a matrix

:&lt;math&gt;T_{h} = \{ (x, y) : \text{ the }k\text{-bits exchanged on input } (x , y) \text{ is }h\}&lt;/math&gt;

Then, &lt;math&gt;T_{h} \subseteq X \times Y&lt;/math&gt;, and it is not hard to show that &lt;math&gt;T_{h}&lt;/math&gt; is a combinatorial rectangle in &lt;math&gt;A&lt;/math&gt;.

=== Example: &lt;math&gt;EQ&lt;/math&gt; ===

We consider the case where Alice and Bob try to determine whether or not their input strings are equal.  Formally, define the ''Equality'' function, denoted &lt;math&gt;EQ : \{0, 1\}^n \times \{0, 1\}^n \rightarrow \{0, 1\}&lt;/math&gt;, by &lt;math&gt;EQ(x, y) = 1&lt;/math&gt; iff &lt;math&gt;x = y&lt;/math&gt;.  As we demonstrate below, any deterministic communication protocol solving &lt;math&gt;EQ&lt;/math&gt; requires &lt;math&gt;n&lt;/math&gt; bits of communication in the worst case.  As a warm-up example, consider the simple case of &lt;math&gt;x, y \in \{0, 1\}^3&lt;/math&gt;.  The equality function in this case can be represented by the matrix below.  The rows represent all the possibilities of &lt;math&gt;x&lt;/math&gt;, the columns those of &lt;math&gt;y&lt;/math&gt;.

{| class=&quot;wikitable&quot; style=&quot;font-family: monospace; text-align: right; margin-left: auto; margin-right: auto; border: none;&quot;
! EQ
! 000
! 001
! 010
! 011
! 100
! 101
! 110
! 111
|-
! 000
| 1
| 0
| 0
| 0
| 0
| 0
| 0
| 0
|-
! 001
| 0
| 1
| 0
| 0
| 0
| 0
| 0
| 0
|-
! 010
| 0
| 0
| 1
| 0
| 0
| 0
| 0
| 0
|-
! 011
| 0
| 0
| 0
| 1
| 0
| 0
| 0
| 0
|-
! 100
| 0
| 0
| 0
| 0
| 1
| 0
| 0
| 0
|-
! 101
| 0
| 0
| 0
| 0
| 0
| 1
| 0
| 0
|-
! 110
| 0
| 0
| 0
| 0
| 0
| 0
| 1
| 0
|-
! 111
| 0
| 0
| 0
| 0
| 0
| 0
| 0
| 1
|-
|}

As you can see, the function only evaluates to 1 when &lt;math&gt;x&lt;/math&gt; equals &lt;math&gt;y&lt;/math&gt; (i.e., on the diagonal).  It is also fairly easy to see how communicating a single bit divides your possibilities in half.  If you know that the first bit of &lt;math&gt;y&lt;/math&gt; is 1, you only need to consider half of the columns (where &lt;math&gt;y&lt;/math&gt; can equal 100, 101, 110, or 111).

=== Theorem: &lt;math&gt;D(EQ) = n&lt;/math&gt; ===

Proof. Assume that &lt;math&gt;D(EQ) \leq n-1&lt;/math&gt;.  This means that there exists &lt;math&gt;x \neq x'&lt;/math&gt; such that &lt;math&gt;(x, x)&lt;/math&gt; and &lt;math&gt;(x', x')&lt;/math&gt; that have the same communication transcript &lt;math&gt;h&lt;/math&gt;.  Since this transcript defines a rectangle, &lt;math&gt;f(x, x')&lt;/math&gt; must also be 1.  By definition &lt;math&gt;x \neq x'&lt;/math&gt; and we know that equality is only true for &lt;math&gt;(a, b)&lt;/math&gt; when &lt;math&gt;a = b&lt;/math&gt;.  This yields a contradiction.

This technique of proving deterministic communication lower bounds is called the ''fooling set'' technique.&lt;ref name=&quot;:0&quot;&gt;{{cite book| last1=Kushilevitz | first1=Eyal
| last2=Nisan | first2=Noam  | author-link2=Noam Nisan
| title=Communication Complexity
| publisher=Cambridge University Press
| year=1997
| isbn=978-0-521-56067-2
}}&lt;/ref&gt;

== Randomized communication complexity ==
In the above definition, we are concerned with the number of bits that must be ''deterministically'' transmitted between two parties. If both the parties are given access to a random number generator, can they determine the value of &lt;math&gt;f&lt;/math&gt; with much less information exchanged? Yao, in his seminal paper&lt;ref name=yao1979/&gt;
answers this question by defining randomized communication complexity.

A randomized protocol &lt;math&gt;R&lt;/math&gt; for a function &lt;math&gt;f&lt;/math&gt; has two-sided error.

:&lt;math&gt;
\Pr[R(x,y) = 0] &gt; \frac{2}{3}, \textrm{if }\, f(x,y) = 0
&lt;/math&gt;

:&lt;math&gt;
\Pr[R(x,y) = 1] &gt; \frac{2}{3}, \textrm{if }\, f(x,y) = 1
&lt;/math&gt;

A randomized protocol is a deterministic protocol that uses an extra random string in addition to its normal input. There are two models for this: a ''public string'' is a random string that is known by both parties beforehand, while a ''private string'' is generated by one party and must be communicated to the other party. A theorem presented below shows that any public string protocol can be simulated by a private string protocol that uses ''O(log n)'' additional bits compared to the original.

Note that in the probability inequalities above, the outcome of the protocol is understood to depend ''only'' on the random string; both strings ''x'' and ''y'' remain fixed. In other words, if R(x,y) yields g(x,y,r) when using random string ''r'', then g(x,y,r) = f(x,y) for at least 2/3 of all choices for the string ''r''.

The randomized complexity is simply defined as the number of bits exchanged in such a protocol.

Note that it is also possible to define a randomized protocol with one-sided error, and the complexity is defined similarly.

=== Example: EQ ===

Returning to the previous example of ''EQ'', if certainty is not required, Alice and Bob can check for equality using only {{tmath|O(\log n)}} messages.  Consider the following protocol:  Assume that Alice and Bob both have access to the same random string &lt;math&gt;z \in \{0,1\}^n&lt;/math&gt;. Alice computes &lt;math&gt;z \cdot x&lt;/math&gt; and sends this bit (call it ''b'') to Bob. (The &lt;math&gt;(\cdot)&lt;/math&gt; is the [[dot product]] in [[finite field#Some small finite fields|GF(2)]].) Then Bob compares ''b'' to &lt;math&gt;z \cdot y&lt;/math&gt;. If they are the same, then Bob accepts, saying ''x'' equals ''y''. Otherwise, he rejects.

Clearly, if &lt;math&gt;x = y&lt;/math&gt;, then &lt;math&gt;z \cdot x = z \cdot y&lt;/math&gt;, so &lt;math&gt;Prob_z[Accept] = 1&lt;/math&gt;. If ''x'' does not equal ''y'', it is still possible that &lt;math&gt;z \cdot x = z \cdot y&lt;/math&gt;, which would give Bob the wrong answer. How does this happen?

If ''x'' and ''y'' are not equal, they must differ in some locations:

:&lt;math&gt;\begin{cases}
x = c_1 c_2 \ldots p   \ldots p'  \ldots x_n \\
y = c_1 c_2 \ldots q   \ldots q'  \ldots y_n \\
z = z_1 z_2 \ldots z_i \ldots z_j \ldots z_n 
\end{cases}&lt;/math&gt;

Where {{mvar|x}} and {{mvar|y}} agree, &lt;math&gt;z_i * x_i = z_i * c_i = z_i * y_i&lt;/math&gt; so those terms affect the dot products equally. We can safely ignore those terms and look only at where {{mvar|x}} and {{mvar|y}} differ. Furthermore, we can swap the bits &lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;y_i&lt;/math&gt; without changing whether or not the dot products are equal. This means we can swap bits so that {{mvar|x}} contains only zeros and {{mvar|y}} contains only ones:

:&lt;math&gt;\begin{cases}
x' = 0   0   \ldots 0   \\
y' = 1   1   \ldots 1   \\
z' = z_1 z_2 \ldots z_{n'} 
\end{cases}&lt;/math&gt;

Note that &lt;math&gt;z' \cdot x' = 0&lt;/math&gt; and &lt;math&gt;z' \cdot y' = \Sigma_i z'_i&lt;/math&gt;. Now, the question becomes: for some random string &lt;math&gt;z'&lt;/math&gt;, what is the probability that &lt;math&gt;\Sigma_i z'_i = 0&lt;/math&gt;?  Since each &lt;math&gt;z'_i&lt;/math&gt; is equally likely to be {{val|0}} or {{val|1}}, this probability is just &lt;math&gt;1/2&lt;/math&gt;. Thus, when {{mvar|x}} does not equal {{mvar|y}},
&lt;math&gt;Prob_z[Accept] = 1/2&lt;/math&gt;. The algorithm can be repeated many times to increase its accuracy. This fits the requirements for a randomized communication algorithm.

This shows that ''if Alice and Bob share a random string of length n'', they can send one bit to each other to compute &lt;math&gt;EQ(x,y)&lt;/math&gt;. In the next section, it is shown that Alice and Bob can exchange only {{tmath|O(\log n)}} bits that are as good as sharing a random string of length ''n''. Once that is shown, it follows that ''EQ'' can be computed in {{tmath|O(\log n)}} messages.

=== Example: GH ===
For yet another example of randomized communication complexity, we turn to an example known as the ''[[gap-Hamming problem]]'' (abbreviated ''GH''). Formally, Alice and Bob both maintain binary messages, &lt;math&gt;x,y \in \{-1, +1\}^n&lt;/math&gt; and would like to determine if the strings are very similar or if they are not very similar. In particular, they would like to find a communication protocol requiring the transmission of as few bits as possible to compute the following partial Boolean function, 

:&lt;math&gt;
\text{GH}_n(x, y) := 
\begin{cases}
-1 &amp; \langle x, y \rangle \leq -\sqrt{n} \\
+1 &amp; \langle x, y \rangle \geq \sqrt{n}. 
\end{cases} 
&lt;/math&gt; 

Clearly, they must communicate all their bits if the protocol is to be deterministic (this is because, if there is a deterministic, strict subset of indices that Alice and Bob relay to one another, then imagine having a pair of strings that on that set disagree in &lt;math&gt;\sqrt{n} - 1&lt;/math&gt; positions. If another disagreement occurs in any position that is not relayed, then this affects the result of &lt;math&gt; \text{GH}_n(x, y)&lt;/math&gt;, and hence would result in an incorrect procedure. 

A natural question one then asks is, if we're permitted to err &lt;math&gt;1/3&lt;/math&gt; of the time (over random instances &lt;math&gt; x, y&lt;/math&gt; drawn uniformly at random from &lt;math&gt; \{-1, +1\}^n &lt;/math&gt;), then can we get away with a protocol with fewer bits? It turns out that the answer somewhat surprisingly is no, due to a result of Chakrabarti and Regev in 2012: they show that for random instances, any procedure which is correct at least &lt;math&gt;2/3&lt;/math&gt; of the time must send &lt;math&gt;\Omega(n)&lt;/math&gt; bits worth of communication, which is to say essentially all of them.

=== Public coins versus private coins ===

It is easier to create random protocols when both parties have access to the same random string (shared string protocol). It is still possible to use these protocols even when the two parties don't share a random string (private string protocol) with a small communication cost.  Any shared string random protocol using any number of random string can be simulated by a private string protocol that uses an extra ''O(log n)'' bits.

Intuitively, we can find some set of strings that has enough randomness in it to run the random protocol with only a small increase in error.  This set can be shared beforehand, and instead of drawing a random string, Alice and Bob need only agree on which string to choose from the shared set. This set is small enough that the choice can be communicated efficiently. A formal proof follows.

Consider some random protocol ''P'' with a maximum error rate of 0.1. Let &lt;math&gt;R&lt;/math&gt; be &lt;math&gt;100n&lt;/math&gt; strings of length ''n'', numbered &lt;math&gt;r_1, r_2, \dots, r_{100n}&lt;/math&gt;. Given such an &lt;math&gt;R&lt;/math&gt;, define a new protocol &lt;math&gt;P'_R&lt;/math&gt; which randomly picks some &lt;math&gt;r_i&lt;/math&gt; and then runs ''P'' using &lt;math&gt;r_i&lt;/math&gt; as the shared random string. It takes ''O''(log&amp;nbsp;100''n'') = ''O''(log&amp;nbsp;''n'') bits to communicate the choice of &lt;math&gt;r_i&lt;/math&gt;.

Let us define &lt;math&gt;p(x,y)&lt;/math&gt; and &lt;math&gt;p'_R(x,y)&lt;/math&gt; to be the probabilities that &lt;math&gt;P&lt;/math&gt; and &lt;math&gt;P'_R&lt;/math&gt;  compute the correct value for the input &lt;math&gt;(x,y)&lt;/math&gt;.

For a fixed &lt;math&gt;(x,y)&lt;/math&gt;, we can use [[Hoeffding's inequality]] to get the following equation:

:&lt;math&gt;\Pr_R[|p'_R(x,y) - p(x,y)| \geq 0.1] \leq 2 \exp(-2(0.1)^2 \cdot 100n) &lt; 2^{-2n}&lt;/math&gt;

Thus when we don't have &lt;math&gt;(x,y)&lt;/math&gt; fixed:

:&lt;math&gt;\Pr_R[\exists (x,y):\, |p'_R(x,y) - p(x,y)| \geq 0.1] \leq \sum_{(x,y)} \Pr_R[|p'_R(x,y) - p(x,y)| \geq 0.1] &lt; \sum_{(x,y)} 2^{-2n} = 1&lt;/math&gt;

The last equality above holds because there are &lt;math&gt;2^{2n}&lt;/math&gt; different pairs &lt;math&gt;(x,y)&lt;/math&gt;. Since the probability does not equal 1, there is some &lt;math&gt;R_0&lt;/math&gt; so that for all &lt;math&gt;(x,y)&lt;/math&gt;:

:&lt;math&gt;|p'_{R_0}(x,y) - p(x,y)| &lt; 0.1&lt;/math&gt;

Since &lt;math&gt;P&lt;/math&gt; has at most 0.1 error probability, &lt;math&gt;P'_{R_0}&lt;/math&gt; can have at most 0.2 error probability.

== Quantum communication complexity ==

Quantum communication complexity tries to quantify the communication reduction possible by using quantum effects during a distributed computation.

At least three quantum generalizations of communication complexity have been proposed; for a survey see the suggested text by G. Brassard.

The first one is the [[Quantum entanglement|qubit-communication model]], where the parties can use quantum communication instead of classical communication, for example by exchanging [[photons]] through an [[optical fiber]].
