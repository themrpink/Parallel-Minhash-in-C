===Writer and director===
{| class=&quot;wikitable&quot;
|-
! Year
! Title
! Notes
|-
| 1995
| ''[[L'aube à l'envers]]''
|
|-
| 2002
| ''[[Speak to Me of Love]]''
| ''Parlez-moi d'amour''
|-
| 2007
| ''[[Trivial (film)|Trivial]]''
| ''La disparue de Deauville''
|}

==References==
{{Reflist|30em}}

== Further reading ==
* Frédéric Quinonero: ''Sophie Marceau – La belle échappée.'' Éditions Didier Carpentier, 2010
*''Sophie Marceau - Le cinéma au féminin''. JFN Kiosque (special edition), 2009 ([https://books.google.com/books?id=zB-m_s0sNe8C excerpts (Google)])

==External links==
{{Commons category|Sophie Marceau}}
*{{IMDb name|0000521}}
*{{AllRovi person|45296}}
*[https://www.youtube.com/watch?v=TU8x9tpc-Bg YouTube Portrait]

{{Sophie Marceau}}
{{Navboxes
|title = Awards for Sophie Marceau
|list =
{{Cabourg Film Festival Best Actress Award}}
{{César Award for Most Promising Actress}}
}}
{{Authority control}}

{{DEFAULTSORT:Marceau, Sophie}}
[[Category:1966 births]]
[[Category:Living people]]
[[Category:Actresses from Paris]]
[[Category:French child actresses]]
[[Category:French film actresses]]
[[Category:French film directors]]
[[Category:French women screenwriters]]
[[Category:French screenwriters]]
[[Category:20th-century French actresses]]
[[Category:21st-century French actresses]]
[[Category:French women film directors]]
[[Category:Alumni of the Cours Florent]]
[[Category:Légion d'honneur refusals]]
[[Category:Most Promising Actress César Award winners]]</text>
      <sha1>bb4vw69f7mazppn4pgkbre9ga2r8trb</sha1>
    </revision>
  </page>
  <page>
    <title>Speech synthesis</title>
    <ns>0</ns>
    <id>42799</id>
    <revision>
      <id>991653118</id>
      <parentid>991653102</parentid>
      <timestamp>2020-12-01T04:10:28Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/140.213.33.46|140.213.33.46]] to version by Monkbot. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (3835933) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="80356" xml:space="preserve">{{See also|Speech-generating device}}
{{short description|Artificial production of human speech}}

'''Speech synthesis''' is the artificial production of human [[speech]]. A computer system used for this purpose is called a '''speech computer''' or '''speech synthesizer''', and can be implemented in [[software]] or [[Computer hardware|hardware]] products. A '''text-to-speech''' ('''TTS''') system converts normal language text into speech; other systems render [[symbolic linguistic representation]]s like [[phonetic transcription]]s into speech.&lt;ref&gt;{{Cite book |first1=Jonathan |last1=Allen |first2=M. Sharon |last2=Hunnicutt |first3=Dennis |last3=Klatt |title=From Text to Speech: The MITalk system |publisher=Cambridge University Press |year=1987 |isbn=978-0-521-30641-6 |url-access=registration |url=https://archive.org/details/fromtexttospeech00alle }}&lt;/ref&gt;

Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a [[database]]. Systems differ in the size of the stored speech units; a system that stores [[phone (phonetics)|phones]] or [[diphone]]s provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the [[vocal tract]] and other human voice characteristics to create a completely &quot;synthetic&quot; voice output.&lt;ref&gt;{{Cite journal | doi = 10.1121/1.386780 | last1 = Rubin | first1 = P. | last2 = Baer | first2 = T. | last3 = Mermelstein | first3 = P. | year = 1981 | title = An articulatory synthesizer for perceptual research | journal = Journal of the Acoustical Society of America | volume = 70 | issue = 2| pages = 321–328 | bibcode = 1981ASAJ...70..321R }}&lt;/ref&gt;

The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with [[visual impairment]]s or [[reading disability|reading disabilities]] to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.

[[File:TTS System.svg|550px|thumb|Overview of a typical TTS system]]
{{listen
| filename     = JärDa-utrop.ogg
| title        = Automatic announcement
| description  = A synthetic voice announcing an arriving train in [[Sweden]].
| format       = [[Ogg]]
}}

A text-to-speech system (or &quot;engine&quot;) is composed of two parts:&lt;ref&gt;{{Cite book |first1=Jan P. H. |last1=van Santen |first2=Richard W. |last2=Sproat |first3=Joseph P. |last3=Olive |first4=Julia |last4=Hirschberg |title=Progress in Speech Synthesis |publisher=Springer |year=1997 |isbn=978-0-387-94701-3 |url-access=registration |url=https://archive.org/details/progressinspeech0000unse }}&lt;/ref&gt; a [[Input method|front-end]] and a [[Front and back ends|back-end]]. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called ''[[text normalization]]'', ''pre-processing'', or ''[[Tokenization (lexical analysis)|tokenization]]''. The front-end then assigns [[phonetic transcription]]s to each word, and divides and marks the text into [[prosody (linguistics)|prosodic units]], like [[phrase]]s, [[clause]]s, and [[sentence (linguistics)|sentence]]s. The process of assigning phonetic transcriptions to words is called ''text-to-phoneme'' or ''[[grapheme]]-to-phoneme'' conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end—often referred to as the ''synthesizer''—then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the ''target prosody'' (pitch contour, phoneme durations),&lt;ref&gt;{{Cite journal
| last1 = Van Santen | first1 = J.
| title = Assignment of segmental duration in text-to-speech synthesis
| doi = 10.1006/csla.1994.1005
| journal = Computer Speech &amp; Language
| volume = 8
| issue = 2
| pages = 95–128
|date=April 1994
}}&lt;/ref&gt; which is then imposed on the output speech.

== History ==

Long before the invention of [[electronics|electronic]] [[signal processing]], some people tried to build machines to emulate human speech. Some early legends of the existence of &quot;[[Brazen Head]]s&quot; involved Pope [[Silvester II]] (d. 1003 AD), [[Albertus Magnus]] (1198–1280), and [[Roger Bacon]] (1214–1294).

In 1779 the [[Germany|German]]-[[Denmark|Danish]] scientist [[Christian Gottlieb Kratzenstein]] won the first prize in a competition announced by the Russian [[Russian Academy of Sciences|Imperial Academy of Sciences and Arts]] for models he built of the human [[vocal tract]] that could produce the five long [[vowel]] sounds (in [[help:IPA|International Phonetic Alphabet]] notation: {{IPA|[aː]}}, {{IPA|[eː]}}, {{IPA|[iː]}}, {{IPA|[oː]}} and {{IPA|[uː]}}).&lt;ref name=&quot;Helsinki&quot;&gt;[http://www.acoustics.hut.fi/publications/files/theses/lemmetty_mst/chap2.html History and Development of Speech Synthesis], Helsinki University of Technology, Retrieved on November 4, 2006&lt;/ref&gt; There followed the [[bellows]]-operated &quot;[[Wolfgang von Kempelen's Speaking Machine|acoustic-mechanical speech machine]]&quot; of [[Wolfgang von Kempelen]] of [[Pressburg]], [[Hungary]], described in a 1791 paper.&lt;ref&gt;''Mechanismus der menschlichen Sprache nebst der Beschreibung seiner sprechenden Maschine'' (&quot;Mechanism of the human speech with description of its speaking machine&quot;, J. B. Degen, Wien). {{in lang|de}}&lt;/ref&gt; This machine added models of the tongue and lips, enabling it to produce [[consonant]]s as well as vowels. In 1837, [[Charles Wheatstone]] produced a &quot;speaking machine&quot; based on von Kempelen's design, and in 1846, Joseph Faber exhibited the &quot;[[Euphonia (device)|Euphonia]]&quot;. In 1923 Paget resurrected Wheatstone's design.&lt;ref&gt;{{Cite journal|last=Mattingly|first=Ignatius G.|year=1974|editor1-last=Sebeok|editor1-first=Thomas A.|title=Speech synthesis for phonetic and phonological models|url=http://www.haskins.yale.edu/Reprints/HL0173.pdf|url-status=dead|journal=Current Trends in Linguistics|location=Mouton, The Hague|volume=12|pages=2451–2487|archive-url=https://web.archive.org/web/20130512085755/http://www.haskins.yale.edu/Reprints/HL0173.pdf|archive-date=2013-05-12|access-date=2011-12-13}}&lt;/ref&gt;

In the 1930s [[Bell Labs]] developed the [[vocoder]],  which automatically analyzed speech into its fundamental tones and resonances. From his work on the vocoder, [[Homer Dudley]] developed a keyboard-operated voice-synthesizer called [[The Voder]] (Voice Demonstrator), which he exhibited at the [[1939 New York World's Fair]].

[[Franklin S. Cooper|Dr. Franklin S. Cooper]] and his colleagues at [[Haskins Laboratories]] built the [[Pattern playback]] in the late 1940s and completed it in 1950. There were several different versions of this hardware device; only one currently survives. The machine converts pictures of the acoustic patterns of speech in the form of a spectrogram back into sound. Using this device, [[Alvin Liberman]] and colleagues discovered acoustic cues for the perception of [[phonetic]] segments (consonants and vowels).

=== Electronic devices ===
[[File:Computer and speech synthesiser housing, 19 (9663804888).jpg|thumb| Computer and speech synthesiser housing used by [[Stephen Hawking]] in 1999]]
The first computer-based speech-synthesis systems originated in the late 1950s. Noriko Umeda ''et al.'' developed the first general English text-to-speech system in 1968, at the [[Electrotechnical Laboratory]] in Japan.&lt;ref&gt;{{cite journal | last1 = Klatt | first1 = D | year = 1987 | title = Review of text-to-speech conversion for English | journal = Journal of the Acoustical Society of America | volume = 82 | issue = 3| pages = 737–93 | doi= 10.1121/1.395275| pmid = 2958525 | bibcode = 1987ASAJ...82..737K }}&lt;/ref&gt; In 1961, physicist [[John Larry Kelly, Jr]] and his colleague [[Louis Gerstman]]&lt;ref&gt;{{cite news|last=Lambert|first=Bruce|date=March 21, 1992|title=Louis Gerstman, 61, a Specialist In Speech Disorders and Processes|work=The New York Times|url=https://www.nytimes.com/1992/03/21/nyregion/louis-gerstman-61-a-specialist-in-speech-disorders-and-processes.html}}&lt;/ref&gt; used an [[IBM 704]] computer to synthesize speech, an event among the most prominent in the history of [[Bell Labs]].{{citation needed|date=April 2016}} Kelly's voice recorder synthesizer ([[vocoder]]) recreated the song &quot;[[Daisy Bell]]&quot;, with musical accompaniment from [[Max Mathews]]. Coincidentally, [[Arthur C. Clarke]] was visiting his friend and colleague John Pierce at the Bell Labs Murray Hill facility. Clarke was so impressed by the demonstration that he used it in the climactic scene of his screenplay for his novel ''[[2001: A Space Odyssey (novel)|2001: A Space Odyssey]]'',&lt;ref&gt;{{cite web|url=http://www.lsi.usp.br/~rbianchi/clarke/ACC.Biography.html |title=Arthur C. Clarke Biography |access-date=5 December 2017 |url-status=dead |archive-url=https://web.archive.org/web/19971211154551/http://www.lsi.usp.br/~rbianchi/clarke/ACC.Biography.html |archive-date=December 11, 1997 }}&lt;/ref&gt; where the [[HAL 9000]] computer sings the same song as astronaut [[David Bowman (Space Odyssey)|Dave Bowman]] puts it to sleep.&lt;ref&gt;{{cite web|url=http://www.bell-labs.com/news/1997/march/5/2.html |title=Where &quot;HAL&quot; First Spoke (Bell Labs Speech Synthesis website) |publisher=Bell Labs |access-date=2010-02-17 |url-status=dead |archive-url=https://web.archive.org/web/20000407081031/http://www.bell-labs.com/news/1997/march/5/2.html |archive-date=2000-04-07 }}&lt;/ref&gt; Despite the success of purely electronic speech synthesis, research into mechanical speech-synthesizers continues.&lt;ref&gt;[http://www.takanishi.mech.waseda.ac.jp/top/research/voice/index.htm Anthropomorphic Talking Robot Waseda-Talker Series] {{webarchive|url=https://web.archive.org/web/20160304034116/http://www.takanishi.mech.waseda.ac.jp/top/research/voice/index.htm |date=2016-03-04 }}&lt;/ref&gt;{{Third-party inline|date=July 2019}}

[[Linear predictive coding]] (LPC), a form of [[speech coding]], began development with the work of [[Fumitada Itakura]] of [[Nagoya University]] and Shuzo Saito of [[Nippon Telegraph and Telephone]] (NTT) in 1966. Further developments in LPC technology were made by [[Bishnu S. Atal]] and [[Manfred R. Schroeder]] at [[Bell Labs]] during the 1970s.&lt;ref&gt;{{cite journal |last1=Gray |first1=Robert M. |title=A History of Realtime Digital Speech on Packet Networks: Part II of Linear Predictive Coding and the Internet Protocol |journal=Found. Trends Signal Process. |date=2010 |volume=3 |issue=4 |pages=203–303 |doi=10.1561/2000000036 |url=https://ee.stanford.edu/~gray/lpcip.pdf |issn=1932-8346}}&lt;/ref&gt; LPC was later the basis for early speech synthesizer chips, such as the [[Texas Instruments LPC Speech Chips]] used in the [[Speak &amp; Spell (toy)|Speak &amp; Spell]] toys from 1978.

In 1975, Fumitada Itakura developed the [[line spectral pairs]] (LSP) method for high-compression speech coding, while at NTT.&lt;ref&gt;{{cite journal |last1=Zheng |first1=F. |last2=Song |first2=Z. |last3=Li |first3=L. |last4=Yu |first4=W. |title=The Distance Measure for Line Spectrum Pairs Applied to Speech Recognition |journal=Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP'98) |date=1998 |issue=3 |pages=1123–6 |url=http://www.work.caltech.edu/~ling/pub/icslp98lsp.pdf}}&lt;/ref&gt;&lt;ref name=&quot;ieee&quot;&gt;{{cite web |title=List of IEEE Milestones |url=https://ethw.org/Milestones:List_of_IEEE_Milestones |publisher=[[IEEE]] |access-date=15 July 2019}}&lt;/ref&gt;&lt;ref name=ItakuraHistory&gt;{{cite web|url=https://ethw.org/Oral-History:Fumitada_Itakura|title=Fumitada Itakura Oral History|publisher=IEEE Global History Network|date=20 May 2009|access-date=2009-07-21}}&lt;/ref&gt; From 1975 to 1981, Itakura studied problems in speech analysis and synthesis based on the LSP method.&lt;ref name=ItakuraHistory/&gt; In 1980, his team developed an LSP-based speech synthesizer chip. LSP is an important technology for speech synthesis and coding, and in the 1990s was adopted by almost all international speech coding standards as an essential component, contributing to the enhancement of digital speech communication over mobile channels and the internet.&lt;ref name=&quot;ieee&quot;/&gt;

In 1975, [[MUSA (MUltichannel Speaking Automaton)|MUSA]] was released, and was one of the first Speech Synthesis systems. It consisted of a stand-alone computer hardware and a specialized software that enabled it to read Italian. A second version, released in 1978, was also able to sing Italian in an &quot;a cappella&quot; style.
[[File:DECtalk demo.flac|thumb|DECtalk demo recording using the Perfect Paul and Uppity Ursula voices]]
Dominant systems in the 1980s and 1990s were the [[DECtalk]] system, based largely on the work of [[Dennis H. Klatt|Dennis Klatt]] at MIT, and the Bell Labs system;&lt;ref&gt;{{Cite book |first1= Richard W. |last1= Sproat |title= Multilingual Text-to-Speech Synthesis: The Bell Labs Approach |publisher= Springer |year= 1997 |isbn= 978-0-7923-8027-6}}&lt;/ref&gt; the latter was one of the first multilingual language-independent systems, making extensive use of [[natural language processing]] methods.

[[Handheld]] electronics featuring speech synthesis began emerging in the 1970s. One of the first was the [[Telesensory Systems|Telesensory Systems Inc.]] (TSI) ''Speech+'' portable calculator for the blind in 1976.&lt;ref&gt;[TSI Speech+ &amp; other speaking calculators]&lt;/ref&gt;&lt;ref&gt;Gevaryahu, Jonathan, [ &quot;TSI S14001A Speech Synthesizer LSI Integrated Circuit Guide&quot;]{{dead link|date= December 2011}}&lt;/ref&gt; Other devices had primarily educational purposes, such as the [[Speak &amp; Spell (toy)|Speak &amp; Spell toy]] produced by [[Texas Instruments]] in 1978.&lt;ref&gt;Breslow, et al. {{patent|US|4326710|title=Talking electronic game}}: &quot;Talking electronic game&quot;, April 27, 1982&lt;/ref&gt; Fidelity released a speaking version of its electronic chess computer in 1979.&lt;ref&gt;[http://www.ismenio.com/chess_fidelity_vcc.html Voice Chess Challenger]&lt;/ref&gt; The first [[video game]] to feature speech synthesis was the 1980 [[shoot 'em up]] [[arcade game]], ''[[Stratovox]]'' (known in Japan as ''Speak &amp; Rescue''), from [[Sunsoft|Sun Electronics]].&lt;ref&gt;[http://www.gamesradar.com/f/gamings-most-important-evolutions/a-20101008102331322035/p-2 Gaming's most important evolutions] {{webarchive|url=https://web.archive.org/web/20110615221800/http://www.gamesradar.com/f/gamings-most-important-evolutions/a-20101008102331322035/p-2 |date=2011-06-15 }}, [[GamesRadar]]&lt;/ref&gt; The first [[personal computer game]] with speech synthesis was ''[[Stealth game#History|Manbiki Shoujo]]'' (''Shoplifting Girl''), released in 1980 for the [[PET 2001]], for which the game's developer, Hiroshi Suzuki, developed a &quot;''zero cross''&quot; programming technique to produce a synthesized speech waveform.&lt;ref&gt;{{cite book |last=Szczepaniak |first=John |year=2014 |title=The Untold History of Japanese Game Developers |publisher=SMG Szczepaniak |volume=1 |pages=544–615 |isbn=978-0992926007 }}&lt;/ref&gt; Another early example, the arcade version of ''[[Berzerk (arcade game)|Berzerk]]'', also dates from 1980. The [[Milton Bradley Company]] produced the first multi-player [[electronic game]] using voice synthesis, ''[[Milton (game)|Milton]]'', in the same year.

Early electronic speech-synthesizers sounded robotic and were often barely intelligible. The quality of synthesized speech has steadily improved, but {{as of | 2016 | lc = on}} output from contemporary speech synthesis systems remains clearly distinguishable from actual human speech.

Synthesized voices typically sounded male until 1990, when [[Ann Syrdal]], at [[AT&amp;T Bell Laboratories]], created a female voice.&lt;ref name=NewYorkTimes&gt;{{cite news|url=https://www.nytimes.com/2020/08/20/technology/ann-syrdal-who-helped-give-computers-a-female-voice-dies-at-74.html|title=Ann Syrdal, Who Helped Give Computers a Female Voice, Dies at 74|work=The New York Times|date=2020-08-20|author=CadeMetz|access-date=2020-08-23}}&lt;/ref&gt; 

Kurzweil predicted in 2005 that as the [[cost-performance ratio]] caused speech synthesizers to become cheaper and more accessible, more people would benefit from the use of text-to-speech programs.&lt;ref&gt;{{cite book
  |last = Kurzweil
  |first = Raymond
  |author-link = Raymond Kurzweil
  |title = The Singularity is Near
  |publisher = [[Penguin Books]]
  |year = 2005
  |isbn = 978-0-14-303788-0}}
&lt;/ref&gt;

== Synthesizer technologies ==

The most important qualities of a speech synthesis system are ''naturalness'' and ''[[Intelligibility (communication)|intelligibility]]''.'''&lt;ref&gt;{{cite book|last1=Taylor|first1=Paul|title=Text-to-speech synthesis|url=https://archive.org/details/texttospeechsynt00tayl_030|url-access=limited|date=2009|publisher=Cambridge University Press|location=Cambridge, UK|isbn=9780521899277|page=[https://archive.org/details/texttospeechsynt00tayl_030/page/n26 3]}}&lt;/ref&gt;''' Naturalness describes how closely the output sounds like human speech, while intelligibility is the ease with which the output is understood. The ideal speech synthesizer is both natural and intelligible. Speech synthesis systems usually try to maximize both characteristics.

The two primary technologies generating synthetic speech waveforms are ''concatenative synthesis'' and ''[[formant]] synthesis''. Each technology has strengths and weaknesses, and the intended uses of a synthesis system will typically determine which approach is used.

=== Concatenation synthesis ===
{{main|Concatenative synthesis}}
Concatenative synthesis is based on the [[concatenation]] (or stringing together) of segments of recorded speech. Generally, concatenative synthesis produces the most natural-sounding synthesized speech. However, differences between natural variations in speech and the nature of the automated techniques for segmenting the waveforms sometimes result in audible glitches in the output. There are three main sub-types of concatenative synthesis.

==== Unit selection synthesis ====

Unit selection synthesis uses large [[database]]s of recorded speech. During database creation, each recorded utterance is segmented into some or all of the following: individual [[phone (phonetics)|phones]], [[diphone]]s, half-phones, [[syllable]]s, [[morpheme]]s, [[word]]s, [[phrase]]s, and [[sentence (linguistics)|sentence]]s. Typically, the division into segments is done using a specially modified [[speech recognition|speech recognizer]] set to a &quot;forced alignment&quot; mode with some manual correction afterward, using visual representations such as the [[waveform]] and [[spectrogram]].&lt;ref&gt;[[Alan W. Black]], [https://www.cs.cmu.edu/~awb/papers/IEEE2002/allthetime/allthetime.html Perfect synthesis for all of the people all of the time.] IEEE TTS Workshop 2002.&lt;/ref&gt; An [[index (database)|index]] of the units in the speech database is then created based on the segmentation and acoustic parameters like the [[fundamental frequency]] ([[pitch (music)|pitch]]), duration, position in the syllable, and neighboring phones. At [[Run time (program lifecycle phase)|run time]], the desired target utterance is created by determining the best chain of candidate units from the database (unit selection). This process is typically achieved using a specially weighted [[decision tree]].

Unit selection provides the greatest naturalness, because it applies only a small amount of [[digital signal processing]] (DSP) to the recorded speech. DSP often makes recorded speech sound less natural, although some systems use a small amount of signal processing at the point of concatenation to smooth the waveform. The output from the best unit-selection systems is often indistinguishable from real human voices, especially in contexts for which the TTS system has been tuned. However, maximum naturalness typically require unit-selection speech databases to be very large, in some systems ranging into the [[gigabyte]]s of recorded data, representing dozens of hours of speech.&lt;ref&gt;John Kominek and [[Alan W. Black]]. (2003). CMU ARCTIC databases for speech synthesis. CMU-LTI-03-177. Language Technologies Institute, School of Computer Science, Carnegie Mellon University.&lt;/ref&gt; Also, unit selection algorithms have been known to select segments from a place that results in less than ideal synthesis (e.g. minor words become unclear) even when a better choice exists in the database.&lt;ref&gt;Julia Zhang. [http://groups.csail.mit.edu/sls/publications/2004/zhang_thesis.pdf Language Generation and Speech Synthesis in Dialogues for Language Learning], masters thesis, Section 5.6 on page 54.&lt;/ref&gt; Recently, researchers have proposed various automated methods to detect unnatural segments in unit-selection speech synthesis systems.&lt;ref&gt;William Yang Wang and Kallirroi Georgila. (2011). [https://www.cs.cmu.edu/~yww/papers/asru2011.pdf Automatic Detection of Unnatural Word-Level Segments in Unit-Selection Speech Synthesis], IEEE ASRU 2011.&lt;/ref&gt;

==== Diphone synthesis ====

Diphone synthesis uses a minimal speech database containing all the [[diphone]]s (sound-to-sound transitions) occurring in a language. The number of diphones depends on the [[phonotactics]] of the language: for example, Spanish has about 800 diphones, and German about 2500. In diphone synthesis, only one example of each diphone is contained in the speech database. At runtime, the target [[prosody (linguistics)|prosody]] of a sentence is superimposed on these minimal units by means of [[digital signal processing]] techniques such as [[linear predictive coding]], [[PSOLA]]&lt;ref&gt;{{cite web|title=Pitch-Synchronous Overlap and Add (PSOLA) Synthesis|url=http://www.fon.hum.uva.nl/praat/manual/PSOLA.html|url-status=dead|archive-url=https://web.archive.org/web/20070222180903/http://www.fon.hum.uva.nl/praat/manual/PSOLA.html|archive-date=February 22, 2007|access-date=2008-05-28}}&lt;/ref&gt; or [[MBROLA]].&lt;ref&gt;T. Dutoit, V. Pagel, N. Pierret, F. Bataille, O. van der Vrecken. [http://ai2-s2-pdfs.s3.amazonaws.com/7b1f/dadf05b8f968a5b361f6f82852ade62c8010.pdf The MBROLA Project: Towards a set of high quality speech synthesizers of use for non commercial purposes]. ''ICSLP Proceedings'', 1996.&lt;/ref&gt; or more recent techniques such as pitch modification in the source domain using [[discrete cosine transform]].&lt;ref&gt;{{cite journal | last1 = Muralishankar | first1 = R | last2 = Ramakrishnan | first2 = A.G. | last3 = Prathibha | first3 = P | year = 2004 | title = Modification of Pitch using DCT in the Source Domain | journal = Speech Communication | volume = 42 | issue = 2| pages = 143–154 | doi=10.1016/j.specom.2003.05.001}}&lt;/ref&gt; Diphone synthesis suffers from the sonic glitches of concatenative synthesis and the robotic-sounding nature of formant synthesis, and has few of the advantages of either approach other than small size. As such, its use in commercial applications is declining,{{Citation needed|date=January 2012}} although it continues to be used in research because there are a number of freely available software implementations. An early example of Diphone synthesis is a teaching robot, leachim, that was invented by [[Michael J. Freeman]].&lt;ref&gt;{{Cite news|url=http://content.time.com/time/magazine/article/0,9171,904056,00.html|title=Education: Marvel of The Bronx|date=1974-04-01|work=Time|access-date=2019-05-28|language=en-US|issn=0040-781X}}&lt;/ref&gt; Leachim contained information regarding class curricular and certain biographical information about the 40 students whom it was programmed to teach.&lt;ref&gt;{{Cite web|url=http://cyberneticzoo.com/robots/1960-rudy-the-robot-michael-freeman-american/|title=1960 - Rudy the Robot - Michael Freeman (American)|date=2010-09-13|website=cyberneticzoo.com|language=en-US|access-date=2019-05-23}} {{verify source |date=August 2019 |reason=This ref was deleted ([[Special:Diff/899239713]]) by a bug in VisualEditor and later restored by a bot from the original cite at [[Special:Permalink/898386283]] cite #5 - please verify the cite's accuracy and remove this {verify source} template. [[User:GreenC_bot/Job_18]]}}&lt;/ref&gt; It was tested in a fourth grade classroom in [[The Bronx|the Bronx, New York]].&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=bNECAAAAMBAJ&amp;q=Leachim+Michael+Freeman&amp;pg=PA40|title=New York Magazine|last=LLC|first=New York Media|date=1979-07-30|publisher=New York Media, LLC|language=en}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=_QJmAAAAMAAJ&amp;q=leachim|title=The Futurist|date=1978|publisher=World Future Society.|pages=359, 360, 361|language=en}}&lt;/ref&gt;

==== Domain-specific synthesis ====

Domain-specific synthesis concatenates prerecorded words and phrases to create complete utterances. It is used in applications where the variety of texts the system will output is limited to a particular domain, like transit schedule announcements or weather reports.&lt;ref&gt;L.F. Lamel, J.L. Gauvain, B. Prouts, C. Bouhier, R. Boesch. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.6101&amp;rep=rep1&amp;type=pdf Generation and Synthesis of Broadcast Messages], ''Proceedings ESCA-NATO Workshop and Applications of Speech Technology'', September 1993.&lt;/ref&gt; The technology is very simple to implement, and has been in commercial use for a long time, in devices like talking clocks and calculators. The level of naturalness of these systems can be very high because the variety of sentence types is limited, and they closely match the prosody and intonation of the original recordings.{{Citation needed|date=February 2007}}

Because these systems are limited by the words and phrases in their databases, they are not general-purpose and can only synthesize the combinations of words and phrases with which they have been preprogrammed. The blending of words within naturally spoken language however can still cause problems unless the many variations are taken into account. For example, in [[rhotic and non-rhotic accents|non-rhotic]] dialects of English the ''&quot;r&quot;'' in words like ''&quot;clear&quot;'' {{IPA|/ˈklɪə/}} is usually only pronounced when the following word has a vowel as its first letter (e.g. ''&quot;clear out&quot;'' is realized as {{IPA|/ˌklɪəɹˈʌʊt/}}). Likewise in [[French language|French]], many final consonants become no longer silent if followed by a word that begins with a vowel, an effect called [[Liaison (French)|liaison]]. This [[alternation (linguistics)|alternation]] cannot be reproduced by a simple word-concatenation system, which would require additional complexity to be [[context-sensitive grammar|context-sensitive]].

=== Formant synthesis ===

[[Formant]] synthesis does not use human speech samples at runtime. Instead, the synthesized speech output is created using [[additive synthesis]] and an acoustic model ([[physical modelling synthesis]]).&lt;ref&gt;Dartmouth College: [http://digitalmusics.dartmouth.edu/~book/MATCpages/chap.4/4.4.formant_synth.html ''Music and Computers''] {{webarchive|url=https://web.archive.org/web/20110608035309/http://digitalmusics.dartmouth.edu/~book/MATCpages/chap.4/4.4.formant_synth.html |date=2011-06-08 }}, 1993.&lt;/ref&gt; Parameters such as [[fundamental frequency]], [[phonation|voicing]], and [[noise]] levels are varied over time to create a [[waveform]] of artificial speech. This method is sometimes called ''rules-based synthesis''; however, many concatenative systems also have rules-based components.
Many systems based on formant synthesis technology generate artificial, robotic-sounding speech that would never be mistaken for human speech. However, maximum naturalness is not always the goal of a speech synthesis system, and formant synthesis systems have advantages over concatenative systems. Formant-synthesized speech can be reliably intelligible, even at very high speeds, avoiding the acoustic glitches that commonly plague concatenative systems. High-speed synthesized speech is used by the visually impaired to quickly navigate computers using a [[screen reader]]. Formant synthesizers are usually smaller programs than concatenative systems because they do not have a database of speech samples. They can therefore be used in [[embedded system]]s, where [[data storage device|memory]] and [[microprocessor]] power are especially limited. Because formant-based systems have complete control of all aspects of the output speech, a wide variety of prosodies and [[intonation (linguistics)|intonation]]s can be output, conveying not just questions and statements, but a variety of emotions and tones of voice.

Examples of non-real-time but highly accurate intonation control in formant synthesis include the work done in the late 1970s for the [[Texas Instruments]] toy [[Speak &amp; Spell (game)|Speak &amp; Spell]], and in the early 1980s [[Sega]] [[Video arcade|arcade]] machines&lt;ref&gt;Examples include [[Astro Blaster]], [[Space Fury]], and [[Star Trek (arcade game)|Star Trek: Strategic Operations Simulator]]&lt;/ref&gt; and in many [[Atari, Inc.]] arcade games&lt;ref&gt;Examples include [[Star Wars (arcade game)|Star Wars]], [[Firefox (arcade game)|Firefox]], [[Star Wars: Return of the Jedi (arcade game)|Return of the Jedi]], [[Road Runner (video game)|Road Runner]], [[Star Wars: The Empire Strikes Back (arcade game)|The Empire Strikes Back]], [[Indiana Jones and the Temple of Doom (arcade game)|Indiana Jones and the Temple of Doom]], [[720°]], [[Gauntlet (arcade game)|Gauntlet]], [[Gauntlet II]], [[A.P.B. (video game)|A.P.B.]], [[Paperboy (video game)|Paperboy]], [[RoadBlasters]],  [http://www.arcade-museum.com/game_detail.php?game_id=10319 Vindicators Part II], [[Escape from the Planet of the Robot Monsters]].&lt;/ref&gt; using the [[Texas Instruments LPC Speech Chips|TMS5220 LPC Chips]]. Creating proper intonation for these projects was painstaking, and the results have yet to be matched by real-time text-to-speech interfaces.&lt;ref&gt;{{Cite book |author=John Holmes and Wendy Holmes |title=Speech Synthesis and Recognition |edition=2nd |publisher=CRC |year=2001 |isbn=978-0-7484-0856-6}}&lt;/ref&gt;

=== Articulatory synthesis ===

[[Articulatory synthesis]] refers to computational techniques for synthesizing speech based on models of the human [[vocal tract]] and the articulation processes occurring there. The first articulatory synthesizer regularly used for laboratory experiments was developed at [[Haskins Laboratories]] in the mid-1970s by [[Philip Rubin]], Tom Baer, and Paul Mermelstein. This synthesizer, known as ASY, was based on vocal tract models developed at [[Bell Laboratories]] in the 1960s and 1970s by Paul Mermelstein, Cecil Coker, and colleagues.

Until recently, articulatory synthesis models have not been incorporated into commercial speech synthesis systems. A notable exception is the [[NeXT]]-based system originally developed and marketed by Trillium Sound Research, a spin-off company of the [[University of Calgary]], where much of the original research was conducted. Following the demise of the various incarnations of NeXT (started by [[Steve Jobs]] in the late 1980s and merged with Apple Computer in 1997), the Trillium software was published under the GNU General Public License, with work continuing as [[gnuspeech]]. The system, first marketed in 1994, provides full articulatory-based text-to-speech conversion using a waveguide or transmission-line analog of the human oral and nasal tracts controlled by Carré's &quot;distinctive region model&quot;.

More recent synthesizers, developed by Jorge C. Lucero and colleagues, incorporate models of vocal fold biomechanics, glottal aerodynamics and acoustic wave propagation in the bronqui, traquea, nasal and oral cavities, and thus constitute full systems of physics-based speech simulation.&lt;ref name=&quot;:0&quot;&gt;{{Cite journal|url = http://www.cic.unb.br/~lucero/papers/768_Paper.pdf|title = Physics-based synthesis of disordered voices|last1 = Lucero|first1 = J. C.|date = 2013|journal = Interspeech 2013|access-date = Aug 27, 2015|last2 = Schoentgen|first2 = J.|last3 = Behlau|first3 = M.|publisher = International Speech Communication Association|location = Lyon, France}}&lt;/ref&gt;&lt;ref name=&quot;:1&quot;&gt;{{Cite journal|last1=Englert|first1=Marina|last2=Madazio|first2=Glaucya|last3=Gielow|first3=Ingrid|last4=Lucero|first4=Jorge|last5=Behlau|first5=Mara|date=2016|title=Perceptual error identification of human and synthesized voices|journal=Journal of Voice|volume=30|issue=5|pages=639.e17–639.e23|doi=10.1016/j.jvoice.2015.07.017|pmid=26337775}}&lt;/ref&gt;

=== HMM-based synthesis ===

HMM-based synthesis is a synthesis method based on [[hidden Markov model]]s, also called Statistical Parametric Synthesis. In this system, the [[frequency spectrum]] ([[vocal tract]]), [[fundamental frequency]] (voice source), and duration ([[prosody (linguistics)|prosody]]) of speech are modeled simultaneously by HMMs. Speech [[waveforms]] are generated from HMMs themselves based on the [[maximum likelihood]] criterion.&lt;ref&gt;{{cite web|url=http://hts.sp.nitech.ac.jp/ |title=The HMM-based Speech Synthesis System |publisher=Hts.sp.nitech.ac.j |access-date=2012-02-22}}&lt;/ref&gt;

=== Sinewave synthesis ===
[[Sinewave synthesis]] is a technique for synthesizing speech by replacing the [[formants]] (main bands of energy) with pure tone whistles.&lt;ref&gt;{{Cite journal
 |last1        = Remez
 |first1       = R.
 |last2        = Rubin
 |first2       = P.
 |last3        = Pisoni
 |first3       = D.
 |last4        = Carrell
 |first4       = T.
 |title        = Speech perception without traditional speech cues
 |doi          = 10.1126/science.7233191
 |journal      = Science
 |volume       = 212
 |issue        = 4497
 |pages        = 947–949
 |date         = 22 May 1981
 |pmid         = 7233191
 |bibcode = 1981Sci...212..947R
 |url          = http://www.bsos.umd.edu/hesp/mwinn/Remez_et_al_1981.pdf
 |access-date  = 2011-12-14
 |archive-url  = https://web.archive.org/web/20111216113028/http://www.bsos.umd.edu/hesp/mwinn/Remez_et_al_1981.pdf
 |archive-date = 2011-12-16
 |url-status     = dead
}}&lt;!-- in case PDF link dies, paper also available here and here:
http://people.ece.cornell.edu/land/courses/ece4760/Speech/remez_rubin_pisoni_carrell1981.pdf
http://www.haskins.yale.edu/Reprints/HL0338.pdf --&gt;&lt;/ref&gt;

=== Deep learning-based synthesis ===

==== Formulation ====
Given an input text or some sequence of linguistic unit &lt;math&gt;Y&lt;/math&gt;, the target speech &lt;math&gt;X&lt;/math&gt; can be derived by 

&lt;math&gt;X=\arg\max P(X|Y, \theta)&lt;/math&gt;

where &lt;math&gt;\theta&lt;/math&gt; is the model parameter.
