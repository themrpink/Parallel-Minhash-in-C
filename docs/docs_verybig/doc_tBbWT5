A min-priority queue is an abstract data type that provides 3 basic operations : {{mono|add_with_priority()}}, {{mono|decrease_priority()}} and {{mono|extract_min()}}. As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably, [[Fibonacci heap]] {{harv|Fredman|Tarjan|1984}} or [[Brodal queue]] offer optimal implementations for those 3 operations. As the algorithm is slightly different, we mention it here, in pseudo-code as well :

 1  '''function''' Dijkstra(''Graph'', ''source''):
 2      dist[''source''] ← 0                           ''// Initialization''
 3
 4      create vertex priority queue Q
 5
 6      '''for each''' vertex ''v'' in ''Graph'':          
 7          '''if''' ''v'' ≠ ''source''
 8              dist[''v''] ← INFINITY                 ''// Unknown distance from source to v''
 9              prev[''v''] ← UNDEFINED                ''// Predecessor of v''
 10
 11         ''Q''.add_with_priority(''v'', dist[''v''])
 12
 13
 14     '''while''' ''Q'' is not empty:                      ''// The main loop''
 15         ''u'' ← ''Q''.extract_min()                    ''// Remove and return best vertex''
 16         '''for each''' neighbor ''v'' of ''u'':              ''// only v that are still in Q''
 17             ''alt'' ← dist[''u''] + length(''u'', ''v'')
 18             '''if''' ''alt'' &lt; dist[''v'']
 19                 dist[''v''] ← ''alt''
 20                 prev[''v''] ← ''u''
 21                 ''Q''.decrease_priority(''v'', ''alt'')
 22
 23     '''return''' dist, prev

Instead of filling the priority queue with all nodes in the initialization phase, it is also possible to initialize it to contain only ''source''; then, inside the {{mono|'''if''' ''alt'' &lt; dist[''v'']}} block, the &lt;var&gt;decrease_priority&lt;/var&gt; becomes an &lt;var&gt;add_with_priority&lt;/var&gt; operation if the node is not already in the queue.&lt;ref name=&quot;mehlhorn&quot;/&gt;{{rp|198}}

Yet another alternative is to add nodes unconditionally to the priority queue and to instead check after extraction that no shorter connection was found yet. This can be done by additionally extracting the associated priority {{mono|''p''}} from the queue and only processing further {{mono|'''if''' ''p'' ≤ dist[''u'']}} inside the {{mono|'''while''' ''Q'' is not empty}} loop.

These alternatives can use entirely array-based priority queues without decrease-key functionality which have been found to achieve even faster computing times in practice.&lt;ref name=chen_07&gt;{{cite book|first1=M.|last1=Chen|first2=R. A.|last2=Chowdhury|first3=V.|last3=Ramachandran|first4=D. L.|last4=Roche|first5=L.|last5=Tong|title=Priority Queues and Dijkstra's Algorithm – UTCS Technical Report TR-07-54 – 12 October 2007|publisher=The University of Texas at Austin, Department of Computer Sciences|location=Austin, Texas|year=2007|url=http://www.cs.sunysb.edu/~rezaul/papers/TR-07-54.pdf|ref=chen}}&lt;/ref&gt;

== Proof of correctness ==
''Proof of Dijkstra's algorithm is constructed by induction on the number of visited nodes.''

''Invariant hypothesis'': For each node {{mono|v}}, {{mono|dist[v]}} is the shortest distance from {{mono|source}} to {{mono|v}} when traveling via visited nodes only, or infinity if no such path exists. (Note: we do not assume {{mono|dist[v]}} is the actual shortest distance for unvisited nodes.)

The base case is when there is just one visited node, namely the initial node {{mono|source}}, in which case the hypothesis is [[triviality (mathematics)|trivial]].

Otherwise, assume the hypothesis for ''n-1'' visited nodes. In which case, we choose an edge {{mono|vu}} where {{mono|u}} has the least {{mono|dist[u]}} of any unvisited nodes and the edge {{mono|vu}} is such that {{mono|1=dist[u] = dist[v] + length[v,u]}}. {{mono|dist[u]}} is considered to be the shortest distance from {{mono|source}} to {{mono|u}} because if there were a shorter path, and if {{mono|w}} was the first unvisited node on that path then by the original hypothesis {{mono|dist[w]}} &gt; {{mono|dist[u]}} which creates a contradiction. Similarly if there were a shorter path to {{mono|u}} without using unvisited nodes, and if the last but one node on that path were {{mono|w}}, then we would have had {{mono|1=dist[u] = dist[w] + length[w,u]}}, also a contradiction.

After processing {{mono|u}} it will still be true that for each unvisited node {{mono|w}}, {{mono|dist[w]}} will be the shortest distance from {{mono|source}} to {{mono|w}} using visited nodes only, because if there were a shorter path that doesn't go by {{mono|u}} we would have found it previously, and if there were a shorter path using {{mono|u}} we would have updated it when processing {{mono|u}}.

After all nodes are visited, the shortest path from {{mono|source}} to any node {{mono|v}} consists only of visited nodes, therefore {{mono|dist[v]}} is the shortest distance.

== Running time ==

Bounds of the running time of Dijkstra's algorithm on a graph with edges {{mvar|E}} and vertices {{mvar|V}} can be expressed as a function of the number of edges, denoted &lt;math&gt;|E|&lt;/math&gt;, and the number of vertices, denoted &lt;math&gt;|V|&lt;/math&gt;, using [[big-O notation]]. The complexity bound depends mainly on the data structure used to represent the set {{mvar|Q}}. In the following, upper bounds can be simplified because &lt;math&gt;|E|&lt;/math&gt; is &lt;math&gt;\Theta(|V|^2)&lt;/math&gt; for any graph, but that simplification disregards the fact that in some problems, other upper bounds on &lt;math&gt;|E|&lt;/math&gt; may hold.

For any data structure for the vertex set {{mvar|Q}}, the running time is in&lt;ref name=&quot;Intro&quot;&gt;{{harvnb|Cormen|Leiserson|Rivest|Stein|2001}}&lt;/ref&gt;
:&lt;math&gt;\Theta(|E| \cdot T_\mathrm{dk} + |V| \cdot T_\mathrm{em}),&lt;/math&gt;
where &lt;math&gt;T_\mathrm{dk}&lt;/math&gt; and &lt;math&gt;T_\mathrm{em}&lt;/math&gt; are the complexities of the ''decrease-key'' and ''extract-minimum'' operations in {{mvar|Q}}, respectively. The simplest version of Dijkstra's algorithm stores the vertex set {{mvar|Q}} as an ordinary linked list or array, and extract-minimum is simply a linear search through all vertices in {{mvar|Q}}. In this case, the running time is &lt;math&gt;\Theta(|E| + |V|^2) = \Theta(|V|^2)&lt;/math&gt;.

If the graph is stored as an adjacency list, the running time for a dense graph (i.e., where &lt;math&gt;|E|\in\Theta(|V|^2)&lt;/math&gt;) is
:&lt;math&gt;\Theta((|V|^2) \log |V|)&lt;/math&gt;.

For [[sparse graph]]s, that is, graphs with far fewer than &lt;math&gt;|V|^2&lt;/math&gt; edges, Dijkstra's algorithm can be implemented more efficiently by storing the graph in the form of [[adjacency list]]s and using a [[self-balancing binary search tree]], [[binary heap]], [[pairing heap]], or [[Fibonacci heap]] as a [[priority queue]] to implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to keep this structure up to date as the priority queue {{mvar|Q}} changes. With a self-balancing binary search tree or binary heap, the algorithm requires
:&lt;math&gt;\Theta((|E| + |V|) \log |V|)&lt;/math&gt;
time in the worst case (where &lt;math&gt;\log&lt;/math&gt; denotes the binary logarithm &lt;math&gt;\log_2&lt;/math&gt;); for connected graphs this time bound can be simplified to &lt;math&gt;\Theta( | E | \log | V | )&lt;/math&gt;.  The [[Fibonacci heap]] improves this to
:&lt;math&gt;\Theta(|E| + |V| \log|V|).&lt;/math&gt;

When using binary heaps, the [[Best, worst and average case|average case]] time complexity is lower than the worst-case: assuming edge costs are drawn independently from a common [[probability distribution]], the expected number of ''decrease-key'' operations is bounded by &lt;math&gt;\Theta(|V| \log (|E|/|V|))&lt;/math&gt;, giving a total running time of&lt;ref name=&quot;mehlhorn&quot;&gt;{{cite book |last1=Mehlhorn |first1=Kurt |author1-link=Kurt Mehlhorn|first2=Peter |last2=Sanders|author2-link=Peter Sanders (computer scientist) |title=Algorithms and Data Structures: The Basic Toolbox |publisher=Springer |year=2008 |chapter=Chapter 10. Shortest Paths |chapterurl=http://people.mpi-inf.mpg.de/~mehlhorn/ftp/Toolbox/ShortestPaths.pdf |isbn=978-3-540-77977-3 |doi=10.1007/978-3-540-77978-0}}&lt;/ref&gt;{{rp|199–200}}
:&lt;math&gt;O\left(|E| + |V| \log \frac{|E|}{|V|} \log |V|\right).&lt;/math&gt;

===Practical optimizations and infinite graphs===
In common presentations of Dijkstra's algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).{{r|mehlhorn}}{{rp|198}} This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.&lt;ref name=&quot;felner&quot;&gt;{{cite conference |first=Ariel |last=Felner |title=Position Paper: Dijkstra's Algorithm versus Uniform Cost Search or a Case Against Dijkstra's Algorithm |conference=Proc. 4th Int'l Symp. on Combinatorial Search |year=2011 |url=http://www.aaai.org/ocs/index.php/SOCS/SOCS11/paper/view/4017/4357}} In a route-finding problem, Felner finds that the queue can be a factor 500–600 smaller, taking some 40% of the running time.&lt;/ref&gt;

Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called ''uniform-cost search'' (UCS) in the artificial intelligence literature{{r|felner}}&lt;ref name=&quot;aima&quot;&gt;{{Cite AIMA|3|pages=75, 81}}&lt;/ref&gt;&lt;ref&gt;Sometimes also ''least-cost-first search'': {{cite journal |last=Nau |first=Dana S. |title=Expert computer systems |journal=Computer |publisher=IEEE |volume=16 |issue=2 |year=1983 |pages=63–85 |url=https://www.cs.umd.edu/~nau/papers/nau1983expert.pdf |doi=10.1109/mc.1983.1654302}}&lt;/ref&gt; and can be expressed in pseudocode as

 '''procedure''' uniform_cost_search(Graph, start, goal) '''is'''
     node ← start
     cost ← 0
     frontier ← priority queue containing node only
     explored ← empty set
     '''do'''
         '''if''' frontier is empty '''then'''
             '''return''' failure
         node ← frontier.pop()
         '''if''' node is goal '''then'''
             '''return''' solution
         explored.add(node)
         '''for each''' of node's neighbors ''n'' '''do'''
             '''if''' ''n'' is not in explored '''then'''
                 frontier.add(''n'')

The complexity of this algorithm can be expressed in an alternative way for very large graphs: when {{math|''C''&lt;sup&gt;*&lt;/sup&gt;}} is the length of the shortest path from the start node to any node satisfying the &quot;goal&quot; predicate, each edge has cost at least {{mvar|ε}}, and the number of neighbors per node is bounded by {{mvar|b}}, then the algorithm's worst-case time and space complexity are both in {{math|''O''(''b''&lt;sup&gt;1+⌊''C''&lt;sup&gt;*&lt;/sup&gt; {{frac}} ''ε''⌋&lt;/sup&gt;)}}.{{r|aima}}

Further optimizations of Dijkstra's algorithm for the single-target case include [[bidirectional search|bidirectional]] variants, goal-directed variants such as the [[A* algorithm]] (see {{slink||Related problems and algorithms}}), graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), and hierarchical decompositions of the input graph that reduce {{math|''s''–''t''}} routing to connecting {{mvar|s}} and {{mvar|t}} to their respective &quot;[[Transit Node Routing|transit nodes]]&quot; followed by shortest-path computation between these transit nodes using a &quot;highway&quot;.&lt;ref name=&quot;speedup&quot;&gt;{{cite conference |last1=Wagner |first1=Dorothea |first2=Thomas |last2=Willhalm |title=Speed-up techniques for shortest-path computations |conference=STACS |pages=23–36 |year=2007}}&lt;/ref&gt;
Combinations of such techniques may be needed for optimal practical performance on specific problems.&lt;ref&gt;{{cite journal |last1=Bauer |first1=Reinhard |first2=Daniel |last2=Delling |first3=Peter |last3=Sanders |first4=Dennis |last4=Schieferdecker |first5=Dominik |last5=Schultes |first6=Dorothea |last6=Wagner |s2cid=1661292 |title=Combining hierarchical and goal-directed speed-up techniques for Dijkstra's algorithm |journal=J. Experimental Algorithmics |volume=15 |pages=2.1 |year=2010|doi=10.1145/1671970.1671976 |url=https://publikationen.bibliothek.kit.edu/1000014952 }}&lt;/ref&gt;

===Specialized variants===
When arc weights are small integers (bounded by a parameter &lt;math&gt;C&lt;/math&gt;), specialized queues which take advantage of this fact can be used to speed up Dijkstra's algorithm. The first algorithm of this type was '''Dial's algorithm''' {{harv|Dial|1969}} for graphs with positive integer edge weights, which uses a [[bucket queue]] to obtain a running time &lt;math&gt;O(|E|+|V|C)&lt;/math&gt;. The use of a [[Van Emde Boas tree]] as the priority queue brings the complexity to &lt;math&gt;O(|E|\log\log C)&lt;/math&gt; {{harv|Ahuja|Mehlhorn|Orlin|Tarjan|1990}}. Another interesting variant based on a combination of a new [[radix heap]] and the well-known Fibonacci heap runs in time &lt;math&gt;O(|E|+|V|\sqrt{\log C})&lt;/math&gt; {{harv|Ahuja|Mehlhorn|Orlin|Tarjan|1990}}. Finally, the best algorithms in this special case are as follows. The algorithm given by {{harv|Thorup|2000}} runs in &lt;math&gt;O(|E|\log\log|V|)&lt;/math&gt; time and the algorithm given by {{harv|Raman|1997}} runs in &lt;math&gt;O(|E| + |V|\min\{(\log|V|)^{1/3+\varepsilon}, (\log C)^{1/4+\varepsilon}\})&lt;/math&gt; time.

== Related problems and algorithms ==

The functionality of Dijkstra's original algorithm can be extended with a variety of modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.

Dijkstra's algorithm is usually the working principle behind [[link-state routing protocol]]s, [[OSPF]] and [[IS-IS]] being the most common ones.

Unlike Dijkstra's algorithm, the [[Bellman–Ford algorithm]] can be used on graphs with negative edge weights, as long as the graph contains no [[negative cycle]] reachable from the source vertex ''s''. The presence of such cycles means there is no shortest path, since the total weight becomes lower each time the cycle is traversed. (This statement assumes that a &quot;path&quot; is allowed to repeat vertices. In [[graph theory]] that is normally not allowed.  In [[theoretical computer science]] it often is allowed.) It is possible to adapt Dijkstra's algorithm to handle negative weight edges by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles), such an algorithm is called [[Johnson's algorithm]].

The [[A-star algorithm|A* algorithm]] is a generalization of Dijkstra's algorithm that cuts down on the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the &quot;distance&quot; to the target. This approach can be viewed from the perspective of [[linear programming]]: there is a natural [[Shortest path problem#Linear programming formulation|linear program for computing shortest paths]], and solutions to its [[dual linear program]] are feasible if and only if they form a [[consistent heuristic]] (speaking roughly, since the sign conventions differ from place to place in the literature). This feasible dual / consistent heuristic defines a non-negative [[reduced cost]] and A* is essentially running Dijkstra's algorithm with these reduced costs. If the dual satisfies the weaker condition of [[Admissible heuristic|admissibility]], then A* is instead more akin to the Bellman–Ford algorithm.

The process that underlies Dijkstra's algorithm is similar to the [[Greedy algorithm|greedy]] process used in [[Prim's algorithm]].  Prim's purpose is to find a [[minimum spanning tree]] that connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim's does not evaluate the total weight of the path from the starting node, only the individual edges.

[[Breadth-first search]] can be viewed as a special-case of Dijkstra's algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.

The [[fast marching method]] can be viewed as a continuous version of Dijkstra's algorithm which computes the geodesic distance on a triangle mesh.

=== Dynamic programming perspective ===

From a [[dynamic programming]] point of view, Dijkstra's algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the '''Reaching''' method.&lt;ref name=sniedovich_06&gt;{{cite journal | last = Sniedovich | first = M. | title = Dijkstra's algorithm revisited: the dynamic programming connexion | journal = Journal of Control and Cybernetics | volume = 35 | issue = 3 | pages = 599–620 | year = 2006 | url = http://matwbn.icm.edu.pl/ksiazki/cc/cc35/cc3536.pdf }} [http://www.ifors.ms.unimelb.edu.au/tutorial/dijkstra_new/index.html Online version of the paper with interactive computational modules.]&lt;/ref&gt;&lt;ref name=denardo_03&gt;{{cite book | last = Denardo | first = E.V. | title = Dynamic Programming: Models and Applications | publisher = [[Dover Publications]] | location = Mineola, NY | year = 2003 | isbn = 978-0-486-42810-9}}&lt;/ref&gt;&lt;ref name=sniedovich_10&gt;{{cite book | last = Sniedovich | first = M. | title = Dynamic Programming: Foundations and Principles | publisher = [[Francis &amp; Taylor]] | year = 2010 | isbn = 978-0-8247-4099-3  }}&lt;/ref&gt;

In fact, Dijkstra's explanation of the logic behind the algorithm,&lt;ref&gt;{{harvnb|Dijkstra|1959|p=270}}&lt;/ref&gt; namely
{{quote|
'''Problem 2.''' Find the path of minimum total length between two given nodes &lt;math&gt;P&lt;/math&gt; and &lt;math&gt;Q&lt;/math&gt;.

We use the fact that, if &lt;math&gt;R&lt;/math&gt; is a node on the minimal path from &lt;math&gt;P&lt;/math&gt; to &lt;math&gt;Q&lt;/math&gt;, knowledge of the latter implies the knowledge of the minimal path from &lt;math&gt;P&lt;/math&gt; to &lt;math&gt;R&lt;/math&gt;.
}}

is a paraphrasing of [[Richard Bellman|Bellman's]] famous [[Bellman equation#Bellman's principle of optimality|Principle of Optimality]] in the context of the shortest path problem.

== Applications ==
Least-cost paths are calculated for instance to establish tracks of electricity lines or oil pipelines. The algorithm has also been used to calculate optimal long-distance footpaths in Ethiopia and contrast them with the situation on the ground.&lt;ref&gt;Nyssen, J., Tesfaalem Ghebreyohannes, Hailemariam Meaza, Dondeyne, S., 2020. Exploration of a medieval African map (Aksum, Ethiopia) – How do historical maps fit with topography? In: De Ryck, M., Nyssen, J., Van Acker, K., Van Roy, W., Liber Amicorum: Philippe De Maeyer In Kaart. Wachtebeke (Belgium): University Press: 165-178.&lt;/ref&gt;

== See also ==
* [[A* search algorithm]]
* [[Bellman–Ford algorithm]]
* [[Euclidean shortest path]]
* [[Flood fill]]
* [[Floyd–Warshall algorithm]]
* [[Johnson's algorithm]]
* [[Longest path problem]]
* [[Parallel all-pairs shortest path algorithm]]

==Notes==
{{reflist}}

== References ==
* {{cite book | author1-link = Thomas H. Cormen | first1 = Thomas H. | last1 = Cormen | author2-link = Charles E. Leiserson | first2 = Charles E. | last2 = Leiserson | author3-link = Ronald L. Rivest | first3 = Ronald L. | last3 = Rivest | author4-link = Clifford Stein | first4 = Clifford | last4 = Stein | title = Introduction to Algorithms | edition = Second | publisher = [[MIT Press]] and [[McGraw–Hill]] | year = 2001 | isbn = 0-262-03293-7 | chapter = Section 24.3: Dijkstra's algorithm | pages = 595–601 | ref = harv| title-link = Introduction to Algorithms }}
* {{cite journal
 | last = Dial | first = Robert B.
 | s2cid = 6754003
 | doi = 10.1145/363269.363610
 | url = https://dl.acm.org/doi/10.1145/363269.363610
 | url-access = subscription
 | issue = 11
 | journal = [[Communications of the ACM]]
 | pages = 632–633
 | title = Algorithm 360: Shortest-path forest with topological ordering [H]
 | volume = 12
 | year = 1969
 | ref = harv}}
* {{cite conference|first1=Michael Lawrence|last1=Fredman|authorlink1=Michael Fredman|first2=Robert E.|last2=Tarjan|authorlink2=Robert Tarjan|title=Fibonacci heaps and their uses in improved network optimization algorithms|conference=25th Annual Symposium on Foundations of Computer Science|year=1984|publisher=[[IEEE]]|pages=338–346|ref=harv|doi=10.1109/SFCS.1984.715934}}
* {{cite journal|first1=Michael Lawrence|last1=Fredman|authorlink1=Michael Fredman|first2=Robert E.|last2=Tarjan|s2cid=7904683|authorlink2=Robert Tarjan|title=Fibonacci heaps and their uses in improved network optimization algorithms|journal=Journal of the Association for Computing Machinery|volume=34|year=1987|pages=596–615|ref=harv|doi=10.1145/28869.28874|issue=3}}
* {{cite journal | first1 = F. Benjamin | last1 = Zhan | first2 = Charles E. | last2 = Noon | s2cid = 14986297 |date=February 1998 | title = Shortest Path Algorithms: An Evaluation Using Real Road Networks | journal = [[Transportation Science]] | volume = 32 | issue = 1 | pages = 65–73 | doi = 10.1287/trsc.32.1.65}}
* {{cite book|first1=M.|last1=Leyzorek|first2=R. S.|last2=Gray|first3=A. A.|last3=Johnson|first4=W. C.|last4=Ladew|first5=S. R.|last5=Meaker, Jr.|first6=R. M.|last6=Petry|first7=R. N.|last7=Seitz|title=Investigation of Model Techniques – First Annual Report – 6 June 1956 – 1 July 1957 – A Study of Model Techniques for Communication Systems|publisher=Case Institute of Technology|location=Cleveland, Ohio|year=1957|ref=harv}}
* {{cite journal|first1=D.E.|last1=Knuth|title=A Generalization of Dijkstra's Algorithm|journal=[[Information Processing Letters]]|volume=6|number=1|pages=1–5|year=1977|authorlink1=Donald Knuth|doi=10.1016/0020-0190(77)90002-3}}
* {{cite journal|first1=Ravindra K.|last1=Ahuja|first2=Kurt|last2=Mehlhorn|first3=James B.|last3=Orlin|first4=Robert E.|last4=Tarjan|s2cid=5499589|title=Faster Algorithms for the Shortest Path Problem|journal=Journal of the ACM|volume=37|number=2|pages=213–223| date=April 1990 |doi=10.1145/77600.77615|ref=harv|url=https://dspace.mit.edu/bitstream/1721.1/47994/1/fasteralgorithms00sloa.pdf|hdl=1721.1/47994}}
* {{cite journal|first1=Rajeev|last1=Raman|s2cid=18031586|title=Recent results on the single-source shortest paths problem|journal=SIGACT News|volume=28|issue=2|pages=81–87|year=1997|ref=harv|doi=10.1145/261342.261352}}
* {{cite journal|first1=Mikkel|last1=Thorup|s2cid=5221089|title=On RAM priority Queues|journal=SIAM Journal on Computing|volume=30|issue=1|pages=86–109|year=2000|doi=10.1137/S0097539795288246|ref=harv}}
* {{cite journal|first1=Mikkel|last1=Thorup|s2cid=207654795|title=Undirected single-source shortest paths with positive integer weights in linear time|journal=Journal of the ACM|volume=46|issue=3|pages=362–394|year=1999|doi=10.1145/316542.316548|ref=harv|url=http://www.diku.dk/~mthorup/PAPERS/sssp.ps.gz}}

== External links ==
{{Commons category|Dijkstra's algorithm}}
* [http://purl.umn.edu/107247 Oral history interview with Edsger W. Dijkstra], [[Charles Babbage Institute]], University of Minnesota, Minneapolis
* [http://blog.cleancoder.com/uncle-bob/2016/10/26/DijkstrasAlg.html Implementation of Dijkstra's algorithm using TDD], [[Robert Cecil Martin]], The Clean Code Blog
* [http://www.gilles-bertrand.com/2014/03/disjkstra-algorithm-description-shortest-path-pseudo-code-data-structure-example-image.html Graphical explanation of Dijkstra's algorithm step-by-step on an example], [[Gilles Bertrand]]

{{Edsger Dijkstra}}
{{Optimization algorithms}}

[[Category:Edsger W. Dijkstra|Algorithm]]
[[Category:1959 in computing]]
[[Category:Graph algorithms]]
[[Category:Search algorithms]]
[[Category:Routing algorithms]]
[[Category:Combinatorial optimization]]
[[Category:Articles with example pseudocode]]
[[Category:Dutch inventions]]</text>
      <sha1>9spumwu3vl59i9v0onavi9om8cfnr2d</sha1>
    </revision>
  </page>
  <page>
    <title>Subwoofer</title>
    <ns>0</ns>
    <id>45810</id>
    <revision>
      <id>991554666</id>
      <parentid>987497695</parentid>
      <timestamp>2020-11-30T17:47:13Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor />
      <comment>[[User:Monkbot/task 18|Task 18 (cosmetic)]]: eval 108 templates: del empty params (57×); hyphenate params (8×); del |url-status= (1×);</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="110721" xml:space="preserve">{{short description|Loudspeaker designed to reproduce low-pitched audio frequencies}}
[[File:Bravox E2k.jpg|thumb|250px|12-inch (30&amp;nbsp;cm) subwoofer [[Speaker driver|driver]] (loudspeaker). A driver is commonly installed in an enclosure (often a wooden cabinet) to prevent the sound waves coming off the back of the driver diaphragm from canceling out the sound waves being generated from the front of the subwoofer.]]

A '''subwoofer''' (or '''sub''') is a [[loudspeaker]] designed to reproduce low-pitched audio [[frequency|frequencies]] known as [[bass (sound)|bass]] and [[sub-bass]], lower in frequency than those which can be (optimally) generated by a [[woofer]]. The typical frequency range for a subwoofer is about 20–200&amp;nbsp;Hz for consumer products,&lt;ref name=&quot;CrutchfieldGlossary&quot;&gt;{{cite web|url=http://www.crutchfield.com/S-guVIljONa12/Learn/learningcenter/home/speakers_glossary.html |title=Home Speakers Glossary |last=Barstow |first=Loren |date=January 18, 2010 |work=Learn: Home|publisher=Crutchfield New Media, LLC |accessdate=April 24, 2010}}&lt;/ref&gt; below 100&amp;nbsp;Hz for professional live sound,&lt;ref name=Young2008/&gt; and below 80&amp;nbsp;Hz in [[THX]]-certified systems.&lt;ref name=DellaSala&gt;{{cite web|url=http://www.audioholics.com/tweaks/get-good-bass/setting-the-subwoofer-lfe-crossover-for-best-performance|title=Setting the Subwoofer / LFE Crossover for Best Performance |last=DellaSala|first=Gene|date=August 29, 2004|work=Tips &amp; Tricks: Get Good Bass|publisher=Audioholics|accessdate=March 3, 2010}}&lt;/ref&gt; Subwoofers are never used alone, as they are intended to ''augment'' the low-frequency range of loudspeakers that cover the higher frequency bands. While the term &quot;subwoofer&quot; technically only refers to the speaker driver, in common parlance, the term often refers to a subwoofer driver mounted in a [[speaker enclosure]] (cabinet), often with a built-in amplifier.

Subwoofers are made up of one or more [[woofer]]s mounted in a [[loudspeaker enclosure]]—often made of wood—capable of withstanding air pressure while resisting deformation. Subwoofer enclosures come in a variety of designs, including [[bass reflex]] (with a port or vent), using a subwoofer and one or more [[passive radiator speaker]]s in the enclosure, [[acoustic suspension]] (sealed enclosure), [[Loudspeaker enclosure#Infinite baffle|infinite baffle]], [[Horn loudspeaker|horn-loaded]], [[Loudspeaker enclosure#Tapped horn|tapped horn]], [[Loudspeaker enclosure#Transmission line|transmission line]], [[Loudspeaker enclosure#Compound or band-pass|bandpass]] or [[Isobaric loudspeaker|isobaric]] designs, representing unique trade-offs with respect to efficiency, low-frequency range, cabinet size and cost. Passive subwoofers have a subwoofer driver and enclosure and they are powered by an external amplifier. Active subwoofers include a built-in amplifier.&lt;ref&gt;{{cite web|url=http://www.home-theater-designers.com/glossary.html |title=Glossary of Terms |work=Home Theater Design |publisher=ETS-eTech |page=1 |access-date=March 3, 2010 |url-status=dead |archive-url=https://web.archive.org/web/20120723231911/http://www.home-theater-designers.com/glossary.html |archive-date=July 23, 2012 }}&lt;/ref&gt;

The first home audio subwoofers were developed in the 1960s to add bass response to home stereo systems. Subwoofers came into greater popular consciousness in the 1970s with the introduction of [[Sensurround]] in movies such as [[Earthquake (1974 film)|''Earthquake'']], which produced loud low-frequency sounds through large subwoofers. With the advent of the [[compact cassette]] and the [[compact disc]] in the 1980s, the easy reproduction of deep ''and'' loud bass was no longer limited by the ability of a [[phonograph record]] stylus to track a groove,&lt;ref name=&quot;aes.org&quot;&gt;{{cite web |url=http://www.aes.org/e-lib/browse.cfm?elib=905 |work=AES E-Library |title=Tracking Ability Specifications for Phonograph Cartridges |last=Kogen |first=J. H. |date=October 1967 |publisher=[[Audio Engineering Society]] |accessdate=April 24, 2010}}&lt;/ref&gt; and producers could add more low-frequency content to recordings. As well, during the 1990s, DVDs were increasingly recorded with &quot;[[surround sound]]&quot; processes that included a [[low-frequency effects]] (LFE) channel, which could be heard using the subwoofer in [[home cinema|home theater]] systems. During the 1990s, subwoofers also became increasingly popular in home [[stereo system]]s, custom [[car audio]] installations, and in [[PA system]]s. By the 2000s, subwoofers became almost universal in [[sound reinforcement system]]s in nightclubs and concert venues.

==History==
[[File:Infinity Servo Statik subwoofer.jpg|thumb|right|250px|View of the underside of the downward-firing [[Infinity (audio)|Infinity]] Servo Statik 1, showing the size of the 18-inch (45 cm) custom-wound Cerwin Vega driver in relation to a can of Diet Coke, to show scale.]]

===1920s to 1950s precursors===
From about 1900 to the 1950s, the &quot;lowest frequency in practical use&quot; in recordings, broadcasting and music playback was 100&amp;nbsp;Hz.&lt;ref name=&quot;Fink, Robert 2018. p. 105&quot;&gt;Fink, Robert. &quot;Below 100 Hz: Towards a Musicology of Bass Culture&quot;. In ''The Relentless Pursuit of Tone: Timbre in Popular Music'', eds. Fink, Robert; Latour, Melinda; Wallmark, Zachary. Oxford University Press, 2018. p. 105&lt;/ref&gt; When sound was developed for motion pictures, the basic RCA sound system was a single 8&quot; speaker mounted in straight horn, an approach which was deemed unsatisfactory by Hollywood decisionmakers, who hired [[Western Electric]] engineers to develop a better speaker system.&lt;ref name=&quot;ReferenceA&quot;&gt;Eargle, John. M. ''The JBL Story – 60 Years of Audio Innovation''.&lt;/ref&gt; The early Western Electric experiments added a set of 18&quot; drivers for the low end in a large, open-backed baffle (extending the range down to 50&amp;nbsp;Hz) and a high-frequency unit, but [[Metro-Goldwyn-Mayer|MGM]] was not pleased with the sound of the three-way system, as they had concerns about the delay between the different drivers.&lt;ref name=&quot;ReferenceA&quot;/&gt;

In 1933, the head of MGM's sound department, [[Douglas Shearer]], worked with [[John Kenneth Hilliard|John Hilliard]] and [[James B. Lansing]] (who would later found [[Altec Lansing]] in 1941 and [[JBL (company)|JBL]] in 1946) to develop a new speaker system that used a two-way enclosure with a W-shaped bass horn that could go as low as 40&amp;nbsp;Hz.&lt;ref name=&quot;ReferenceA&quot;/&gt; The Shearing-Lansing 500-A ended up being used in &quot;screening rooms, dubbing theaters, and early sound reinforcement&quot;.&lt;ref name=&quot;ReferenceA&quot;/&gt; In the late 1930s, Lansing created a smaller two-way speaker with a 15&quot; woofer in a vented enclosure, which he called the Iconic system; it was used as a studio monitor and in high-end home hi-fi set-ups.&lt;ref name=&quot;ReferenceA&quot;/&gt;

During the 1940s [[swing era]], to get deeper bass, &quot;pipelike opening[s]&quot; were cut into speaker enclosures, creating [[bass reflex]] enclosures, as it was found that even a fairly inexpensive speaker enclosure, once modified in this way, could &quot;transmit the driving power of a heavy...drumbeat—and sometimes not much else—to a crowded dancefloor.&quot;&lt;ref name=&quot;Fink, Robert 2018. p. 105&quot;/&gt; Prior to the development of the first subwoofers, woofers were used to reproduce bass frequencies, usually with a crossover point set at 500&amp;nbsp;Hz and a 15&quot; loudspeaker in an infinite baffle or in professional sound applications, a  &quot;hybrid horn-loaded&quot; bass reflex enclosure (such as the 15&quot; [[Altec Lansing]] A-7 enclosure nicknamed the &quot;Voice of the Theater&quot;, which was introduced in 1946).&lt;ref name=&quot;Hill, Adam J. 2010&quot;&gt;Hill, Adam J.; Hawksford, Malcolm O. J.; Rosenthal, Adam P.; Gand, Gary. &quot;Subwoofer positioning, orientation and calibration for large-scale sound reinforcement&quot;. Audio Engineering Society Convention Paper 7981, presented at the 128th Convention, May 22–25, 2010, London, UK&lt;/ref&gt; In the mid-1950s, the [[Academy of Motion Picture Arts and Sciences]] selected the &quot;big, boxy&quot; Altec A-7 as the industry standard for movie sound reproduction in theaters.&lt;ref&gt;{{cite web |url=https://www.soundandvision.com/content/living-legend-altec-lansings-%E2%80%98voice-theatre%E2%80%99 |title=Living Legend: Altec Lansing's 'Voice of the Theatre'|author=&lt;!--Not stated--&gt; |date=15 December 2016 |website=www.soundandvision.com |publisher=Sound and Vision |access-date=1 January 2019 }}&lt;/ref&gt;
