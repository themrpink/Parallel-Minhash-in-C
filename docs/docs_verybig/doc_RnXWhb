Substituting this in the second equation gives:

:&lt;math&gt;\frac{\partial \ell(r,p)}{\partial r} = \left[\sum_{i=1}^N \psi(k_i + r)\right] - N\psi(r) + N\ln\left(\frac{r}{r + \sum_{i=1}^N k_i/N}\right) = 0&lt;/math&gt;

This equation cannot be solved for ''r'' in [[Closed-form expression|closed form]]. If a numerical solution is desired, an iterative technique such as [[Newton's method]] can be used. Alternatively, the [[expectation–maximization algorithm]] can be used.&lt;ref name=&quot;aramidis1999&quot; /&gt;

==Occurrence and applications==

===Waiting time in a Bernoulli process===

For the special case where ''r'' is an integer, the negative binomial distribution is known as the '''Pascal distribution'''. It is the probability distribution of a certain number of failures and successes in a series of [[Independent identically-distributed random variables|independent and identically distributed]] Bernoulli trials. For ''k''&amp;nbsp;+&amp;nbsp;''r'' [[Bernoulli trial]]s with success probability ''p'', the negative binomial gives the probability of ''k'' successes and ''r'' failures, with a failure on the last trial. In other words, the negative binomial distribution is the probability distribution of the number of successes before the ''r''th failure in a [[Bernoulli process]], with probability ''p'' of successes on each trial. A Bernoulli process is a [[discrete random variable|discrete]] time process, and so the number of trials, failures, and successes are integers.

Consider the following example. Suppose we repeatedly throw a die, and consider a 1 to be a &quot;failure&quot;.  The probability of success on each trial is 5/6. The number of successes before the third failure belongs to the infinite set { 0,&amp;nbsp;1,&amp;nbsp;2,&amp;nbsp;3,&amp;nbsp;... }. That number of successes is a negative-binomially distributed random variable.

When ''r'' = 1 we get the probability distribution of number of successes before the first failure (i.e. the probability of the first failure occurring on the (''k''&amp;nbsp;+&amp;nbsp;1)st trial), which is a [[geometric distribution]]:
: &lt;math&gt;
    f(k; r, p) = (1-p) \cdot p^k \!
  &lt;/math&gt;

===Overdispersed Poisson===

The negative binomial distribution, especially in its alternative parameterization described above, can be used as an alternative to the Poisson distribution.  It is especially useful for discrete data over an unbounded positive range whose sample [[variance]] exceeds the sample [[mean]]. In such cases, the observations are [[Overdispersion|overdispersed]] with respect to a Poisson distribution, for which the mean is equal to the variance. Hence a Poisson distribution is not an appropriate model.  Since the negative binomial distribution has one more parameter than the Poisson, the second parameter can be used to adjust the variance independently of the mean. See [[Cumulant#Cumulants of some discrete probability distributions|Cumulants of some discrete probability distributions]].

An application of this is to annual counts of [[tropical cyclone]]s in the [[Atlantic Ocean|North Atlantic]] or to monthly to 6-monthly counts of wintertime [[extratropical cyclone]]s over Europe, for which the variance is greater than the mean.&lt;ref&gt;{{cite journal|last=Villarini |first=G. |author2=Vecchi, G.A. |author3=Smith, J.A.|year=2010 |title=Modeling of the dependence of tropical storm counts in the North Atlantic Basin on climate indices |journal=[[Monthly Weather Review]] |volume=138 |issue=7 |pages=2681–2705 |doi=10.1175/2010MWR3315.1  |url= |accessdate= |pmid=|bibcode=2010MWRv..138.2681V |doi-access=free }}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Mailier |first=P.J. |author2=Stephenson, D.B. |author3=Ferro, C.A.T. |author4= Hodges, K.I. |year=2006 |title=Serial Clustering of Extratropical Cyclones |journal=[[Monthly Weather Review]] |volume=134 |issue=8 |pages=2224–2240 |doi=10.1175/MWR3160.1 |url= |accessdate= |pmid=|bibcode=2006MWRv..134.2224M }}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Vitolo |first=R. |author2=Stephenson, D.B. |author3=Cook, Ian M. |author4= Mitchell-Wallace, K. |year=2009 |title=Serial clustering of intense European storms |journal=[[Meteorologische Zeitschrift]] |volume=18 |issue=4 |pages=411–424 |doi=10.1127/0941-2948/2009/0393 |url= https://semanticscholar.org/paper/817e6732db9189e727a5138b957825f2b5715dea|accessdate= |pmid=|bibcode=2009MetZe..18..411V |s2cid=67845213 }}&lt;/ref&gt;  In the case of modest overdispersion, this may produce substantially similar results to an overdispersed Poisson distribution.&lt;ref&gt;{{cite book  | last = McCullagh | first = Peter | authorlink= Peter McCullagh |author2=Nelder, John |authorlink2=John Nelder  | title = Generalized Linear Models |edition=Second | publisher = Boca Raton: Chapman and Hall/CRC | year = 1989 | isbn = 978-0-412-31760-6 |ref=McCullagh1989}}&lt;/ref&gt;&lt;ref&gt;{{cite book | last = Cameron | first = Adrian C. | author2 = Trivedi, Pravin K. | title = Regression analysis of count data | publisher = Cambridge University Press | year = 1998 | isbn = 978-0-521-63567-7 | ref = Cameron1998 | url-access = registration | url = https://archive.org/details/regressionanalys00came }}&lt;/ref&gt;

The negative binomial distribution is also commonly used to model data in the form of discrete sequence read counts from high-throughput RNA and DNA sequencing experiments.&lt;ref&gt;
{{cite journal|last=Robinson |first=M.D. |author2=Smyth, G.K. |year=2007 |title=Moderated statistical tests for assessing differences in tag abundance. |journal=[[Bioinformatics]] |volume=23 |issue=21 |pages=2881–2887 |doi=10.1093/bioinformatics/btm453 |pmid=17881408|doi-access=free }}
&lt;/ref&gt;&lt;ref&gt;
{{cite web |url=http://www.bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.pdf |title=Differential analysis of count data – the DESeq2 package |last1=Love |first1=Michael |last2=Anders |first2=Simon |date=October 14, 2014 |publisher= |accessdate=October 14, 2014}}
&lt;/ref&gt;&lt;ref&gt;
{{cite web |url=http://www.bioconductor.org/packages/release/bioc/vignettes/edgeR/inst/doc/edgeRUsersGuide.pdf |title=edgeR: differential expression analysis of digital gene expression data |last1=Chen |first1=Yunshun |last2=Davis |first2=McCarthy |date=September 25, 2014 |publisher= |accessdate=October 14, 2014}}&lt;/ref&gt;

==History==

This distribution was first studied in 1713, by Montmort, as the distribution of the number of trials required in an experiment to obtain a given number of successes.&lt;ref name=Montmort1713&gt;Montmort PR de (1713) Essai d'analyse sur les jeux de hasard. 2&lt;sup&gt;nd&lt;/sup&gt; ed. Quillau, Paris&lt;/ref&gt; It had previously been mentioned by [[Blaise Pascal|Pascal]].&lt;ref name=Pascal1679&gt;Pascal B (1679) Varia Opera Mathematica. D. Petri de Fermat. Tolosae&lt;/ref&gt;

==See also==
* [[Coupon collector's problem]]
* [[Beta negative binomial distribution]]
* [[Extended negative binomial distribution]]
* [[Negative multinomial distribution]]
* [[Binomial distribution]]
* [[Poisson distribution]]
* [[Exponential family]]
* [[Vector generalized linear model]]
* [[Compound Poisson distribution]]

== References ==
{{Reflist}}

{{-}}
{{ProbDistributions|discrete-infinite}}

{{DEFAULTSORT:Negative Binomial Distribution}}
[[Category:Discrete distributions]]
[[Category:Exponential family distributions]]
[[Category:Compound probability distributions]]
[[Category:Factorial and binomial topics]]
[[Category:Infinitely divisible probability distributions]]</text>
      <sha1>7ybxkv6bq7rth91has3ghyu71379594</sha1>
    </revision>
  </page>
  <page>
    <title>Process (computing)</title>
    <ns>0</ns>
    <id>45178</id>
    <revision>
      <id>990746272</id>
      <parentid>988358636</parentid>
      <timestamp>2020-11-26T07:48:06Z</timestamp>
      <contributor>
        <ip>223.196.166.73</ip>
      </contributor>
      <comment>added citations</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15822" xml:space="preserve">{{short description|Particular execution of a computer program}}
[[File:Htop.png|thumb|right|400px|A list of processes as displayed by [[htop]]]]

In computing, a '''process''' is the [[Instance (computer science)|instance]] of a [[computer program]] that is being executed by one or many threads. It contains the program code and its activity. Depending on the [[operating system]] (OS), a process may be made up of multiple [[thread (computing)|threads of execution]] that execute instructions [[Concurrency (computer science)|concurrently]].&lt;ref name=&quot;OSC Chap4&quot;&gt;{{cite book |last1=Silberschatz |first1=Abraham |authorlink=Abraham Silberschatz |last2=Cagne |first2=Greg |last3=Galvin |first3=Peter Baer |date=2004 |title=Operating system concepts with Java |edition=Sixth |chapter=Chapter 4. Processes |publisher=[[John Wiley &amp; Sons]] |isbn=0-471-48905-0}}&lt;/ref&gt;&lt;ref name=&quot;Vah96&quot;&gt;{{cite book |last=Vahalia |first=Uresh |date=1996 |title=UNIX Internals: The New Frontiers |chapter=Chapter 2. The Process and the Kernel |publisher=Prentice-Hall Inc. |isbn=0-13-101908-2 |url-access=registration |url=https://archive.org/details/unixinternalsnew00vaha }}&lt;/ref&gt;

While a computer program is a passive collection of [[Instruction set|instructions]], a process is the actual execution of those instructions. Several processes may be associated with the same program; for example, opening up several instances of the same program often results in more than one process being executed.

[[Computer multitasking|Multitasking]] is a method to allow multiple processes to share [[Central processing unit|processors]] (CPUs) and other system resources. Each CPU (core) executes a single [[Task (computing)|task]] at a time. However, multitasking allows each processor to [[context switch|switch]] between tasks that are being executed without having to wait for each task to finish ([[Preemption (computing)|preemption]]). Depending on the operating system implementation, switches could be performed when tasks initiate and wait for completion of [[input/output]] operations, when a task voluntarily yields the CPU, on hardware [[interrupt]]s, and when the operating system scheduler decides that a process has expired its fair share of CPU time (e.g, by the [[Completely Fair Scheduler]] of the [[Linux kernel]]).

A common form of multitasking is provided by CPU's [[time-sharing]] that is a method for interleaving the execution of users processes and threads, and even of independent kernel tasks - although the latter feature is feasible only in preemptive [[Kernel (operating system)|kernels]] such as [[Linux kernel|Linux]]. Preemption has an important side effect for interactive process that are given higher priority with respect to CPU bound processes, therefore users are immediately assigned computing resources at the simple pressing of a key or when moving a mouse. Furthermore, applications like video and music reproduction are given some kind of real-time priority, preempting any other lower priority process. In time-sharing systems, [[context switch]]es are performed rapidly, which makes it seem like multiple processes are being executed simultaneously on the same processor. This simultaneous execution of multiple processes is called [[Concurrency (computer science)|concurrency]].

For security and reliability, most modern [[operating system]]s prevent direct [[inter-process communication|communication]] between independent processes, providing strictly mediated and controlled inter-process communication functionality. It's the main chip in a computer it processes instruction, performs calculations, and manages flow of information through a computer system it communicates with input, output, and storage systems to perform task.

==Representation==
[[File:KSysGuard Process Table Screenshot.png|alt=A process table as displayed by KDE System Guard|thumb|401x401px|A process table as displayed by [[KDE System Guard]]]]
In general, a computer system process consists of (or is said to ''own'') the following resources:
* An ''image'' of the executable [[machine code]] associated with a program.
* Memory (typically some region of [[virtual memory]]); which includes the executable code, process-specific data (input and output), a [[call stack]] (to keep track of active [[subroutine]]s and/or other events), and a [[Memory management#Dynamic memory allocation|heap]] to hold intermediate computation data generated during run time.
* Operating system descriptors of resources that are allocated to the process, such as [[file descriptor]]s ([[Unix]] terminology) or [[Handle (computing)|handles]] ([[Microsoft Windows|Windows]]), and data sources and sinks.
* [[Computer security|Security]] attributes, such as the process owner and the process' set of permissions (allowable operations).
* [[Central processing unit|Processor]] state ([[context (computing)|context]]), such as the content of [[processor register|registers]] and physical memory addressing. The ''state'' is typically stored in computer registers when the process is executing, and in memory otherwise.&lt;ref name=&quot;OSC Chap4&quot;/&gt;

The operating system holds most of this information about active processes in data structures called [[process control block]]s.  Any subset of the resources, typically at least the processor state, may be associated with each of the process' [[Thread (computer science)|threads]] in operating systems that support threads or ''child'' processes.

The operating system keeps its processes separate and allocates the resources they need, so that they are less likely to interfere with each other and cause system failures (e.g., [[deadlock]] or [[thrashing (computer science)|thrashing]]). The operating system may also provide mechanisms for [[inter-process communication]] to enable processes to interact in safe and predictable ways.

==Multitasking and process management==
{{Main|Process management (computing)}}

A [[Computer multitasking|multitasking]] [[operating system]] may just switch between processes to give the appearance of many processes [[Execution (computing)|executing]] simultaneously (that is, in [[Parallel computing|parallel]]), though in fact only one process can be executing at any one time on a single [[Central processing unit|CPU]] (unless the CPU has multiple cores, then [[Thread (computing)#Multithreading|multithreading]] or other similar technologies can be used).{{Efn|Some modern CPUs combine two or more independent processors in a [[Multi-core processor|multi-core]] configuration and can execute several processes simultaneously. Another technique called [[simultaneous multithreading]] (used in [[Intel]]'s [[Hyper-threading]] technology) can simulate simultaneous execution of multiple processes or threads.}}

It is usual to associate a single process with a main program, and child processes with any spin-off, parallel processes, which behave like [[Asynchrony (computer programming)|asynchronous]] subroutines.&lt;ref&gt;{{Cite web|last=Hu$tle|first=Blogger's|date=2020-11-06|title=Downloading Youtube Videos|url=https://bloggershustle.medium.com/how-to-download-youtube-videos-1581019562b4|url-status=live|archive-url=|archive-date=|access-date=2020-11-26|website=Medium|language=en}}&lt;/ref&gt; A process is said to ''own'' resources, of which an ''image'' of its program (in memory) is one such resource. However, in multiprocessing systems ''many'' processes may run off of, or share, the same [[Reentrancy (computing)|reentrant]] program at the same location in memory, but each process is said to own its own ''image'' of the program.

Processes are often called &quot;tasks&quot; in [[embedded system|embedded]] operating systems. The sense of &quot;process&quot; (or task) is &quot;something that takes up time&quot;, as opposed to &quot;memory&quot;, which is &quot;something that takes up space&quot;.{{Efn|Tasks and processes refer essentially to the same entity. And, although they have somewhat different terminological histories, they have come to be used as synonyms. Today, the term process is generally preferred over task, except when referring to &quot;multitasking&quot;, since the alternative term, &quot;multiprocessing&quot;, is too easy to confuse with multiprocessor (which is a computer with two or more CPUs).}}

The above description applies to both processes managed by an operating system, and processes as defined by [[process calculus|process calculi]].

If a process requests something for which it must wait, it will be blocked. When the process is in the [[Process state|blocked state]], it is eligible for swapping to disk, but this is transparent in a [[virtual memory]] system, where regions of a process's memory may be really on disk and not in [[Computer data storage#Primary storage|main memory]] at any time. Note that even portions of active processes/tasks (executing programs) are eligible for swapping to disk, if the portions have not been used recently. Not all parts of an executing program and its data have to be in physical memory for the associated process to be active.

===Process states===
{{Main|Process state}}
[[File:Process states.svg|right|300px|thumb|The various process states, displayed in a [[state diagram]], with arrows indicating possible transitions between states.]]

An operating system [[Kernel (computing)|kernel]] that allows multitasking needs processes to have [[process states|certain states]]. Names for these states are not standardised, but they have similar functionality.&lt;ref name=&quot;OSC Chap4&quot;/&gt;

* First, the process is &quot;created&quot; by being loaded from a [[Auxiliary memory|secondary storage]] device ([[hard disk drive]], [[CD-ROM]], etc.) into [[main memory]]. After that the [[Scheduling (computing)|process scheduler]] assigns it the &quot;waiting&quot; state.
* While the process is &quot;waiting&quot;, it waits for the [[scheduling (computing)|scheduler]] to do a so-called [[context switch]]. The context switch loads the process into the processor and changes the state to &quot;running&quot; while the previously &quot;running&quot; process is stored in a &quot;waiting&quot; state.
* If a process in the &quot;running&quot; state needs to wait for a resource (wait for user input or file to open, for example), it is assigned the &quot;blocked&quot; state. The process state is changed back to &quot;waiting&quot; when the process no longer needs to wait (in a blocked state).
* Once the process finishes execution, or is terminated by the operating system, it is no longer needed. The process is removed instantly or is moved to the &quot;terminated&quot; state. When removed, it just waits to be removed from main memory.&lt;ref name=&quot;OSC Chap4&quot; /&gt;&lt;ref name=&quot;Stallings&quot;&gt;{{cite book |last=Stallings |first=William |date=2005 |title=Operating Systems: internals and design principles |edition=5th |publisher=Prentice Hall |isbn=0-13-127837-1}} (particularly chapter 3, section 3.2, &quot;process states&quot;, including figure 3.9 &quot;process state transition with suspend states&quot;)&lt;/ref&gt;

==Inter-process communication==
{{Main|Inter-process communication}}

When processes need to communicate with each other they must share parts of their [[address space]]s or use other forms of inter-process communication (IPC).
For instance in a shell pipeline, the output of the first process need to pass to the second one, and so on; another example is a task that can be decomposed into cooperating but partially independent processes which can run at once (i.e., using concurrency, or true parallelism - the latter model is a particular case of concurrent execution and is feasible whenever enough CPU cores are available for all the processes that are ready to run).

It is even possible for two or more processes to be running on different machines that may run different operating system (OS), therefore some mechanisms for communication and synchronization (called [[communications protocol]]s for distributed computing) are needed (e.g., the Message Passing Interface, often simply called [[Message Passing Interface|MPI]]).

==History==
{{See also|History of operating systems}}

By the early 1960s, computer control software had evolved from [[monitor control software]], for example [[IBM 7090/94 IBSYS|IBSYS]], to [[executive control software]]. Over time, computers got faster while [[Time-sharing|computer time]] was still neither cheap nor fully utilized; such an environment made [[Computer multitasking#Multiprogramming|multiprogramming]] possible and necessary. Multiprogramming means that several programs run [[Concurrency (computer science)|concurrently]]. At first, more than one program ran on a single processor, as a result of underlying [[uniprocessor system|uniprocessor]] computer architecture, and they shared scarce and limited hardware resources; consequently, the concurrency was of a ''serial'' nature. On later systems with [[Multiprocessing|multiple processors]], multiple programs may run concurrently in ''[[Parallel computing|parallel]]''.

Programs consist of sequences of instructions for processors. A single processor can run only one instruction at a time: it is impossible to run more programs at the same time. A program might need some [[System resource|resource]], such as an input device, which has a large delay, or a program might start some slow operation, such as sending output to a printer. This would lead to processor being &quot;idle&quot; (unused). To keep the processor busy at all times, the execution of such a program is halted and the operating system switches the processor to run another program. To the user, it will appear that the programs run at the same time (hence the term &quot;parallel&quot;).

Shortly thereafter, the notion of a &quot;program&quot; was expanded to the notion of an &quot;executing program and its context&quot;. The concept of a process was born, which also became necessary with the invention of [[Reentrancy (computing)|re-entrant code]]. [[Thread (computer science)|Threads]] came somewhat later. However, with the advent of concepts such as [[time-sharing]], [[computer network]]s, and multiple-CPU [[shared memory]] computers, the old &quot;multiprogramming&quot; gave way to true [[Computer multitasking|multitasking]], multiprocessing and, later, [[Thread (computing)#Multithreading|multithreading]].

==See also==

{{div col|colwidth=22em}}
* [[Child process]]
* [[Exit (system call)|Exit]]
* [[Fork (system call)|Fork]]
* [[Light-weight process]]
* [[Orphan process]]
* [[Parent process]]
* [[Process group]]
* [[wait (system call)|Wait]]
* [[Working directory]]
* [[Zombie process]]
{{div col end}}

==Notes==
{{notelist|30em}}

==References==
{{reflist|30em}}

==Further reading==
* Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau (2014). &quot;[https://web.archive.org/web/20200916133128/https://pages.cs.wisc.edu/~remzi/OSTEP/ Operating Systems: Three Easy Pieces]&quot;. Arpaci-Dusseau Books. Relevant chapters: [https://web.archive.org/web/20200916133128/https://pages.cs.wisc.edu/~remzi/OSTEP/cpu-intro.pdf Abstraction: The Process] [https://web.archive.org/web/20200916133128/https://pages.cs.wisc.edu/~remzi/OSTEP/cpu-api.pdf The Process API]
* Gary D. Knott (1974) ''[http://doi.acm.org/10.1145/775280.775282 A proposal for certain process management and intercommunication primitives]'' ACM SIGOPS Operating Systems Review. Volume 8, Issue 4 (October 1974). pp.&amp;nbsp;7 – 44

==External links==
{{Wikiversity|at=Operating Systems/Process and Thread|Processes and Threads}}
*{{Commonscatinline}}
*[http://www.processlibrary.com/ Online Resources For Process Information]
*[http://www.file.net/ Computer Process Information Database and Forum]
*[https://osnote.space/process-models-with-process-creation-termination-methods.html Process Models with Process Creation &amp; Termination Methods]

{{Parallel Computing}}
{{Operating System}}
{{Authority control}}

{{DEFAULTSORT:Process (computing)}}
[[Category:Process (computing)| ]]
[[Category:Concurrent computing]]
[[Category:Operating system technology]]</text>
      <sha1>1zc80ppucusp6dpi3b2ajh72h80p7d2</sha1>
    </revision>
  </page>
  <page>
    <title>Dresden, Germany</title>
    <ns>0</ns>
    <id>45180</id>
    <redirect title="Dresden" />
    <revision>
      <id>15940466</id>
      <parentid>36601</parentid>
      <timestamp>2002-03-24T08:43:17Z</timestamp>
      <contributor>
        <username>David Parker</username>
        <id>42</id>
      </contributor>
      <comment>*</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21" xml:space="preserve">#REDIRECT [[Dresden]]</text>
      <sha1>lfvfz73lzv7juask85ke5cmkwb3635c</sha1>
    </revision>
  </page>
  <page>
    <title>Clara Schumann</title>
    <ns>0</ns>
    <id>45181</id>
    <revision>
      <id>988967909</id>
      <parentid>988429944</parentid>
      <timestamp>2020-11-16T08:34:33Z</timestamp>
      <contributor>
        <username>Aza24</username>
        <id>34452882</id>
      </contributor>
      <minor />
      <comment>link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="68336" xml:space="preserve">{{short description|German musician and composer}}
{{Use dmy dates|date=March 2020}}
{{Use American English|date=December 2019}}
{{Infobox person
| name               = Clara Schumann
| image              = Franz von Lenbach - Clara Schumann (Pastell 1878).jpg
| alt                = Black-on-beige drawing of a woman's face and upper body, with attention to facial features
| caption            = Schumann in 1878
| birth_name         = Clara Josephine Wieck
| birth_date         = {{birth date|df=y|1819|09|13}}
| birth_place        = [[Leipzig]], [[German Confederation]]
| death_date         = {{death date and age|df=y|1896|05|20|1819|09|13}}
| death_place        = [[Frankfurt]], [[German Empire]]
| occupation         = {{plainlist|
* Pianist
* Composer
* Piano teacher
}}
| organization       = [[Hoch Conservatory|Dr. Hoch's Konservatorium]]
| spouse             = {{Marriage|[[Robert Schumann]]|1840|1856|reason=died}}
| children           = 8, including [[Eugenie Schumann]]
| parents            = {{plainlist|
* [[Friedrich Wieck]]
* [[Mariane Bargiel|Mariane Wieck]]
}}
| relatives          = [[Marie Wieck]] (paternal half-sister)&lt;br/&gt;[[Woldemar Bargiel]] (maternal half-brother)
}}
