If {{nowrap|'''Y''' {{=}} '''c''' + '''BX'''}} is an [[affine transformation]] of &lt;math&gt;\mathbf{X}\ \sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma),&lt;/math&gt; where '''c''' is an &lt;math&gt;M \times 1&lt;/math&gt; vector of constants and '''B''' is a constant &lt;math&gt;M \times N&lt;/math&gt; matrix, then '''Y''' has a multivariate normal distribution with expected value {{nowrap|'''c''' + '''Bμ'''}} and variance '''BΣB'''&lt;sup&gt;T&lt;/sup&gt; i.e., &lt;math&gt;\mathbf{Y} \sim \mathcal{N} \left(\mathbf{c} + \mathbf{B} \boldsymbol\mu, \mathbf{B} \boldsymbol\Sigma \mathbf{B}^{\rm T}\right)&lt;/math&gt;. In particular, any subset of the ''X&lt;sub&gt;i&lt;/sub&gt;'' has a marginal distribution that is also multivariate normal.
To see this, consider the following example: to extract the subset (''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, ''X''&lt;sub&gt;4&lt;/sub&gt;)&lt;sup&gt;T&lt;/sup&gt;, use

:&lt;math&gt;
\mathbf{B}
=
\begin{bmatrix}
 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; \ldots &amp; 0
\end{bmatrix}
&lt;/math&gt;

which extracts the desired elements directly.

Another corollary is that the distribution of {{nowrap|'''Z''' {{=}} '''b''' · '''X'''}}, where '''b''' is a constant vector with the same number of elements as '''X''' and the dot indicates the [[dot product]], is univariate Gaussian with &lt;math&gt;Z\sim\mathcal{N}\left(\mathbf{b}\cdot\boldsymbol\mu, \mathbf{b}^{\rm T}\boldsymbol\Sigma \mathbf{b}\right)&lt;/math&gt;. This result follows by using

:&lt;math&gt;
\mathbf{B}=\begin{bmatrix}
b_1    &amp; b_2    &amp; \ldots &amp; b_n
\end{bmatrix} = \mathbf{b}^{\rm T}.
&lt;/math&gt;
Observe how the positive-definiteness of '''Σ''' implies that the variance of the dot product must be positive.

An affine transformation of '''X''' such as 2'''X''' is not the same as the [[Sum of normally distributed random variables|sum of two independent realisations]] of '''X'''.

===Geometric interpretation===
{{see also|Confidence region}}

The equidensity contours of a non-singular multivariate normal distribution are [[ellipsoid]]s (i.e. linear transformations of [[hypersphere]]s) centered at the mean.&lt;ref&gt;{{cite web|author=Nikolaus Hansen|title=The CMA Evolution Strategy: A Tutorial|url=http://www.lri.fr/~hansen/cmatutorial.pdf|access-date=2012-01-07|archive-url=https://web.archive.org/web/20100331114258/http://www.lri.fr/~hansen/cmatutorial.pdf|archive-date=2010-03-31|url-status=dead|bibcode=2016arXiv160400772H|year=2016|arxiv=1604.00772}}&lt;/ref&gt; Hence the multivariate normal distribution is an example of the class of [[elliptical distribution]]s. The directions of the principal axes of the ellipsoids are given by the eigenvectors of the covariance matrix &lt;math&gt;\boldsymbol\Sigma&lt;/math&gt;. The squared relative lengths of the principal axes are given by the corresponding eigenvalues.

If {{nowrap|'''Σ''' {{=}} '''UΛU'''&lt;sup&gt;T&lt;/sup&gt; {{=}} '''UΛ'''&lt;sup&gt;1/2&lt;/sup&gt;('''UΛ'''&lt;sup&gt;1/2&lt;/sup&gt;)&lt;sup&gt;T&lt;/sup&gt;}} is an [[eigendecomposition]] where the columns of '''U''' are unit eigenvectors and '''Λ''' is a [[diagonal matrix]] of the eigenvalues, then we have

::&lt;math&gt;\mathbf{X}\ \sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma) \iff \mathbf{X}\ \sim \boldsymbol\mu+\mathbf{U}\boldsymbol\Lambda^{1/2}\mathcal{N}(0, \mathbf{I}) \iff \mathbf{X}\ \sim \boldsymbol\mu+\mathbf{U}\mathcal{N}(0, \boldsymbol\Lambda).&lt;/math&gt;

Moreover, '''U''' can be chosen to be a [[rotation matrix]], as inverting an axis does not have any effect on ''N''(0, '''Λ'''), but inverting a column changes the sign of '''U''''s determinant. The distribution ''N''('''μ''', '''Σ''') is in effect ''N''(0, '''I''') scaled by '''Λ'''&lt;sup&gt;1/2&lt;/sup&gt;, rotated by '''U''' and translated by '''μ'''.

Conversely, any choice of '''μ''', full rank matrix '''U''', and positive diagonal entries Λ&lt;sub&gt;''i''&lt;/sub&gt; yields a non-singular multivariate normal distribution. If any Λ&lt;sub&gt;''i''&lt;/sub&gt; is zero and '''U''' is square, the resulting covariance matrix '''UΛU'''&lt;sup&gt;T&lt;/sup&gt; is [[singular matrix|singular]]. Geometrically this means that every contour ellipsoid is infinitely thin and has zero volume in ''n''-dimensional space, as at least one of the principal axes has length of zero; this is the [[degenerate distribution|degenerate case]].

&quot;The radius around the true mean in a bivariate normal random variable, re-written in [[polar coordinates]] (radius and angle), follows a [[Hoyt distribution]].&quot;&lt;ref&gt;{{cite web |title=The Hoyt Distribution (Documentation for R package 'shotGroups' version 0.6.2) |author=Daniel Wollschlaeger |url=http://finzi.psych.upenn.edu/usr/share/doc/library/shotGroups/html/hoyt.html }}{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

In one dimension the probability of finding a sample of the normal distribution in the interval &lt;math&gt;\mu\pm \sigma&lt;/math&gt; is approximately 68.27%, but in higher dimensions the probability of finding a sample in the region of the standard deviation ellipse is lower.&lt;ref&gt;{{Cite journal|last=Wang|first=Bin|last2=Shi|first2=Wenzhong|last3=Miao|first3=Zelang|date=2015-03-13|editor-last=Rocchini|editor-first=Duccio|title=Confidence Analysis of Standard Deviational Ellipse and Its Extension into Higher Dimensional Euclidean Space|url=https://dx.plos.org/10.1371/journal.pone.0118537|journal=PLOS ONE|language=en|volume=10|issue=3|pages=e0118537|doi=10.1371/journal.pone.0118537|issn=1932-6203|pmc=4358977|pmid=25769048}}&lt;/ref&gt;
{| class=&quot;wikitable&quot;
! Dimensionality !! Probability 
|-
| 1 || 0.6827 
|-
|2 ||0.3935 
|-
|3 ||0.1987 
|-
|4 ||0.0902 
|-
|5 ||0.0374 
|-
|6 ||0.0144 
|-
|7 ||0.0052 
|-
|8 ||0.0018 
|-
|9 ||0.0006 
|-
|10 ||0.0002
|}

==Statistical inference==
===Parameter estimation===
{{further|Estimation of covariance matrices}}
The derivation of the [[maximum likelihood|maximum-likelihood]] [[estimator]] of the covariance matrix of a multivariate normal distribution is straightforward.

In short, the  probability density function (pdf) of a multivariate normal is

:&lt;math&gt;f(\mathbf{x})= \frac{1}{\sqrt { (2\pi)^k|\boldsymbol \Sigma| } }  \exp\left(-{1 \over 2} (\mathbf{x}-\boldsymbol\mu)^{\rm T} \boldsymbol\Sigma^{-1} ({\mathbf x}-\boldsymbol\mu)\right)&lt;/math&gt;

and the ML estimator of the covariance matrix from a sample of ''n'' observations is

:&lt;math&gt;\widehat{\boldsymbol\Sigma} = {1 \over n}\sum_{i=1}^n ({\mathbf x}_i-\overline{\mathbf x})({\mathbf x}_i-\overline{\mathbf x})^T&lt;/math&gt;

which is simply the [[sample covariance matrix]].  This is a [[biased estimator]] whose expectation is

:&lt;math&gt;E[\widehat{\boldsymbol\Sigma}] = \frac{n-1}{n} \boldsymbol\Sigma.&lt;/math&gt;

An unbiased sample covariance is

:&lt;math&gt;\widehat{\boldsymbol\Sigma} = {1 \over n-1}\sum_{i=1}^n (\mathbf{x}_i-\overline{\mathbf{x}})(\mathbf{x}_i-\overline{\mathbf{x}})^{\rm T}. 
        = {1 \over n-1} [X'(I - \frac{1}{n}*J) X] &lt;/math&gt;  (matrix form; I is Identity matrix, J is matrix of ones)

The [[Fisher information matrix]] for estimating the parameters of a multivariate normal distribution has a closed form expression. This can be used, for example, to compute the [[Cramér–Rao bound]] for parameter estimation in this setting. See [[Fisher information#Multivariate normal distribution|Fisher information]] for more details.

===Bayesian inference===
In [[Bayesian statistics]], the [[conjugate prior]] of the mean vector is another multivariate normal distribution, and the conjugate prior of the covariance matrix is an [[inverse-Wishart distribution]] &lt;math&gt;\mathcal{W}^{-1}&lt;/math&gt; .  Suppose then that ''n'' observations have been made
:&lt;math&gt;\mathbf{X} = \{\mathbf{x}_1,\dots,\mathbf{x}_n\} \sim \mathcal{N}(\boldsymbol\mu,\boldsymbol\Sigma)&lt;/math&gt;
and that a conjugate prior has been assigned, where
:&lt;math&gt;p(\boldsymbol\mu,\boldsymbol\Sigma)=p(\boldsymbol\mu\mid\boldsymbol\Sigma)\ p(\boldsymbol\Sigma),&lt;/math&gt;
where
:&lt;math&gt;p(\boldsymbol\mu\mid\boldsymbol\Sigma) \sim\mathcal{N}(\boldsymbol\mu_0,m^{-1}\boldsymbol\Sigma) ,&lt;/math&gt;
and
:&lt;math&gt;p(\boldsymbol\Sigma) \sim \mathcal{W}^{-1}(\boldsymbol\Psi,n_0).&lt;/math&gt;

Then,{{citation needed|date=July 2012}}

:&lt;math&gt;
\begin{array}{rcl}
p(\boldsymbol\mu\mid\boldsymbol\Sigma,\mathbf{X}) &amp; \sim &amp; \mathcal{N}\left(\frac{n\bar{\mathbf{x}} + m\boldsymbol\mu_0}{n+m},\frac{1}{n+m}\boldsymbol\Sigma\right),\\
p(\boldsymbol\Sigma\mid\mathbf{X}) &amp; \sim &amp; \mathcal{W}^{-1}\left(\boldsymbol\Psi+n\mathbf{S}+\frac{nm}{n+m}(\bar{\mathbf{x}}-\boldsymbol\mu_0)(\bar{\mathbf{x}}-\boldsymbol\mu_0)', n+n_0\right),
\end{array}
&lt;/math&gt;
where
:&lt;math&gt;
\begin{align}
\bar{\mathbf{x}} &amp; = \frac{1}{n}\sum_{i=1}^{n} \mathbf{x}_i ,\\
\mathbf{S} &amp; = \frac{1}{n}\sum_{i=1}^{n} (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})' .
\end{align}
&lt;/math&gt;

=== Multivariate normality tests ===

Multivariate normality tests check a given set of data for similarity to the multivariate [[normal distribution]].  The [[null hypothesis]] is that the [[data set]] is similar to the normal distribution, therefore a sufficiently small [[p-value|''p''-value]] indicates non-normal data. Multivariate normality tests include the Cox–Small test&lt;ref&gt;{{Cite journal | last1 = Cox | first1 = D. R. | last2 = Small | first2 = N. J. H. | doi = 10.1093/biomet/65.2.263 | title = Testing multivariate normality | journal = Biometrika | volume = 65 | issue = 2 | pages = 263 | year = 1978 }}&lt;/ref&gt;
and Smith and Jain's adaptation&lt;ref&gt;{{Cite journal | last1 = Smith | first1 = S. P. | last2 = Jain | first2 = A. K. | doi = 10.1109/34.6789 | title = A test to determine the multivariate normality of a data set | journal = IEEE Transactions on Pattern Analysis and Machine Intelligence | volume = 10 | issue = 5 | pages = 757 | year = 1988 }}&lt;/ref&gt; of the Friedman–Rafsky test created by [[Larry rafsky|Larry Rafsky]] and [[Jerome H. Friedman|Jerome Friedman]].&lt;ref&gt;{{Cite journal | last1 = Friedman | first1 = J. H. | last2 = Rafsky | first2 = L. C. | doi = 10.1214/aos/1176344722 | title = Multivariate Generalizations of the Wald–Wolfowitz and Smirnov Two-Sample Tests | journal = The Annals of Statistics | volume = 7 | issue = 4 | pages = 697 | year = 1979 }}&lt;/ref&gt;

'''Mardia's test'''&lt;ref name=Mardia/&gt; is based on multivariate extensions of [[skewness]] and [[kurtosis]] measures. For a sample {'''x'''&lt;sub&gt;1&lt;/sub&gt;, ..., '''x'''&lt;sub&gt;''n''&lt;/sub&gt;} of ''k''-dimensional vectors we compute
: &lt;math&gt;\begin{align}
  &amp; \widehat{\boldsymbol\Sigma} = {1 \over n} \sum_{j=1}^n \left(\mathbf{x}_j - \bar{\mathbf{x}}\right)\left(\mathbf{x}_j - \bar{\mathbf{x}}\right)^T \\

  &amp; A = {1 \over 6n} \sum_{i=1}^n \sum_{j=1}^n \left[ (\mathbf{x}_i - \bar{\mathbf{x}})^T\;\widehat{\boldsymbol\Sigma}^{-1} (\mathbf{x}_j - \bar{\mathbf{x}}) \right]^3 \\

  &amp; B = \sqrt{\frac{n}{8k(k+2)}}\left\{{1 \over n} \sum_{i=1}^n \left[ (\mathbf{x}_i - \bar{\mathbf{x}})^T\;\widehat{\boldsymbol\Sigma}^{-1} (\mathbf{x}_i - \bar{\mathbf{x}}) \right]^2 - k(k+2) \right\}
  \end{align}&lt;/math&gt;
Under the null hypothesis of multivariate normality, the statistic ''A'' will have approximately a [[chi-squared distribution]] with {{nowrap|{{frac2|1|6}}⋅''k''(''k'' + 1)(''k'' + 2)}} degrees of freedom, and ''B'' will be approximately [[standard normal]] ''N''(0,1).

Mardia's kurtosis statistic is skewed and converges very slowly to the limiting normal distribution.  For medium size samples &lt;math&gt;(50 \le n &lt; 400)&lt;/math&gt;, the parameters of the asymptotic distribution of the kurtosis statistic are modified&lt;ref&gt;Rencher (1995), pages 112–113.&lt;/ref&gt; For small sample tests (&lt;math&gt;n&lt;50&lt;/math&gt;) empirical critical values are used. Tables of critical values for both statistics are given by Rencher&lt;ref&gt;Rencher (1995), pages 493–495.&lt;/ref&gt; for ''k''&amp;nbsp;=&amp;nbsp;2,&amp;nbsp;3,&amp;nbsp;4.

Mardia's tests are affine invariant but not consistent.  For example, the multivariate skewness test is not consistent against
symmetric non-normal alternatives.&lt;ref&gt;{{Cite journal | last1 = Baringhaus | first1 = L. | last2 = Henze | first2 = N. | doi = 10.1016/0047-259X(91)90031-V | title = Limit distributions for measures of multivariate skewness and kurtosis based on projections | journal = Journal of Multivariate Analysis | volume = 38 | pages = 51–69 | year = 1991 }}&lt;/ref&gt;

The '''BHEP test'''&lt;ref name=BH/&gt; computes the norm of the difference between the empirical [[characteristic function (probability theory)|characteristic function]] and the theoretical characteristic function of the normal distribution. Calculation of the norm is performed in the [[Lp space|L&lt;sup&gt;2&lt;/sup&gt;(''μ'')]] space of square-integrable functions with respect to the Gaussian weighting function &lt;math&gt;\scriptstyle \mu_\beta(\mathbf{t}) = (2\pi\beta^2)^{-k/2} e^{-|\mathbf{t}|^2/(2\beta^2)}&lt;/math&gt;. The test statistic is
: &lt;math&gt;\begin{align}
    T_\beta &amp;= \int_{\mathbb{R}^k} \left| {1 \over n} \sum_{j=1}^n e^{i\mathbf{t}^T\widehat{\boldsymbol\Sigma}^{-1/2}(\mathbf{x}_j - \bar{\mathbf{x})}} - e^{-|\mathbf{t}|^2/2} \right|^2 \; \boldsymbol\mu_\beta(\mathbf{t}) \, d\mathbf{t} \\
            &amp;= {1 \over n^2} \sum_{i,j=1}^n e^{-{\beta^2 \over 2}(\mathbf{x}_i-\mathbf{x}_j)^T\widehat{\boldsymbol\Sigma}^{-1}(\mathbf{x}_i-\mathbf{x}_j)} - \frac{2}{n(1 + \beta^2)^{k/2}}\sum_{i=1}^n e^{ -\frac{\beta^2}{2(1+\beta^2)} (\mathbf{x}_i-\bar{\mathbf{x}})^T\widehat{\boldsymbol\Sigma}^{-1}(\mathbf{x}_i-\bar{\mathbf{x}})} + \frac{1}{(1 + 2\beta^2)^{k/2}}
  \end{align}&lt;/math&gt;
The limiting distribution of this test statistic is a weighted sum of chi-squared random variables,&lt;ref name=BH/&gt; however in practice it is more convenient to compute the sample quantiles using the Monte-Carlo simulations.{{citation needed|date=July 2012}}

A detailed survey of these and other test procedures is available.&lt;ref name=Henze/&gt;

==Computational methods==
===Drawing values from the distribution===

A widely used method for drawing (sampling) a random vector '''x''' from the ''N''-dimensional multivariate normal distribution with mean vector '''μ''' and [[covariance matrix]] '''Σ''' works as follows:&lt;ref name=Gentle/&gt;

# Find any real matrix '''A''' such that {{nowrap|'''A'''&amp;thinsp;'''A'''&lt;sup&gt;T&lt;/sup&gt; {{=}} '''Σ'''}}. When '''Σ''' is positive-definite, the [[Cholesky decomposition]] is typically used, and the [[Cholesky decomposition#Avoiding taking square roots|extended form]] of this decomposition can always be used  (as the covariance matrix may be only positive semi-definite) in both cases a suitable matrix '''A''' is obtained. An alternative is to use the matrix '''A''' = '''UΛ'''&lt;sup&gt;½&lt;/sup&gt; obtained from a [[Eigendecomposition of a matrix#Real symmetric matrices|spectral decomposition]] '''Σ''' = '''UΛU'''&lt;sup&gt;−1&lt;/sup&gt; of '''Σ'''. The former approach is more computationally straightforward but the matrices '''A''' change for different orderings of the elements of the random vector, while the latter approach gives matrices that are related by simple re-orderings. In theory both approaches give equally good ways of determining a suitable matrix '''A''', but there are differences in computation time.
# Let {{nowrap|'''z''' {{=}} (''z''&lt;sub&gt;1&lt;/sub&gt;, …, ''z&lt;sub&gt;N&lt;/sub&gt;'')&lt;sup&gt;T&lt;/sup&gt;}} be a vector whose components are ''N'' [[statistical independence|independent]] [[normal distribution|standard normal]] variates (which can be generated, for example, by using the [[Box–Muller transform]]).
# Let '''x''' be {{nowrap|'''μ''' + '''Az'''}}. This has the desired distribution due to the affine transformation property.

== See also ==
* [[Chi distribution]], the [[probability density function|pdf]] of the [[Norm (mathematics)#p-norm|2-norm]] (or [[Euclidean norm]]) of a multivariate normally distributed vector (centered at zero).
* [[Complex normal distribution]], an application of bivariate normal distribution
*  [[Gaussian copula|Copula]], for the definition of the Gaussian or normal copula model.
* [[Multivariate t-distribution]], which is another widely used spherically symmetric multivariate distribution.
* [[Multivariate stable distribution]] extension of the multivariate normal distribution, when the index (exponent in the characteristic function) is between zero and two.
* [[Mahalanobis distance]]
* [[Wishart distribution]]
* [[Matrix normal distribution]]

== References ==
{{Reflist|refs=

&lt;ref name = Siotani&gt;{{cite journal
  | last = Siotani |first=Minoru
  | title = Tolerance regions for a multivariate normal population
  | journal = Annals of the Institute of Statistical Mathematics
  | year = 1964
  | volume = 16
  | number = 1
  | pages = 135–153
  | doi = 10.1007/BF02868568
  | url = http://www.ism.ac.jp/editsec/aism/pdf/016_1_0135.pdf
  }}&lt;/ref&gt;

&lt;ref name=Mardia&gt;{{cite journal
  | last = Mardia | first = K. V.
  | year = 1970
  | title = Measures of multivariate skewness and kurtosis with applications
  | journal = Biometrika
  | volume = 57 | issue = 3  | pages = 519–530
  | doi = 10.1093/biomet/57.3.519
  }}&lt;/ref&gt;

&lt;!--
&lt;ref name=EP&gt;{{cite journal
  | last1 = Epps | first1 = Lawrence B.
  | last2 = Pulley  | first2 = Lawrence B.
  | year = 1983
  | title = A test for normality based on the empirical characteristic function
  | journal = Biometrika
  | volume = 70 | issue = 3 | pages = 723–726
  | doi = 10.1093/biomet/70.3.723
  }}&lt;/ref&gt;
--&gt;

&lt;ref name=BH&gt;{{cite journal
  | last1 = Baringhaus | first1 = L.
  | last2 = Henze      | first2 = N.
  | year = 1988
  | title = A consistent test for multivariate normality based on the empirical characteristic function
  | journal = Metrika
  | volume = 35 | issue = 1 | pages = 339–348
  | doi = 10.1007/BF02613322
  }}&lt;/ref&gt;

&lt;ref name=Henze&gt;{{cite journal
  | last = Henze | first = Norbert
  | year = 2002
  | title = Invariant tests for multivariate normality: a critical review
  | journal = Statistical Papers
  | volume = 43 | issue = 4   | pages = 467–506
  | doi = 10.1007/s00362-002-0119-6
  }}&lt;/ref&gt;

&lt;ref name=HT&gt;{{cite journal
  | last1 = Hamedani | first1 = G. G.
  | last2 = Tata     | first2 = M. N.
  | year = 1975
  | title = On the determination of the bivariate normal distribution from distributions of linear combinations of the variables
  | journal = The American Mathematical Monthly
  | volume = 82 | issue = 9   | pages = 913–915
  | doi = 10.2307/2318494
  | jstor = 2318494
 }}&lt;/ref&gt;

&lt;ref name=wyattlms&gt;{{cite web |last=Wyatt |first=John
  | title=Linear least mean-squared error estimation
  | url=http://web.mit.edu/6.041/www/LECTURE/lec22.pdf
  | work=Lecture notes course on applied probability
  | date=November 26, 2008
  | archive-date=October 10, 2015
  | archive-url=https://web.archive.org/web/20151010114443/http://web.mit.edu/6.041/www/LECTURE/lec22.pdf
  | access-date=23 January 2012 }}&lt;/ref&gt;

&lt;ref name=rao&gt;{{cite book
  | last = Rao |first=C. R. |author-link=C. R. Rao
  | title =  Linear Statistical Inference and Its Applications
  | year = 1973
  | publisher = Wiley
  | location = New York
  | pages = 527–528
  | isbn = 0-471-70823-2
  }}&lt;/ref&gt;

&lt;ref name=Gentle&gt;{{cite book
  | last = Gentle |first=J. E.
  | title =  Computational Statistics
  | year = 2009
  | publisher = Springer
  | location = New York
  | pages = 315–316
  | doi = 10.1007/978-0-387-98144-4
  | series = Statistics and Computing
 | isbn = 978-0-387-98143-7
 | url = http://cds.cern.ch/record/1639470
 }}&lt;/ref&gt;
