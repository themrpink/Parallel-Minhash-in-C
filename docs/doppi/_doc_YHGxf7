Typically, the input text will first be passed to an acoustic feature generator, then the acoustic features are passed to the neural vocoder. For the acoustic feature generator, the [[Loss function]] is typically L1 or L2 loss. These loss functions put a constraint that the output acoustic feature distributions must be Gaussian or Laplacian. In practice, since the human voice band ranges from approximately 300 to 4000 Hz, the loss function will be designed to have more penality on this range:

&lt;math&gt;loss=\alpha \text{loss}_{\text{human}} + (1 - \alpha) \text{loss}_{\text{other}}&lt;/math&gt;

where &lt;math&gt;\text{loss}_{\text{human}}&lt;/math&gt; is the loss from human voice band and &lt;math&gt;\alpha&lt;/math&gt; is a scalar typically around 0.5. The acoustic feature is typically [[Spectrogram]] or spectrogram in [[Mel scale]]. These features capture the time-frequency relation of speech signal and thus, it is sufficient to generate intelligent outputs with these acoustic features. The [[Mel-frequency cepstrum]] feature used in the speech recognition task is not suitable for speech synthesis because it reduces too much information.

==== Brief history ====
In September 2016, [[DeepMind]] proposed [https://arxiv.org/abs/1609.03499 WaveNet], a deep generative model of raw audio waveforms. This shows the community that deep learning-based models have the capability to model raw waveforms and perform well on generating speech from acoustic features like spectrograms or spectrograms in mel scale, or even from some preprocessed linguistic features. In early 2017, [[Mila (research institute)]] proposed [https://mila.quebec/en/publication/char2wav-end-to-end-speech-synthesis/ char2wav], a model to produce raw waveform in an end-to-end method. Also, [[Google]] and [[Facebook]] proposed [https://arxiv.org/abs/1703.10135 Tacotron] and [https://arxiv.org/abs/1707.06588 VoiceLoop], respectively, to generate acoustic features directly from the input text. In the later in the same year, Google proposed [https://arxiv.org/abs/1712.05884 Tacotron2] which combined the WaveNet vocoder with the revised Tacotron architecture to perform end-to-end speech synthesis. Tacotron2 can generate high-quality speech approaching the human voice. Since then, end-to-end methods became the hottest research topic because many researchers around the world start to notice the power of the end-to-end speech synthesizer.

==== Advantages and disadvantages ====

The advantages of end-to-end methods are as follows:
* Only need a single model to perform text analysis, acoustic modeling and audio synthesis, i.e. synthesizing speech directly from characters
* Less feature engineering
* Easily allows for rich conditioning on various attributes, e.g. speaker or language
* Adaptation to new data is easier
* More robust than multi-stage models because no component's error can compound
* Powerful model capacity to capture the hidden internal structures of data
* Capable to generate intelligible and natural speech
* No need to maintain a large database, i.e. small footprint

Despite the many advantages mentioned, end-to-end methods still have many challenges to be solved:
* Auto-regressive-based models suffer from slow inference problem
* Output speech are not robust when data are not sufficient
* Lack of controllability compared with traditional concatenative and statistical parametric approaches
* Tend to learn the flat prosody by averaging over training data
* Tend to output smoothed acoustic features because the l1 or l2 loss is used

==== Challenges ====

- ''Slow inference problem''

To solve the slow inference problem, [[Microsoft]] research and [[Baidu]] research both proposed using non-auto-regressive models to make the inference process faster. The [https://arxiv.org/abs/1905.09263 FastSpeech] model proposed by Microsoft use Transformer architecture with a duration model to achieve the goal. Besides, the duration model which borrows from traditional methods makes the speech production more robust.

- ''Robustness problem''

Researchers found that the robustness problem is strongly related to the text alignment failures, and this drives many researchers to revise the attention mechanism which utilize the strong local relation and monotonic properties of speech. 

- ''Controllability problem''

To solve the controllability problem, many works about variational auto-encoder are proposed.&lt;ref&gt;{{cite arXiv |last=Hsu |first=Wei-Ning |eprint=1810.07217 |title=Hierarchical Generative Modeling for Controllable Speech Synthesis |class=cs.CL |date=2018 }}&lt;/ref&gt;&lt;ref&gt;{{cite arXiv |last=Habib |first=Raza |eprint=1910.01709 |title=Semi-Supervised Generative Modeling for Controllable Speech Synthesis |class=cs.CL |date=2019 }}&lt;/ref&gt;

- ''Flat prosody problem''

GST-Tacotron can slightly alleviate the flat prosody problem, however, it still depends on the training data.

- ''Smoothed acoustic output problem''

To generate more realistic acoustic features, GAN learning strategy can be applied.

However, in practice, neural vocoder can generalize well even when the input features are more smooth than real data.

==== Semi-supervised learning ====

Currently, self-supervised learning gain a lot of attention because of better utilizing unlabelled data. Research&lt;ref&gt;{{cite arXiv |last=Chung |first=Yu-An |eprint=1808.10128 |title=Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis |class=cs.CL |date=2018 }}&lt;/ref&gt;&lt;ref&gt;{{cite arXiv |last=Ren |first=Yi |eprint=1905.06791 |title=Almost Unsupervised Text to Speech and Automatic Speech Recognition |class=cs.CL |date=2019 }}&lt;/ref&gt; shows that with the aid of self-supervised loss, the need of paired data decreases.

==== Zero-shot speaker adaptation ====

Zero-shot speaker adaptation is promising because a single model can generate speech with various speaker styles and characteristic. In June 2018, Google proposed to use pre-trained speaker verification model as speaker encoder to extract speaker embedding&lt;ref&gt;{{cite arXiv |last=Jia |first=Ye |eprint=1806.04558 |title=Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis |class=cs.CL |date=2018 }}&lt;/ref&gt;. The speaker encoder then becomes a part of the neural text-to-speech model and it can decide the style and characteristic of the output speech. This shows the community that only using a single model to generate speech of multiple style is possible.

==== Neural vocoder ====

Neural vocoder plays a important role in deep learning-based speech synthesis to generate high-quality speech from acoustic features. The [[WaveNet]] model proposed in 2016 achieves great performance on speech quality. Wavenet factorised the joint probability of a waveform &lt;math&gt;\mathbf{x}=\{x_1,...,x_T\}&lt;/math&gt; as a product of conditional probabilities as follows

&lt;math&gt;p_{\theta}(\mathbf{x})=\prod_{t=1}^{T}p(x_t|x_1,...,x_{t-1})&lt;/math&gt;

Where &lt;math&gt;\theta&lt;/math&gt; is the model parameter including many dilated convolution layers. Therefore, each audio sample &lt;math&gt;x_t&lt;/math&gt; is therefore conditioned on the samples at all previous timesteps. However, the auto-regressive nature of WaveNet makes the inference process dramatically slow. To solve the slow inference problem that comes from the auto-regressive characteristic of WaveNet model, Parallel WaveNet&lt;ref&gt;{{cite arXiv |last=van den Oord |first=Aaron  |eprint=1711.10433 |title=Parallel WaveNet: Fast High-Fidelity Speech Synthesis |class=cs.CL |date=2018 }}&lt;/ref&gt; is proposed. Parallel WaveNet is an inverse autoregressive flow-based model which is trained by knowledge distillation with a pre-trained teacher WaveNet model. Since inverse autoregressive flow-based model is non-auto-regressive when performing inference, the inference speed is faster than real-time. In the meanwhile, [[Nvidia]] proposed a flow-based WaveGlow&lt;ref&gt;{{cite arXiv |last=Prenger |first=Ryan |eprint=1811.00002 |title=WaveGlow: A Flow-based Generative Network for Speech Synthesis |class=cs.SD |date=2018 }}&lt;/ref&gt; model which can also generate speech with faster than real-time speed. However, despite the high inference speed, parallel WaveNet has the limitation of the need of a pre-trained WaveNet model and WaveGlow takes many weeks to converge with limited computing devices. This issue is solved by Parallel WaveGAN&lt;ref&gt;{{cite arXiv |last=Yamamoto |first=Ryuichi |eprint=1910.11480 |title=Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram |class=eess.AS |date=2019 }}&lt;/ref&gt; which learns to produce speech by multi-resolution spectral loss and GANs learning strategy.

== Challenges ==

=== Text normalization challenges ===

The process of normalizing text is rarely straightforward. Texts are full of [[heteronym (linguistics)|heteronym]]s, [[number]]s, and [[abbreviation]]s that all require expansion into a phonetic representation. There are many spellings in English which are pronounced differently based on context. For example, &quot;My latest project is to learn how to better project my voice&quot; contains two pronunciations of &quot;project&quot;.

Most text-to-speech (TTS) systems do not generate [[Semantics#Linguistics|semantic]] representations of their input texts, as processes for doing so are unreliable, poorly understood, and computationally ineffective. As a result, various [[heuristic]] techniques are used to guess the proper way to disambiguate [[homograph]]s, like examining neighboring words and using statistics about frequency of occurrence.

Recently TTS systems have begun to use HMMs (discussed above) to generate &quot;[[Part-of-speech tagging|parts of speech]]&quot; to aid in disambiguating homographs. This technique is quite successful for many cases such as whether &quot;read&quot; should be pronounced as &quot;red&quot; implying past tense, or as &quot;reed&quot; implying present tense. Typical error rates when using HMMs in this fashion are usually below five percent. These techniques also work well for most European languages, although access to required training [[Text corpus|corpora]] is frequently difficult in these languages.

Deciding how to convert numbers is another problem that TTS systems have to address. It is a simple programming challenge to convert a number into words (at least in English), like &quot;1325&quot; becoming &quot;one thousand three hundred twenty-five.&quot; However, numbers occur in many different contexts; &quot;1325&quot; may also be read as &quot;one three two five&quot;, &quot;thirteen twenty-five&quot; or &quot;thirteen hundred and twenty five&quot;. A TTS system can often infer how to expand a number based on surrounding words, numbers, and punctuation, and sometimes the system provides a way to specify the context if it is ambiguous.&lt;ref&gt;{{cite web | title = Speech synthesis | publisher = World Wide Web Organization | url = http://www.w3.org/TR/speech-synthesis/#S3.1.8}}&lt;/ref&gt; Roman numerals can also be read differently depending on context. For example, &quot;Henry VIII&quot; reads as &quot;Henry the Eighth&quot;, while &quot;Chapter VIII&quot; reads as &quot;Chapter Eight&quot;.

Similarly, abbreviations can be ambiguous. For example, the abbreviation &quot;in&quot; for &quot;inches&quot; must be differentiated from the word &quot;in&quot;, and the address &quot;12 St John St.&quot; uses the same abbreviation for both &quot;Saint&quot; and &quot;Street&quot;. TTS systems with intelligent front ends can make educated guesses about ambiguous abbreviations, while others provide the same result in all cases, resulting in nonsensical (and sometimes comical) outputs, such as &quot;[[Ulysses S. Grant]]&quot; being rendered as &quot;Ulysses South Grant&quot;.

=== Text-to-phoneme challenges ===

Speech synthesis systems use two basic approaches to determine the pronunciation of a word based on its [[spelling]], a process which is often called text-to-phoneme or [[grapheme]]-to-phoneme conversion ([[phoneme]] is the term used by [[Linguistics|linguists]] to describe distinctive sounds in a [[language]]). The simplest approach to text-to-phoneme conversion is the dictionary-based approach, where a large dictionary containing all the words of a language and their correct [[pronunciation]]s is stored by the program. Determining the correct pronunciation of each word is a matter of looking up each word in the dictionary and replacing the spelling with the pronunciation specified in the dictionary. The other approach is rule-based, in which pronunciation rules are applied to words to determine their pronunciations based on their spellings. This is similar to the &quot;sounding out&quot;, or [[synthetic phonics]], approach to learning reading.

Each approach has advantages and drawbacks. The dictionary-based approach is quick and accurate, but completely fails if it is given a word which is not in its dictionary. As dictionary size grows, so too does the memory space requirements of the synthesis system. On the other hand, the rule-based approach works on any input, but the complexity of the rules grows substantially as the system takes into account irregular spellings or pronunciations. (Consider that the word &quot;of&quot; is very common in English, yet is the only word in which the letter &quot;f&quot; is pronounced {{IPA|[v]}}.) As a result, nearly all speech synthesis systems use a combination of these approaches.

Languages with a [[phonemic orthography]] have a very regular writing system, and the prediction of the pronunciation of words based on their spellings is quite successful. Speech synthesis systems for such languages often use the rule-based method extensively, resorting to dictionaries only for those few words, like foreign names and [[Loanword|borrowings]], whose pronunciations are not obvious from their spellings. On the other hand, speech synthesis systems for languages like [[English language|English]], which have extremely irregular spelling systems, are more likely to rely on dictionaries, and to use rule-based methods only for unusual words, or words that aren't in their dictionaries.

=== Evaluation challenges ===
The consistent evaluation of speech synthesis systems may be difficult because of a lack of universally agreed objective evaluation criteria. Different organizations often use different speech data. The quality of speech synthesis systems also depends on the quality of the production technique (which may involve analogue or digital recording) and on the facilities used to replay the speech. Evaluating speech synthesis systems has therefore often been compromised by differences between production techniques and replay facilities.

Since 2005, however, some researchers have started to evaluate speech synthesis systems using a common speech dataset.&lt;ref&gt;{{cite web|url=http://festvox.org/blizzard |title=Blizzard Challenge |publisher=Festvox.org |access-date=2012-02-22}}&lt;/ref&gt;

=== Prosodics and emotional content ===
{{See also|Prosody (linguistics)}}
A study in the journal ''Speech Communication'' by Amy Drahota and colleagues at the [[University of Portsmouth]], [[UK]], reported that listeners to voice recordings could determine, at better than chance levels, whether or not the speaker was smiling.&lt;ref&gt;{{Cite news|title=Smile -and the world can hear you |date=January 9, 2008 |url=http://www.port.ac.uk/aboutus/newsandevents/news/title,74220,en.html |archive-date=May 17, 2008 |archive-url=https://web.archive.org/web/20080517102201/http://www.port.ac.uk/aboutus/newsandevents/news/title%2C74220%2Cen.html |publisher=University of Portsmouth |url-status=dead }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title=Smile – And The World Can Hear You, Even If You Hide |work=Science Daily |date=January 2008 |url=https://www.sciencedaily.com/releases/2008/01/080111224745.htm}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal
 |last1       = Drahota
 |first1      = A.
 |title       = The vocal communication of different kinds of smile
 |doi         = 10.1016/j.specom.2007.10.001
 |journal     = Speech Communication
 |volume      = 50
 |issue       = 4
 |pages       = 278–287
 |year        = 2008
 |url         = http://peer.ccsd.cnrs.fr/docs/00/49/91/97/PDF/PEER_stage2_10.1016%252Fj.specom.2007.10.001.pdf
 |url-status     = dead
 |archive-url  = https://web.archive.org/web/20130703062330/https://peer.ccsd.cnrs.fr/docs/00/49/91/97/PDF/PEER_stage2_10.1016/j.specom.2007.10.001.pdf
 |archive-date = 2013-07-03
}}&lt;!-- also available here: http://ganymedes.lib.unideb.hu:8080/udpeer/bitstream/2437.2/2984/1/PEER_stage2_10.1016%252Fj.specom.2007.10.001.pdf --&gt;&lt;/ref&gt; It was suggested that identification of the vocal features that signal emotional content may be used to help make synthesized speech sound more natural. One of the related issues is modification of the [[pitch contour]] of the sentence, depending upon whether it is an affirmative, interrogative or exclamatory sentence. One of the techniques for pitch modification&lt;ref&gt;{{cite journal|last1=Muralishankar|first1=R.|last2=Ramakrishnan|first2=A. G.|last3=Prathibha|first3=P.|title=Modification of pitch using DCT in the source domain|journal=Speech Communication|date=February 2004|volume=42|issue=2|pages=143–154|doi=10.1016/j.specom.2003.05.001}}&lt;/ref&gt; uses [[discrete cosine transform]] in the source domain ([[linear prediction]] residual). Such pitch synchronous pitch modification techniques need a priori pitch marking of the synthesis speech database using techniques such as epoch extraction using dynamic [[Plosive|plosion]] index applied on the integrated linear prediction residual of the [[Voice (phonetics)|voiced]] regions of speech.&lt;ref&gt;{{cite journal|last1=Prathosh|first1=A. P.|last2=Ramakrishnan|first2=A. G.|last3=Ananthapadmanabha|first3=T. V.|title=Epoch extraction based on integrated linear prediction residual using plosion index|journal=IEEE Trans. Audio Speech Language Processing|date=December 2013|volume=21|issue=12|pages=2471–2480|doi=10.1109/TASL.2013.2273717|s2cid=10491251}}&lt;/ref&gt;

== Dedicated hardware ==

* [[Icophone]]
* [[General Instrument SP0256-AL2]]
* [[National Semiconductor]] DT1050 Digitalker (Mozer – [[Forrest Mozer]])
* [[Texas Instruments LPC Speech Chips]]&lt;ref name=&quot;TI will exit dedicated speech-synthesis chips, transfer products to Sensory&quot;&gt;EE Times. &quot;[http://www.eetimes.com/electronics-news/4102385/TI-will-exit-dedicated-speech-synthesis-chips-transfer-products-to-Sensory TI will exit dedicated speech-synthesis chips, transfer products to Sensory] {{Webarchive|url=https://www.webcitation.org/65WOqYC8t?url=http://www.eetimes.com/electronics-news/4102385/TI-will-exit-dedicated-speech-synthesis-chips-transfer-products-to-Sensory |date=2012-02-17 }}.&quot; June 14, 2001.&lt;/ref&gt;

==Hardware and software systems==
Popular systems offering speech synthesis as a built-in capability.

===Mattel===
The [[Mattel]] [[Intellivision]] game console offered the [[Intellivoice]] Voice Synthesis module in 1982. It included the [[General Instrument SP0256|SP0256 Narrator]] speech synthesizer chip on a removable cartridge. The Narrator had 2kB of Read-Only Memory (ROM), and this was utilized to store a database of generic words that could be combined to make phrases in Intellivision games. Since the Orator chip could also accept speech data from external memory, any additional words or phrases needed could be stored inside the cartridge itself. The data consisted of strings of analog-filter coefficients to modify the behavior of the chip's synthetic vocal-tract model, rather than simple digitized samples.

===SAM===
[[File:C64 Software Automatic Mouth demo.flac|thumb|A demo of SAM on the C64]]
Also released in 1982, [[Software Automatic Mouth]] was the first commercial all-software voice synthesis program. It was later used as the basis for [[Macintalk]]. The program was available for non-Macintosh Apple computers (including the Apple II, and the Lisa), various Atari models and the Commodore 64. The Apple version preferred additional hardware that contained DACs, although it could instead use the computer's one-bit audio output (with the addition of much distortion) if the card was not present. The Atari made use of the embedded POKEY audio chip. Speech playback on the Atari normally disabled interrupt requests and shut down the ANTIC chip during vocal output. The audible output is extremely distorted speech when the screen is on. The Commodore 64 made use of the 64's embedded SID audio chip.

=== Atari ===
Arguably, the first speech system integrated into an [[operating system]] was the 1400XL/1450XL personal computers designed by [[Atari, Inc.]] using the Votrax SC01 chip in 1983. The 1400XL/1450XL computers used a Finite State Machine to enable World English Spelling text-to-speech synthesis.&lt;ref&gt;{{cite web|url=http://www.atarimuseum.com/ahs_archives/archives/pdf/computers/8bits/1400xlmodem.pdf |title=1400XL/1450XL Speech Handler External Reference Specification |access-date=2012-02-22}}&lt;/ref&gt; Unfortunately, the 1400XL/1450XL personal computers never shipped in quantity.

The [[Atari ST]] computers were sold with &quot;stspeech.tos&quot; on floppy disk.

=== Apple ===

The first speech system integrated into an [[operating system]] that shipped in quantity was [[Apple Computer]]'s [[PlainTalk#Original MacInTalk|MacInTalk]]. The software was licensed from 3rd party developers Joseph Katz and Mark Barton (later, SoftVoice, Inc.) and was featured during the 1984 introduction of the Macintosh computer. This January demo required 512 kilobytes of RAM memory. As a result, it could not run in the 128 kilobytes of RAM the first Mac actually shipped with.&lt;ref name=&quot;demo&quot;&gt;{{cite web|url=http://www.folklore.org/StoryView.py?story=Intro_Demo.txt |title=It Sure Is Great To Get Out Of That Bag! |publisher=folklore.org |access-date=2013-03-24}}&lt;/ref&gt; So, the demo was accomplished with a prototype 512k Mac, although those in attendance were not told of this and the synthesis demo created considerable excitement for the Macintosh. In the early 1990s Apple expanded its capabilities offering system wide text-to-speech support. With the introduction of faster PowerPC-based computers they included higher quality voice sampling. Apple also introduced [[speech recognition]] into its systems which provided a fluid command set. More recently, Apple has added sample-based voices. Starting as a curiosity, the speech system of Apple [[Macintosh]] has evolved into a fully supported program, [[PlainTalk]], for people with vision problems. [[VoiceOver]] was for the first time featured in 2005 in [[Mac OS X Tiger]] (10.4). During 10.4 (Tiger) and first releases of 10.5 ([[Mac OS X Leopard|Leopard]]) there was only one standard voice shipping with Mac OS X. Starting with 10.6 ([[Mac OS X Snow Leopard|Snow Leopard]]), the user can choose out of a wide range list of multiple voices. VoiceOver voices feature the taking of realistic-sounding breaths between sentences, as well as improved clarity at high read rates over PlainTalk. Mac OS X also includes [[say (software)|say]], a [[Command-line interface|command-line based]] application that converts text to audible speech. The [[AppleScript]] Standard Additions includes a [[say (software)|say]] verb that allows a script to use any of the installed voices and to control the pitch, speaking rate and modulation of the spoken text.
