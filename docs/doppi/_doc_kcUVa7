==Formal definition==
(''This definition may be extended to any probability distribution using the [[measure theory|measure-theoretic]] [[probability axioms|definition of probability]].'')

A [[random variable]] &lt;math&gt;X&lt;/math&gt; with values in a [[measurable space]] &lt;math&gt;(\mathcal{X}, \mathcal{A})&lt;/math&gt; (usually &lt;math&gt;\mathbb{R}^n&lt;/math&gt; with the [[Borel set]]s as measurable subsets) has as [[probability distribution#Formal definition|probability distribution]] the measure ''X''&lt;sub&gt;∗&lt;/sub&gt;''P'' on &lt;math&gt;(\mathcal{X}, \mathcal{A})&lt;/math&gt;: the '''density''' of &lt;math&gt;X&lt;/math&gt; with respect to a reference measure &lt;math&gt;\mu&lt;/math&gt; on &lt;math&gt;(\mathcal{X}, \mathcal{A})&lt;/math&gt; is the [[Radon–Nikodym derivative]]:

:&lt;math&gt;f = \frac{d X_*P}{d \mu} .&lt;/math&gt;

That is, ''f'' is any measurable function with the property that:

:&lt;math&gt;\Pr [X \in A ] = \int_{X^{-1}A} \, d P = \int_A f \, d \mu&lt;/math&gt;

for any measurable set &lt;math&gt;A \in \mathcal{A}.&lt;/math&gt;

===Discussion===
In the [[#Continuous univariate random variable|continuous univariate case above]], the reference measure is the [[Lebesgue measure]]. The [[probability mass function]] of a [[discrete random variable]] is the density with respect to the [[counting measure]] over the sample space (usually the set of [[integer]]s, or some subset thereof).

It is not possible to define a density with reference to an arbitrary measure (e.g. one can't choose the counting measure as a reference for a continuous random variable). Furthermore, when it does exist, the density is [[almost everywhere]] unique.

==Further details==
Unlike a probability, a probability density function can take on values greater than one; for example, the [[Uniform distribution (continuous)|uniform distribution]] on the interval [0,&amp;nbsp;½] has probability density ''f''(''x'')&amp;nbsp;=&amp;nbsp;2 for 0&amp;nbsp;≤&amp;nbsp;''x''&amp;nbsp;≤&amp;nbsp;½ and ''f''(''x'')&amp;nbsp;=&amp;nbsp;0 elsewhere.

The standard [[normal distribution]] has probability density
: &lt;math&gt;
    f(x) = \frac{1}{\sqrt{2\pi}}\; e^{-x^2/2}.
  &lt;/math&gt;

If a random variable ''X'' is given and its distribution admits a probability density function ''f'', then the [[expected value]] of ''X'' (if the expected value exists) can be calculated as
: &lt;math&gt;
    \operatorname{E}[X] = \int_{-\infty}^\infty x\,f(x)\,dx.
  &lt;/math&gt;

Not every probability distribution has a density function: the distributions of [[discrete random variable]]s do not; nor does the [[Cantor distribution]], even though it has no discrete component, i.e., does not assign positive probability to any individual point.

A distribution has a density function if and only if its [[cumulative distribution function]] ''F''(''x'') is [[absolute continuity|absolutely continuous]]. In this case: ''F'' is [[almost everywhere]] [[derivative|differentiable]], and its derivative can be used as probability density:
: &lt;math&gt;
    \frac{d}{dx}F(x) = f(x).
  &lt;/math&gt;

If a probability distribution admits a density, then the probability of every one-point set {''a''} is zero; the same holds for finite and countable sets.

Two probability densities ''f'' and ''g'' represent the same [[probability distribution]] precisely if they differ only on a set of [[Lebesgue measure|Lebesgue]] [[measure zero]].

In the field of [[statistical physics]], a non-formal reformulation of the relation above between the derivative of the [[cumulative distribution function]] and the probability density function is generally used as the definition of the probability density function. This alternate definition is the following:

If ''dt'' is an infinitely small number, the probability that ''X'' is included within the interval (''t'',&amp;nbsp;''t''&amp;nbsp;+&amp;nbsp;''dt'') is equal to ''f''(''t'')&amp;nbsp;''dt'', or:
: &lt;math&gt;
    \Pr(t&lt;X&lt;t+dt) = f(t)\,dt.
  &lt;/math&gt;

==Link between discrete and continuous distributions==

It is possible to represent certain discrete random variables as well as random variables involving both a continuous and a discrete part with a [[Generalized function|generalized]] probability density function, by using the [[Dirac delta function]]. (This is not possible with a probability density function in the sense defined above, it may be done with a [[Distribution_(mathematics)|distribution]].) For example, consider a binary discrete [[random variable]] having the [[Rademacher distribution]]—that is, taking −1 or 1 for values, with probability ½ each. The density of probability associated with this variable is:

:&lt;math&gt;f(t) = \frac{1}{2}(\delta(t+1)+\delta(t-1)).&lt;/math&gt;

More generally, if a discrete variable can take ''n'' different values among real numbers, then the associated probability density function is:

:&lt;math&gt;f(t) = \sum_{i=1}^np_i\, \delta(t-x_i),&lt;/math&gt;

where &lt;math&gt;x_1\ldots,x_n&lt;/math&gt; are the discrete values accessible to the variable and &lt;math&gt;p_1,\ldots,p_n&lt;/math&gt; are the probabilities associated with these values.

This substantially unifies the treatment of discrete and continuous probability distributions.  For instance, the above expression allows for determining statistical characteristics of such a discrete variable (such as its [[mean]], its [[variance]] and its [[kurtosis]]), starting from the formulas given for a continuous distribution of the probability.

== Families of densities ==
It is common for probability density functions (and [[probability mass function]]s) to
be parametrized—that is, to be characterized by unspecified [[parameter]]s.  For example, the [[normal distribution]] is parametrized in terms of the [[mean]] and the [[variance]], denoted by &lt;math&gt;\mu&lt;/math&gt; and &lt;math&gt;\sigma^2&lt;/math&gt; respectively, giving the family of densities

: &lt;math&gt;
   f(x;\mu,\sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} e^{ -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2 }.
&lt;/math&gt;
It is important to keep in mind the difference between the [[Domain of a function|domain]] of a family of densities and the parameters of the family.  Different values of the parameters describe different distributions of different [[random variable]]s on the same [[sample space]] (the same set of all possible values of the variable); this sample space is the domain of the family of random variables that this family of distributions describes.  A given set of parameters describes a single distribution within the family sharing the functional form of the density. From the perspective of a given distribution, the parameters are constants, and terms in a density function that contain only parameters, but not variables, are part of the [[normalization factor]] of a distribution (the multiplicative factor that ensures that the area under the density—the probability of ''something'' in the domain occurring— equals 1). This normalization factor is outside the [[kernel (statistics)|kernel]] of the distribution.

Since the parameters are constants, reparametrizing a density in terms of different parameters, to give a characterization of a different random variable in the family, means simply substituting the new parameter values into the formula in place of the old ones.  Changing the domain of a probability density, however, is trickier and requires more work: see the section below on change of variables.

== Densities associated with multiple variables ==&lt;!-- This section is linked from [[Sufficiency (statistics)]] --&gt;

For continuous [[random variable]]s ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X&lt;sub&gt;n&lt;/sub&gt;'', it is also possible to define a probability density function associated to the set as a whole, often called '''joint probability density function'''. This density function is defined as a function of the ''n'' variables, such that, for any domain ''D'' in the ''n''-dimensional space of the values of the variables ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X&lt;sub&gt;n&lt;/sub&gt;'', the probability that a realisation of the set variables falls inside the domain ''D'' is

:&lt;math&gt;\Pr \left( X_1,\ldots,X_n \isin D \right)
 = \int_D f_{X_1,\ldots,X_n}(x_1,\ldots,x_n)\,dx_1 \cdots dx_n.&lt;/math&gt;

If ''F''(''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''x''&lt;sub&gt;''n''&lt;/sub&gt;) =&amp;nbsp;Pr(''X''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;≤&amp;nbsp;''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''X''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;≤&amp;nbsp;''x''&lt;sub&gt;''n''&lt;/sub&gt;) is the [[cumulative distribution function]] of the vector (''X''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''X''&lt;sub&gt;''n''&lt;/sub&gt;), then the joint probability density function can be computed as a partial derivative
: &lt;math&gt;
    f(x) = \frac{\partial^n F}{\partial x_1 \cdots \partial x_n} \bigg|_x
  &lt;/math&gt;

===Marginal densities===
For ''i'' = 1, 2, ...,''n'', let ''f''&lt;sub&gt;''X''&lt;sub&gt;''i''&lt;/sub&gt;&lt;/sub&gt;(''x''&lt;sub&gt;''i''&lt;/sub&gt;) be the probability density function associated with variable ''X&lt;sub&gt;i&lt;/sub&gt;'' alone.  This is called the marginal density function, and can be deduced from the probability density associated with the random variables ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X&lt;sub&gt;n&lt;/sub&gt;'' by integrating over all values of the other ''n''&amp;nbsp;−&amp;nbsp;1 variables:

:&lt;math&gt;f_{X_i}(x_i) = \int f(x_1,\ldots,x_n)\, dx_1 \cdots dx_{i-1}\,dx_{i+1}\cdots dx_n .&lt;/math&gt;

===Independence===

Continuous random variables ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X&lt;sub&gt;n&lt;/sub&gt;'' admitting a joint density are all [[statistical independence|independent]] from each other if and only if

:&lt;math&gt;f_{X_1,\ldots,X_n}(x_1,\ldots,x_n) = f_{X_1}(x_1)\cdots f_{X_n}(x_n).&lt;/math&gt;

===Corollary===

If the joint probability density function of a vector of ''n'' random variables can be factored into a product of ''n'' functions of one variable

:&lt;math&gt;f_{X_1,\ldots,X_n}(x_1,\ldots,x_n) = f_1(x_1)\cdots f_n(x_n),&lt;/math&gt;

(where each ''f&lt;sub&gt;i&lt;/sub&gt;'' is not necessarily a density) then the ''n'' variables in the set are all [[statistical independence|independent]] from each other, and the marginal probability density function of each of them is given by

:&lt;math&gt;f_{X_i}(x_i) = \frac{f_i(x_i)}{\int f_i(x)\,dx}.&lt;/math&gt;

===Example===

This elementary example illustrates the above definition of multidimensional probability density functions in the simple case of a function of a set of two variables. Let us call &lt;math&gt;\vec R&lt;/math&gt; a 2-dimensional random vector of coordinates (''X'', ''Y''): the probability to obtain &lt;math&gt;\vec R&lt;/math&gt; in the quarter plane of positive ''x'' and ''y'' is

:&lt;math&gt;\Pr \left( X &gt; 0, Y &gt; 0 \right)
 = \int_0^\infty \int_0^\infty f_{X,Y}(x,y)\,dx\,dy.&lt;/math&gt;

==Function of random variables and change of variables in the probability density function==

If the probability density function of a random variable (or vector) ''X'' is given as ''f&lt;sub&gt;X&lt;/sub&gt;''(''x''), it is possible (but often not necessary; see below) to calculate the probability density function of some variable {{nowrap|''Y {{=}} g''(''X'')}}. This is also called a “change of variable” and is in practice used to generate a random variable of arbitrary shape {{nowrap|''f''&lt;sub&gt;''g''(''X'')&lt;/sub&gt; {{=}} ''f&lt;sub&gt;Y&lt;/sub&gt;''}}  using a known (for instance, uniform) random number generator.

It is tempting to think that in order to find the expected value ''E''(''g''(''X'')), one must first find the probability density ''f''&lt;sub&gt;''g''(''X'')&lt;/sub&gt; of the new random variable {{nowrap|''Y {{=}} g''(''X'')}}.  However, rather than computing

: &lt;math&gt;\operatorname E\big(g(X)\big) = \int_{-\infty}^\infty y f_{g(X)}(y)\,dy, &lt;/math&gt;

one may find instead

: &lt;math&gt;\operatorname E\big(g(X)\big) = \int_{-\infty}^\infty g(x) f_X(x)\,dx.&lt;/math&gt;
