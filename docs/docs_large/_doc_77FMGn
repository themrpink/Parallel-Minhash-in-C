The exponential distribution exhibits [[infinite divisibility (probability)|infinite divisibility]].

===Cumulative distribution function===
The [[cumulative distribution function]] is given by

:&lt;math&gt;F(x;\lambda) = \begin{cases}
1-e^{-\lambda x} &amp; x \ge 0, \\
0 &amp; x &lt; 0.
\end{cases}&lt;/math&gt;

===Alternative parametrization===
The exponential distribution is sometimes parametrized in terms of the [[scale parameter]] {{math|1=''&amp;beta;'' = 1/''&amp;lambda;''}}:

:&lt;math&gt;f(x;\beta) = \begin{cases}
   \frac{1}{\beta} e^{-x/\beta} &amp; x \ge 0, \\
    0 &amp; x &lt; 0.
 \end{cases}&lt;/math&gt;

==Properties==

===Mean, variance, moments and median===
[[File:Mean exp.svg|thumb|The mean is the probability mass centre, that is the [[first moment]].]]
[[File:Median exp.svg|thumb|The median is the [[preimage]] ''F''&lt;sup&gt;−1&lt;/sup&gt;(1/2).]]
The mean or [[expected value]] of an exponentially distributed random variable ''X'' with rate parameter λ is given by

:&lt;math&gt;\operatorname{E}[X] = \frac{1}{\lambda}.&lt;/math&gt;

In light of the examples given [[Exponential_distribution#Occurrence_and_applications|below]], this makes sense: if you receive phone calls at an average rate of 2 per hour, then you can expect to wait half an hour for every call.

The [[variance]] of ''X'' is given by

:&lt;math&gt;\operatorname{Var}[X] = \frac{1}{\lambda^2},&lt;/math&gt;

so the [[standard deviation]] is equal to the mean.

The [[Moment (mathematics)|moments]] of ''X'', for &lt;math&gt;n\in\mathbb{N}&lt;/math&gt; are given by

:&lt;math&gt;\operatorname{E}\left[X^n\right] = \frac{n!}{\lambda^n}.&lt;/math&gt;

The [[central moment]]s of ''X'', for &lt;math&gt;n\in\mathbb{N}&lt;/math&gt; are given by

:&lt;math&gt;\mu_n = \frac{!n}{\lambda^n} = \frac{n!}{\lambda^n}\sum^n_{k=0}\frac{(-1)^k}{k!}.&lt;/math&gt;
where !''n'' is the [[Derangement|subfactorial]] of ''n''

The [[median]] of ''X'' is given by

:&lt;math&gt;\operatorname{m}[X] = \frac{\ln(2)}{\lambda} &lt; \operatorname{E}[X],&lt;/math&gt;

where ln refers to the [[natural logarithm]].  Thus the [[absolute difference]] between the mean and median is

:&lt;math&gt;\left|\operatorname{E}\left[X\right] - \operatorname{m}\left[X\right]\right| = \frac{1 - \ln(2)}{\lambda} &lt; \frac{1}{\lambda} = \operatorname{\sigma}[X],&lt;/math&gt;

in accordance with the [[Chebyshev's inequality#An application - distance between the mean and the median|median-mean inequality]].

===Memorylessness===
An exponentially distributed random variable ''T'' obeys the relation

:&lt;math&gt;\Pr \left (T &gt; s + t \mid T &gt; s \right ) = \Pr(T &gt; t), \qquad \forall s, t \ge 0.&lt;/math&gt;

This can be seen by considering the [[Cumulative distribution function#Complementary cumulative distribution function (tail distribution)|complementary cumulative distribution function]]:

:&lt;math&gt; \begin{align}
  \Pr\left(T &gt; s + t \mid T &gt; s\right)
    &amp;= \frac{\Pr\left(T &gt; s + t \cap T &gt; s\right)}{\Pr\left(T &gt; s\right)} \\[4pt]
    &amp;= \frac{\Pr\left(T &gt; s + t \right)}{\Pr\left(T &gt; s\right)} \\[4pt]
    &amp;= \frac{e^{-\lambda(s + t)}}{e^{-\lambda s}} \\[4pt]
    &amp;= e^{-\lambda t} \\[4pt]
    &amp;= \Pr(T &gt; t).
\end{align} &lt;/math&gt;

When ''T'' is interpreted as the waiting time for an event to occur relative to some initial time, this relation implies that, if ''T'' is conditioned on a failure to observe the event over some initial period of time ''s'', the distribution of the remaining waiting time is the same as the original unconditional distribution. For example, if an event has not occurred after 30 seconds, the [[conditional probability]] that occurrence will take at least 10 more seconds is equal to the unconditional probability of observing the event more than 10 seconds after the initial time.

The exponential distribution and the [[geometric distribution]] are [[memorylessness|the only memoryless probability distributions]].

The exponential distribution is consequently also necessarily the only continuous probability distribution that has a constant [[failure rate]].

===Quantiles===
[[File:Tukey anomaly criteria for Exponential PDF.png|thumb|alt=Tukey anomaly criteria for exponential probability distribution function.| Tukey criteria for anomalies.{{citation needed|date=September 2017}}]]

The [[quantile function]] (inverse cumulative distribution function) for Exp(''λ'') is

:&lt;math&gt;F^{-1}(p;\lambda) = \frac{-\ln(1-p)}{\lambda},\qquad 0 \le p &lt; 1&lt;/math&gt;

The [[quartile]]s are therefore:

*first quartile: ln(4/3)/''λ''
*[[median]]: ln(2)/''λ''
*third quartile: ln(4)/''λ''

And as a consequence the [[interquartile range]] is ln(3)/''λ''.

===Kullback–Leibler divergence===
The directed [[Kullback–Leibler divergence]] in [[nat (unit)|nats]] of &lt;math&gt;e^\lambda&lt;/math&gt; (&quot;approximating&quot; distribution) from &lt;math&gt;e^{\lambda_0}&lt;/math&gt; ('true' distribution) is given by

:&lt;math&gt;\begin{align}
\Delta(\lambda_0 \parallel \lambda) 
&amp;= \mathbb{E}_{\lambda_0}\left( \log \frac{p_{\lambda_0}(x)}{p_\lambda(x)}\right)\\
&amp;= \mathbb{E}_{\lambda_0}\left( \log \frac{\lambda_0 e^{-\lambda_0 x}}{\lambda e^{-\lambda x}}\right)\\
&amp;= \log(\lambda_0) - \log(\lambda) - (\lambda_0 - \lambda)E_{\lambda_0}(x)\\
&amp;= \log(\lambda_0) - \log(\lambda) + \frac{\lambda}{\lambda_0} - 1.
\end{align}
&lt;/math&gt;

===Maximum entropy distribution===
Among all continuous probability distributions with [[Support (mathematics)#In probability and measure theory|support]] [0, &amp;infin;) and mean μ, the exponential distribution with &amp;lambda; = 1/μ has the largest [[differential entropy]]. In other words, it is the [[maximum entropy probability distribution]] for a [[random variate]] ''X'' which is greater than or equal to zero and for which E[''X''] is fixed.&lt;ref&gt;{{cite journal |last1=Park |first1=Sung Y. |last2=Bera |first2=Anil K. |year=2009 |title=Maximum entropy autoregressive conditional heteroskedasticity model |journal=Journal of Econometrics |pages=219–230 |publisher=Elsevier |url=http://www.wise.xmu.edu.cn/Master/Download/..%5C..%5CUploadFiles%5Cpaper-masterdownload%5C2009519932327055475115776.pdf |accessdate=2011-06-02 |archive-url=https://web.archive.org/web/20160307144515/http://wise.xmu.edu.cn/uploadfiles/paper-masterdownload/2009519932327055475115776.pdf |archive-date=2016-03-07 |url-status=dead }}&lt;/ref&gt;

===Distribution of the minimum of exponential random variables===
Let ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X''&lt;sub&gt;''n''&lt;/sub&gt; be [[Independent random variables|independent]] exponentially distributed random variables with rate parameters λ&lt;sub&gt;1&lt;/sub&gt;, ..., λ&lt;sub&gt;''n''&lt;/sub&gt;.  Then

:&lt;math&gt;\min\left\{X_1, \dotsc, X_n \right\}&lt;/math&gt;

is also exponentially distributed, with parameter

:&lt;math&gt;\lambda = \lambda_1 + \dotsb + \lambda_n.&lt;/math&gt;

This can be seen by considering the [[Cumulative distribution function#Complementary cumulative distribution function (tail distribution)|complementary cumulative distribution function]]:

:&lt;math&gt;\begin{align}
      &amp;\Pr\left(\min\{X_1, \dotsc, X_n\} &gt; x\right) \\
  ={} &amp;\Pr\left(X_1 &gt; x, \dotsc, X_n &gt; x\right) \\
  ={} &amp;\prod_{i=1}^n \Pr\left(X_i &gt; x\right) \\
  ={} &amp;\prod_{i=1}^n \exp\left(-x\lambda_i\right) = \exp\left(-x\sum_{i=1}^n \lambda_i\right).
\end{align}&lt;/math&gt;

The index of the variable which achieves the minimum is distributed according to the categorical distribution

:&lt;math&gt;\Pr\left(k \mid X_k = \min\{X_1, \dotsc, X_n\}\right) = \frac{\lambda_k}{\lambda_1 + \dotsb + \lambda_n}.&lt;/math&gt;

A proof is as follows:

:&lt;math&gt;\text{Let } I = \operatorname{argmin}_{i \in \{1, \dotsb, n\}}\{X_1, \dotsc, X_n\}&lt;/math&gt;
:&lt;math&gt;\begin{align}
      \text{then } \Pr (I = k) &amp;= \int_{0}^{\infty} \Pr(X_k = x) \Pr(X_{i \neq k} &gt; x ) dx \\
      &amp;= \int_{0}^{\infty} \lambda_k e^{-\lambda_k x} \left(\prod_{i=1, i\neq k}^{n} e^{-\lambda_i x}\right) dx \\
      &amp;= \lambda_k \int_{0}^{\infty}  e^{-\left(\lambda_1 + \dotsb  +\lambda_n\right) x}  dx \\
      &amp;= \frac{\lambda_k}{\lambda_1 + \dotsb + \lambda_n}.
\end{align}&lt;/math&gt;

Note that

:&lt;math&gt;\max\{X_1, \dotsc, X_n\}&lt;/math&gt;

is not exponentially distributed.&lt;ref&gt;{{cite web|last1=Michael|first1=Lugo|title=The expectation of the maximum of exponentials|url=http://www.stat.berkeley.edu/~mlugo/stat134-f11/exponential-maximum.pdf|accessdate=13 December 2016|archive-url=https://web.archive.org/web/20161220132822/https://www.stat.berkeley.edu/~mlugo/stat134-f11/exponential-maximum.pdf |archive-date=20 December 2016|url-status=dead}}&lt;/ref&gt;
